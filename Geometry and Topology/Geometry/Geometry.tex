\documentclass[a4paper]{article}

\input{C:/Users/liula/Desktop/Latex/Headers V1.2.tex}

\pagestyle{fancy}
\fancyhf{}
\rhead{Labix}
\lhead{Geometry}
\rfoot{\thepage}

\title{Geometry}

\author{Labix}

\date{\today}
\begin{document}
\maketitle
\begin{abstract}
Geometry is one of the oldest topics in mathematics. Almost every topic in mathematics has its origin from Geometry and a lot of mathematicians have somewhat been involved int development. In these notes note we study geometry with a bit of algebraic flavour in it, instead of dealing with axiomatic geometry. 
\end{abstract}
\tableofcontents
\pagebreak
\section{Euclidean Geometry}
\subsection{Geometric Objects in Euclidean Space}
\begin{defn}{Euclidean Metric}{} Let $x,y\in \R^n$. The Euclidean Metric is defined to be $$d(x,y)=\|x-y\|$$
\end{defn}

\begin{defn}{Euclidean Space}{} An Euclidean Space is a metric space which is isometric to $\R^n$, with the Euclidean metric for some $n\in\N$. We use the notation $E^n$ to denote the Euclidean Space. 
\end{defn}

\begin{defn}{Lines}{} Let $n\in\N\setminus\{0\}$. Let $u,v\in\R^n$. Define the line through $u$ and $v$ to be the set $$L_{u,v}=\{u+\lambda v\;|\;\lambda\in\R\}\subset\R^n$$
\end{defn}

\begin{defn}{Collinearity}{} Let $n\in\N\setminus\{0\}$. Let $x,y,z\in\R^n$. We say that $x,y,z$ are collinear if there exists a line in $\R^n$ such that $$x,y,z\in L$$
\end{defn}

\begin{lmm}{Translation Preserves Collinearity}{} Let $x,y,z\in\R^n$ be distinct. They are collinear if and only if $x-z,y-z,0$ are collinear. \tcbline\begin{proof} $x,y,z$ is collinear if and only if $z=(1-\lambda)x+\lambda y\iff 0=(1-\lambda)(x-z)+\lambda(x-z)$. This means $x-z,y-z,0$ are collinear
\end{proof}
\end{lmm}

\begin{prp}{}{} $x,y,z\in\R^n$ are collinear if and only if $$d(x,y)=d(x,z)+d(z,y)$$ where $d(x,y)$ is the Euclidean metric. \tcbline
\begin{proof} Suppose that $x,y,z$ are collinear with $z$ in the middle. Then $$z=(1-\lambda)x+\lambda y$$ for some $\lambda\in(0,1)$. Now we have 
\begin{align*}
d(x,z)+d(z,y)&=\|x-z\|+\|z-y\|\\
&=\|x-(1-\lambda)x-\lambda y\|+\|(1-\lambda)x+\lambda y-y\|\\
&=\|\lambda(x-y)\|+\|(1-\lambda)(x-y)\|\\
&=\lambda\|x-y\|+(1-\lambda)\|x-y\|\\
&=\|x-y\|\\
&=d(x,y)
\end{align*}
Now assume that $d(x,y)=d(x,z)+d(z,y)$. We first translate it such that $d(x-z,y-z)=d(x-z,0)+d(0,y-z)$ and set $u=x-z$ and $v=y-z$. Thus are new equality is $d(u,v)=d(u,0)+d(0,v)$
\begin{align*}
\|u-v\|&=\|u\|+\|v\|\\
\|u-v\|^2&=\|u\|^2+2\|u\|\|v\|+\|v\|^2\\
\langle u-v,u-v\rangle&=\|u\|^2+2\|u\|\|v\|+\|v\|^2\\
\|u\|^2-2\langle u,v\rangle+\|v\|^2&=\|u\|^2+2\|u\|\|v\|+\|v\|^2\\
-\langle u,v\rangle&=\|u\|\|v\|\\
\implies\abs{\langle u,v\rangle}&=\|u\|\|v\|\\
\end{align*}
We know that this happens if and only if $v=\lambda u$ for some $\lambda\in\R$, which implies that $0,u,v$ are linear. 
\end{proof}
\end{prp}

\subsection{Isomorphisms of Euclidean Spaces}
\begin{prp}{}{} Let $n\in\N\setminus\{0\}$. Let $f:\R^n\to\R^n$ be an isometry. Then the following are true. 
\begin{itemize}
\item For any isometry $g:\R^n\to\R^n$, $g\circ f:\R^n\to\R^n$ is an isometry. 
\item The identity function $\text{id}_{\R^n}:\R^n\to\R^n$ is an isometry. 
\item $f^{-1}:\R^n\to\R^n$ is an isometry. 
\end{itemize}
\end{prp}

\begin{defn}{The Isometry Group}{} Let $n\in\N\setminus\{0\}$. Define the group of isometries of $(\R^n,d)$ to be the set $$\text{Isom}(\R^n,d)=\{f:\R^n\to\R^n\;|\;f\text{ is an isometry of }\R^n\}$$ together with group structure given by the composition of functions. 
\end{defn}

\begin{lmm}{}{} For any metric space $(X,d)$, $\text{Isom}(X,d)$ is a group. \tcbline
\begin{proof} Note that if $f,g,h\in\text{Isom}(X,d)$, then $g\circ f$ is an isometry, $h\circ g\circ f$ is associative, the identity mapping $f(x)=x$ for all $x\in X$ is the identity of the group and since $f$ is bijective, there exists an inverse of $f$. 
\end{proof}
\end{lmm}

\begin{prp}{}{} Let $n\in\N\setminus\{0\}$. Let $T:\R^n\to\R^n$ be an isometry. Let $x,y,z\in\R^n$. Then $x,y,z$ are collinear if and only if $T(x),T(y),T(z)$ are collinear. 
\end{prp}

\subsection{Classifying Isometries of Euclidean Space}
\begin{defn}{Affine Maps}{} A map $T:\R^n\to\R^m$ is affine if it is of the form $$T(x)=Ax+b$$ for all $x\in\R^n$ for some linear map $A$ and $b\in\R^k$. 
\end{defn}

\begin{prp}{}{} Let $n,k\in\N\setminus\{0\}$. Let $T:\R^n\to\R^k$ be a function. Then the following are equivalent. 
\begin{itemize}
\item $T$ is an affine map. 
\item The map $L(x)=T(x)-T(0)$ is a linear map. 
\item For all $\lambda\in\R$, $T(\lambda x+(1-\lambda)y)=\lambda T(x)+(1-\lambda)T(y)$
\end{itemize}\tcbline
\begin{proof} Let $T:\R^n\to\R^k$ be a map. 
\begin{itemize}
\item $(1)\iff(2)$: Define $L(x)=T(x)-T(0)$. Then $T$ is affine if and only if $L$ is linear, if and only if $L(\lambda x+\mu y)=\lambda L(x)+\mu L(y)$ if and only if $T(\lambda x+\mu y)-T(0)=\lambda(T(x)-T(0))+\mu(T(y)-T(0))$. 
\item $(2)\implies (3)$: This is obtained by setting $\mu=1-\lambda$. 
\item $(3)\implies (2)$: We have
\begin{align*}
\lambda x+\mu y&=\frac{1}{2}(2\lambda x)+\frac{1}{2}(2\mu y)\\
T(\lambda x+\mu y)&=\frac{1}{2}T(2\lambda x)+\frac{1}{2}T(2\mu y)\tag{By (3)}
\end{align*}
Also by $(3)$, we have
\begin{align*}
2\lambda x&=2\lambda x+(1-2\lambda)0\\
T(2\lambda x)&=2\lambda T(x)+(1-2\lambda)T(0)
\end{align*}
And similarly, $$T(2\mu y)=2\mu T(y)+(1-2\mu)T(0)$$
Combining the three, we have 
\begin{align*}
T(\lambda x+\mu y)&=\frac{1}{2}T(2\lambda x)+\frac{1}{2}T(2\mu y)\\
&=\frac{1}{2}(2\lambda T(x)+(1-2\lambda)T(0))+\frac{1}{2}(2\mu T(y)+(1-2\mu)T(0))\\
&=\lambda T(x)+\left(\frac{1}{2}-\lambda\right)T(0)+\mu T(y)+\left(\frac{1}{2}-\lambda\right)T(0)\\
T(\lambda x+\mu y)-T(0)&=\lambda(T(x)-T(0))+\mu(T(y)-T(0))
\end{align*}
\end{itemize}
\end{proof}
\end{prp}

\begin{prp}{}{} Let $n\in\N\setminus\{0\}$. Let $L:\R^n\to\R^n$ be a linear map. Let $A$ be the matrix representing $T$ under the standard basis. Then the following are equivalent. 
\begin{itemize}
\item $L$ is an isometry
\item $\|L(x)\|=\|x\|$ for all $x\in\R^n$
\item $\langle L(x),L(y)\rangle=\langle x,y\rangle$ for all $x,y\in\R^n$
\item $A$ is an orthogonal matrix. 
\end{itemize}\tcbline
\begin{proof} Let $L:\R^n\to\R^k$ be linear. 
\begin{itemize}
\item $(1)\implies(2)$: This is trivial since isometries are distance preserving
\item $(2)\implies(3)$: This is given by the polarization identity
\item $(3)\implies(4)$: We have that  
\begin{align*}
\left(A^TA\right)_{ij}&=\sum_{k=1}^n(A^T)_{ik}A_{kj}\\
&=\sum_{k=1}^nA_{ki}A_{kj}\\
&=\sum_{k=1}^nL(e_i)_kL(e_j)_k\\
&=\langle L(e_i),L(e_j)\rangle\\
&=\langle e_i,e_j\rangle\tag{By $(3)$}\\
&=\delta_{ij}
\end{align*} Thus $A^TA=I$
\item $(4)\implies(1)$: Suppose that $A^TA=I$. Then $A$ is invertible and thus is a bijection. Thus we just need to show that $d(x,y)=d(L(x),L(y))$.
\begin{align*}
\|L(x)\|^2&=\langle L(x),L(x)\rangle\\
&=(L(x))^TL(x)\\
&=x^TA^TAx\\
&=x^Tx\\
&=\langle x,x\rangle\\
&=\|x\|^2
\end{align*}
Thus $d(L(x),L(y))=\|L(x)-L(y)\|=\|L(x-y)\|=\|x-y\|=d(x,y)$. 
\end{itemize}
\end{proof}
\end{prp}

\begin{prp}{}{} Let $n\in\N\setminus\{0\}$. Let $T:\R^n\to\R^n$ be a function. Then $T$ is an isometry if and only if $T$ is affine and given by $T(x)=Ax+b$, where $A$ is orthogonal. \tcbline
\begin{proof} We have shown that Euclidean Isometries are line preserving, thus by $1.3.2$ they are affine and in the form $T(x)=Ax+b$. We now check that $G(x)=Ax=T(x)-b$ is an isometry. $G$ is distance preserving since
\begin{align*}
d(G(x),G(y))&=\|G(x)-G(y)\|\\
&=\|T(x)-b-T(y)+b\|\\
&=\|T(x)-T(y)\|\\
&=\|x-y\|\tag{$T$ is an isometry}\\
&=d(x,y)
\end{align*}
We now show that $G$ is bijective. Since $T$ is an isometry, $T$ is bijective and $T(x)-b$ is also bijective with $T^{-1}(x)=x+b$ and thus $G$ is an isometry. By the above proposition, $A$ is orthogonal. 
\end{proof}
\end{prp}

\begin{prp}{}{} Let $\phi:O(n,\R)\to\text{Aut}(\R^n)$ be the function defined by $\phi(A)(x)=Ax$ for $x\in\R^n$. Then $\phi$ is a group homomorphism. Moreover, there is a group isomorphism $$\psi:\text{Isom}(\R^n,d)\to \R^n\rtimes_\phi O(n,\R)$$ given by $T\mapsto(T(0),T-T(0))$. 
\end{prp}

\subsection{Canonical Form of Orthogonal Matrices}
\begin{thm}{Canonical Form of Orthogonal Matrices}{} Let $M\in O(n,\R)$ be an orthogonal matrix. Then there exists an orthonormal basis of $\R^n$ such that $M$ is similar to a matrix of the form $$\begin{pmatrix}
I_k &&&&\\
&-I_m&&&\\
&&B_1&&\\
&&&\ddots&\\
&&&&B_l
\end{pmatrix}$$ where $$B_i=\begin{pmatrix}\cos(\theta_i) & -\sin(\theta_i) \\
\sin(\theta_i) & \cos(\theta_i)
\end{pmatrix}$$\tcbline
\begin{proof}
Let $L:\R^n\to\R^n$ be an isometry. We prove the result by induction on $n$. Extend the domain of $L$ such that it becomes a map from $\C^n\to\C^n$. By the fundamental theorem of algebra, $c_A(L)$ has at least one root, $\lambda$. We know that $\abs{\lambda}=1$. There are two cases. 
\begin{itemize}
\item If $\lambda\in\R$, then $\lambda=\pm1$. Choose an eigenvector $z=x+iy$, with $x,y\in\R^n$. We have that $\lambda(x+iy)=L(x+iy)=L(x)+iL(y)$, thus $x,y$ respectively are both real eigenvectors of $\lambda$. At least one of these $x,y$ is non zero, else $z=0$. Now $\frac{x}{\|x\|}$ is a basis for $W=\C\cdot x$ and the matrix for $L$ is $$\begin{pmatrix}
\pm1 & 0\\
0 & L|_{W^\perp}
\end{pmatrix}$$
\item If $\lambda\in\C\setminus\R$, choose an eigenvector $z$. We must have $\overline{\lambda}$ is also an eigenvalue, with eigenvector $\overline{z}$. Let $W=E_{\lambda}\oplus E_{\overline{\lambda}}$. By the above lemma, there exists a real orthonormal basis for $W$. In terms of a basis for $W\oplus W^\perp$, the matrix for $L$ is $$\begin{pmatrix}
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix} & 0\\
0 & L|_{W^\perp}
\end{pmatrix}$$
\end{itemize}
By the above, in both cases, $L|_{W^\perp}$ is an isometry of $W^\perp$, so we can apply the induction step to $W^\perp$. Eventually, we will have decomposed $V$ into mutually orthogonal subspaces $W_1,\dots,W_M$, all invariant under $L$. Either $W_i$ has dimension $2$ with a real orthonormal basis with respect to which the matrix of $L|_W$ has the form as in the above lemma, or $W_i$ has dimenison $1$, with normalized real basis, and $L$ acts as $1$ or $-1$. Since the $W_i$ are mutally orthogonal, the basis consisting of the union of all these basis is orthonormal. By reordering the subspaces appropriately we obtain the above form. 
\end{proof}
\end{thm}

I would say that the main part of this theorem is that you essentially decompose $\R^n$ into $W_1\oplus\dots\oplus W_n$, where they are mutually orthogonal. It might be the case that an isometry in $\R^2$ or $\R^3$ allows some orthogonal to rotate or reflect, leaving the rest invariant. Thus performing each rotation or reflection in $\R^3$ is only transforming one of $W_1,\dots,W_n$. 

\subsection{Isometry Decomposition}
\begin{defn}{Hyperplane}{} Let $V\subseteq\R^n$ be a vector subspace of $\R^{n-1}$. Let $b\in\R^n$. Let $v_n\in\R$ such that $v_n\perp V$. A hyperplane of $\R^n$ is an affine subspace of dimension $n-1$, which has the form $$\Pi=V+b=\{b+v|v\in V\}=\{v\in V|\langle v,v_n\rangle=\langle v_n,b\rangle\}$$ where $V\subseteq\R^n$ and $b\in\R^n$
\end{defn}

\begin{defn}{Fixed Points}{} Let $T:\R^n\to\R^n$ be an isometry. Denote the set of fixed points of $T$ to be $$\text{Fix}(T)=\{x\in\R^n|T(x)=x\}$$
\end{defn}

\begin{defn}{Reflection on Hyperplane}{} Let $\Pi$ be a hyperplane. A reflection in $\Pi$ is an Euclidean Isometry $\rho_\Pi:\R^n\to\R^n$ such that Fix$(\rho_\Pi)=\Pi$
\end{defn}

\begin{prp}{}{} The reflection on hyperplane exists and is unique. \tcbline
\begin{proof} Let $\Pi=V+b$. Pick $v\in V^\perp$. Take a basis of $\R^n$ consisting of $v$ together with a basis for $V$. With respect to the basis, define a linear map $T$ with matrix $$A=\begin{pmatrix}
-1 & 0\\
0 & I_{n-1}
\end{pmatrix}$$
Note that this fixes $V$ and is not an identity. Let $S(x)=x-b$. I claim that $\rho=S^{-1}\circ T\circ S$ is  a reflection. I prove that Fix$(\rho)=\Pi$. This means solving $x=Ax+(I-A)b$. We have that $(A-I)(x-b)=0$. Solving this gives $$x=\begin{pmatrix}
0+b_1\\
t_2+b_2\\
\ddots\\
t_n+b_n
\end{pmatrix}$$
But then $x\in\Pi$ since $x$ is of the form $b+v$ with $v\in V$. Thus $\rho_\Pi$ exists. \\~\\
Let $\rho'$ that is not the identity mapping fixes $\Pi$. Then $R=T\circ\rho'\circ T^{-1}$ by 1.4.2 fixes $V$ and fixes $V^\perp$. Thus $R(v)=\lambda v$ for some $\lambda\in\R$. Since $R$ is an isometry, $\abs{\lambda}=1$. Since $R$ is not the identity, $\lambda=-1$ and thus $R$ has matrix $A$. Thus $\rho=\rho'$ and we have proved uniqueness. 
\end{proof}
\end{prp}

\begin{lmm}{}{} Let $p\in\R^n$. Let $V\subseteq\R^n$ and $v\perp V$. Let $\rho_\Pi$ be the reflection on a hyperplane $\Pi=V+b$. Then $$\rho_\Pi(p)=p-2\langle p-b, v\rangle v$$\tcbline
\begin{proof}
Let $d$ be the shortest Euclidean distance between $P$ and the hyperplane. To obtain the reflection of $P$ along the hyperplane, we calculate the vector that starts at $P$ with magnitude $d$ and direction towards the hyperplane, and perpendicular to it, then add it to $P$ two times to obtain it. \linebreak\linebreak
Let the point where the tail of $v$ is on $\Pi$ be $Q$. Note that $QP=P-b$. Since $\|v\|=1$, we have that the magnitude of the projection of $QP$ on to $v$ is given by $\langle P-b,v\rangle$. Thus using our method, we obtain that $\rho_\Pi(P)=P-2\langle P-b,v\rangle v$. 
\end{proof}
\end{lmm}

\begin{lmm}{}{} Let $p,q\in\R^n$ be distinct. Then there exists a reflection $\rho$ with $\rho(p)=q$. \tcbline
\begin{proof}
Let $v=\frac{p-q}{\|p-q\|}$. Let $b=\frac{p+q}{2}$. I claim that $\Pi=\R v^\perp+b$ admits a reflection such that $\rho_\Pi(p)=q$. 
\begin{align*}
\rho_\Pi(p)&=p-2\left\langle p-\frac{p+q}{2},\frac{p-q}{\|p-q\|}\right\rangle \frac{p-q}{\|p-q\|}\\
&=p-\langle p-q,p-q\rangle\frac{p-q}{\|p-q\|^2}\\
&=q
\end{align*}
Thus we are done. 
\end{proof}
\end{lmm}

\begin{thm}{}{} Let $T\in\text{Isom}(\R^n,d)$ be an isometry of $\R^n$. Then $T$ is the composition of at most $n+1$ reflections. \tcbline
\begin{proof} Let $T$ be an isometry. Let $P=T(0)$ and $Q=0$. Then by the above, there exists a reflection $R$ such that $R(T(0))=0$. Thus $L=R\circ T$ is a linear isometry. Choose an orthonormal basis $v_1,\dots,v_n$ so that by the normal form theorem, the matrix $M$ has the following form $$M=\begin{pmatrix}
I_k &&&&\\
&-I_m&&&\\
&&B_1&&\\
&&&\ddots&\\
&&&&B_l
\end{pmatrix}$$
Denote $J_i$ to be the $n\times n$ identity matrix, except $-1$ is at the $(i,i)$th spot. Note that this is the matrix of a reflection in $(\R v_i)^\perp$. Let $B_\theta$ be the matrix of rotation of degree $\theta$. Then it can be decomposed into $$B_i=A_i\begin{pmatrix}
1 & 0\\
0 & -1
\end{pmatrix}=\begin{pmatrix}
\cos(\theta_i) & \sin(\theta_i)\\
\sin(\theta_i) & -\cos(\theta_i)
\end{pmatrix}\begin{pmatrix}
1 & 0\\
0 & -1
\end{pmatrix}$$
The matrix $A_\theta$ has eigenvalues $1$ and $-1$, with corresponding eigenvectors $\left(\cos\left(\frac{\theta_i}{2}\right),\sin\left(\frac{\theta_i}{2}\right)\right)$ and $\left(\sin\left(\frac{\theta_i}{2}\right),-\cos\left(\frac{\theta_i}{2}\right)\right)$ respectively. \linebreak\linebreak
Define $C_i=J_{k+m+2i}$ and $$D_i=\begin{pmatrix}
I_{k+m+2i-2} & 0 & 0\\
0 & B_i & 0\\
0 & 0 & I_{n-k-m-2i}
\end{pmatrix}$$ and $$E_i=\begin{pmatrix}
I_{k+m+2i-2} & 0 & 0\\
0 & A_i & 0\\
0 & 0 & I_{n-k-m-2i}
\end{pmatrix}$$
Note that $E_i$ is a reflection in the plane orthogonal to $\sin\left(\frac{\theta_i}{2}\right)v_{k+m+2i-1}-\cos\left(\frac{\theta_i}{2}\right)v_{k+m+2i}$. Then we have $$M=J_{k+1}\circ\dots\circ J_{k+m}\circ D_1\circ\dots\circ D_l$$ and thus $$M=J_{k+1}\circ\dots\circ J_{k+m}\circ E_1\circ C_1\circ\dots\circ E_l\circ C_l$$ which is the product of at most $n$ reflections. Since $T=R^{-1}\circ L$, $T$ is the product of at most $n+1$ reflections. 
\end{proof}
\end{thm}

\pagebreak
\section{Spherical Geometry}
\subsection{The $n$-Sphere}
\begin{defn}{$n$-Dimensional Sphere}{} Let $r\geq 0$. Define the $n$ dimensional sphere of radius $r$ to be $$S^n(r)=\{(x_1,\dots,x_{n+1})\in\R^{n+1}:\|x\|=r\}$$
\end{defn}

\begin{defn}{Great Circle}{} A great circle is the intersection of $S^n(r)$ with a two dimensional vector subspace of $\R^{n+1}$
\end{defn}

\begin{defn}{Antipodal Points}{} Let $P,Q\in S^n(r)$. We say that $P$ and $Q$ are antipodal if $Q=-P$. 
\end{defn}

\begin{lmm}{}{} If $P,Q\in S^n(r)$ are not antipodal, then there exists a unique great circle containing $P$ and $Q$. 
\end{lmm}

\begin{defn}{Collinearity}{} Three distinct points on $S^n$ are said to be collinear if there exists a great circle such that the three points lie on it. 
\end{defn}

\begin{defn}{Spherical Triangle}{} A triangle on $S^n(r)$ is three distinct points that are joined together by three great circles. 
\end{defn}

\subsection{Spherical Metric}
\begin{defn}{Spherical Metric}{} Define the Spherical Metric by $$d_S(P,Q)=r\cos^{-1}\left(\frac{\langle P,Q\rangle}{r^2}\right)$$ where we take the domain of $\cos^{-1}$ to be $[0,\pi]$. 
\end{defn}

\begin{prp}{}{} Let $\triangle PQR$ be a spherical triangle of $S^n$. Then $\angle QOR=d(Q,R)$ and vice versa for the other angles. \tcbline
\begin{proof} By definition of a radian, the arc $QR$ is equal to the angle $\angle QOR$. 
\end{proof}
\end{prp}

\begin{defn}{Spherical Angle}{} Suppose that two great circles on $S^n$ intersect at one of the points $P$. Define the spherical angle $P$ to be the smaller angle made on $S^n$ between the two great circles. We write it as $\angle_S$. 
\end{defn}

\begin{prp}{}{} Let $PQR$ be a spherical triangle of $S^2$. Suppose that $\alpha=\angle_SQPR$, $\beta=\angle_SPRQ$ and $\gamma=\angle_S PQR$. Let $a=\angle QOR=d_{S^2}(Q,R)$. Then $$\cos(\alpha)=\cos(\beta)\cos(\gamma)+\sin(\beta)\sin(\gamma)\cos(a)$$\tcbline
\begin{proof}
Note that isometries on $\R^3$ preserves the inner product, since spherical distance is defined by the inner product, the spherical distance is preserves. Thus we may perform an isometry such that $P=(0,0,1)$. We can then perform a  rotation such that $Q$ maps to a point on the $xz$ plane. We can then write $Q=(\sin(\beta), 0, \cos(\beta))$ and $R=(\sin(\gamma)\cos(\alpha), \sin(\gamma)\sin(\alpha), \cos(\gamma))$. Calculating this gives our required formula. 
\end{proof}
\end{prp}

\begin{prp}{Triangle Inequality}{} Let $PQR$ be a spherical triangle of $S^n$ whose sides are shorter arcs given by $\alpha,\beta,\gamma\leq\pi$. Then $$\alpha\leq\beta+\gamma$$ with equality if and only if $PQR$ is collinear. \tcbline
\begin{proof}
Notice that by definition, $\alpha,\beta,\gamma\in[0,\pi]$ thus $\sin(\beta)$ and $\sin(\gamma)$ is greater than or equal to $0$. Since $\cos(a)\geq-1$, we must have from the above proposition, 
\begin{align*}
\cos(\alpha)&\geq\cos(\beta)\cos(\gamma)-\sin(\beta)\sin(\gamma)\\
&=\cos(\beta+\gamma)
\end{align*}
We split it into two cases. If $\beta+\gamma\leq\pi$, then $\cos(x)$ being decreasing function on $[0,\pi]$ implies $\alpha\leq\beta+\gamma$ and we are done. Equality is given as long as $\cos(a)=-1$, which means that $a=\pi$, which forces $\beta+\gamma=\pi$ from the inequality. \\~\\
Now if $\beta+\gamma>\pi$, then the triangle inequality is trivial since $\alpha\leq\pi<\beta+\gamma$. Now equality occurs if and only if $a=\pi$. Then this is true if and only if $\cos(\alpha)=\cos(\beta+\gamma)$ which is true if and only if $\alpha+\beta+\gamma=2\pi$ and we are done. 
\end{proof}
\end{prp}

\begin{prp}{}{} The spherical metric is indeed a metric on $S^n$. And thus $S^n$ is a metric space. \tcbline
\begin{proof}
\end{proof}
\end{prp}

\subsection{Isometries in $S^n$}
\begin{prp}{}{} An isometry on $S^n$ preserves antipodal points and great circles. 
\end{prp}

\begin{prp}{}{} Every isometry in $S^n$ can be extended to a Euclidean Isometry of $\R^{n+1}$. Every Euclidean isometry $T:\R^{n+1}\to\R^{n+1}$ can be restricted to an spherical isometry $T|_{S^n}:S^n\to S^n$. 
\end{prp}

\begin{prp}{}{} Every isometry $T:S^n\to S^n$ is of the form $T(x)=Ax$ for all $x\in S^n$, where $A\in O(n+1)$. 
\end{prp}

\begin{crl}{}{} There exists a group isomorphism $$\text{Isom}(S^n,d_{S^n})\cong O(n+1) $$
\end{crl}

\subsection{Geometry of the $2$-Sphere}
\begin{prp}{}{} Let $C,D$ be two distinct great circles, then their intersection is a pair of antipodal points. 
\end{prp}

\begin{thm}{Girad's Theorem}{} Let $\triangle PQR$ be a spherical triangle on $S^2$. With $P,Q,R$ distinct points and such that the interior of $\triangle PQR$ does not intersect the great circles which contain the sides of $\triangle PQR$. Then the area of the triangle is equal to $\angle PQR+\angle PRQ+\angle QPR-\pi$. 
\end{thm}

The reason that we have this much constraints is for us to make sure that we are talking about the same triangle which should be the smallest triangle among the many triangles created from the great circles (or at least those without a segment of a great circle in it). 

\pagebreak
\section{Hyperbolic Geometry}
\subsection{Lorentz Products}
We first develop the necessary tools to develop the hyperbolic space. 
\begin{defn}{Lorentz Inner Product}{} Let $x,y\in\R^n$. Define the Lorentz inner product to be $$\langle x,y\rangle_L=-x_1y_1+x_2y_2+x_3y_3+\dots+x_ny_n$$
\end{defn}

\begin{defn}{Lorentz Norm}{} Define the Lorentz norm of a vector $x\in\R^n$ to be $$\|x\|_L=\sqrt{\langle x,x\rangle_L}$$ where ther square root to be positive or positive imaginary or $0$. 
\end{defn}

Note that the Lorentz norm allows imnaginary numbers and it could also be negative and thus is not a norm in the usual sense. 

\begin{prp}{Hyperbolic Polarization Identity}{} Let $x,y\in\R^n$. Then $$\langle x,y\rangle_L=\frac{1}{4}\|x+y\|_L^2-\frac{1}{4}\|x-y\|_L^2$$
\end{prp}

\begin{defn}{Classification of Vectors}{} Let $x\in\R^n$. We say that $x$ is 
\begin{itemize}
\item space-like if $\|x\|_L>0$
\item light-like if $\|x\|_L=0$
\item time-like if $\|x\|_L\in\C\setminus\R$
\end{itemize}
\end{defn}

\begin{defn}{Lorentz Orthnormal}{} A set of vectors $v_1,\dots,v_n\in\R^n$ is Lorentz Orthgonal if $\langle v_i,v_j\rangle_L=0$ for $i\neq j$. They are said to be Lorentz Orthonormal if $$\langle v_i,v_j\rangle_L=\begin{cases}
0 & \text{ if }i\neq j\\
1 & \text{ if }2\leq i=j\leq n\\
-1 & \text{ if }i=j=1
\end{cases}$$
\end{defn}

\begin{lmm}{}{} If $v_1,\dots,v_n$ are Lorentz Orthonormal, then they form a basis of $\R^n$. \tcbline
\begin{proof}
We just have to show that they are linearly independent. Suppose that $\sum_{k=1}^na_kv_k=0$ for some $a_1,\dots,a_n\in\R$. For each $i\in\{2,\dots,n\}$, we have that $$0=\left\langle\sum_{k=1}^na_kv_k,v_i\right\rangle_L=a_i$$ and for $i=1$ we have that the inner product is $-a_1$. Thus $a_1=\dots=a_n=0$ and we are done. 
\end{proof}
\end{lmm}

\begin{defn}{Lorentz Cross Product}{} Let $x,y\in\R^3$. Define the Lorentz cross product of $x$ and $y$ to be $$x\times_L y=\begin{pmatrix}
-1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}(x\times y)=\begin{vmatrix}
-e_1 & e_2 & e_3\\
x_1 & x_2 & x_3\\
y_1 & y_2 & y_3
\end{vmatrix}$$
\end{defn}

An easy way to remember everything for Lorentz related operations is that the first element in a coordinate related operation is always negative. 

\begin{lmm}{Hyperbolic Binet Cauchy Identity}{} For $x,y,z,w\in\R^3$, the following identity holds. $$\langle x\times_Ly,z\times_L w\rangle_L=-\langle x,z\rangle_L\cdot\langle y,w\rangle_L+\langle x,w\rangle_L\cdot\langle y,z\rangle_L$$
\end{lmm}

Now that we have mostly developed a sufficient amount linear algebra, we turn to study matrices that corresponds to Lorentz operations. 

\begin{defn}{Lorentz Orthgonal Matrices}{} Let $J=\begin{pmatrix}-1&0\\0&I_{n-1}\end{pmatrix}$. A $n\times n$ real matrix is Loretnz orthgonal if $$A^TJA=J$$
\end{defn}

\begin{prp}{}{} The following are true for any $x,y\in\R^n$. 
\begin{itemize}
\item $\langle x,y\rangle_L=x^TJy=\langle Jx,y\rangle$
\item $x\times_Ly=J(x\times y)=(Jy)\times(Jx)$
\end{itemize}
\end{prp}

$J$ will often replace the function of the identity in Hyperbolic space. From time to time we will see it reappear. 

\begin{prp}{}{} Let $T:\R^n\to\R^n$ be a linear map. The following are equivalent. 
\begin{itemize}
\item $T$ is a Lorentz Transformation
\item The matrix of $T$ is Lorentz Orthogonal
\item $\|T(x)\|_L=\|x\|_L$ for all $x\in\R^n$
\end{itemize} \tcbline
\begin{proof}~\\
\begin{itemize}
\item $(1)\iff(2)$: Suppose that $T$ is a Lorentz transformation. Let the matrix $A$ represent $T$ inthe standard basis so that it has columns $a_1,\dots,a_n$. This means that $T(e_k)=a_k$. Then we have 
\begin{align*}
A^TJA&=\begin{pmatrix}
a_1^T\\\vdots\\a_n^T
\end{pmatrix}J\begin{pmatrix}
a_1 & \cdots & a_n
\end{pmatrix}\\
&=\begin{pmatrix}
a_1^T\\\vdots\\a_n^T
\end{pmatrix}\begin{pmatrix}
Ja_1 & \cdots & Ja_n
\end{pmatrix}\\
&=\begin{pmatrix}
a_1^TJa_1 & \cdots & a_1^TJa_n\\
\vdots & \ddots & \vdots\\
a_n^TJa_1 & \cdots & a_n^TJa_n
\end{pmatrix}\\
&=\begin{pmatrix}
\langle a_1,a_1\rangle_L & \cdots & \langle a_1,a_n\rangle_L\\
\vdots & \ddots & \vdots\\
\langle a_n,a_1\rangle_L & \cdots & \langle a_n,a_n\rangle_L
\end{pmatrix}
\end{align*}
Thus we can see that $A^TJA=J$ if and only if $T(e_1),\dots, T(e_n)$ are Lorentz orthonormal if and only if $T$ is a Lorentz transformation. 
\item $(1)\implies(3)$: Suppose that $T$ is a Lorentz transoformation. Then trivially $$\|T(x)\|_L^2=\langle T(x),T(x)\rangle_L=\langle x,x\rangle_L=\|x\|_L^2$$ Taking the positive square root gives our result. 
\item $(3)\implies(1)$: Using the polarization identity, we have that 
\end{itemize}
\end{proof}
\end{prp}

\subsection{Lorentz Transformations and $O(1,n)$}
\begin{defn}{Lorentz Transformation}{} A map $\phi:\R^n\to\R^n$ is a Lorentz transformation if it preserves the Lorentz inner product: $$\langle\phi(x),\phi(y)\rangle_L=\langle x,y\rangle_L$$ for all $x,y\in\R^n$. We say that $\phi$ is positive if both $x$ and $\phi(x)$ has their first coordinate larger than $0$. 
\end{defn}

\begin{prp}{}{} A map $T:\R^n\to\R^n$ is a Lorentz Transformation if and only if $\{T(e_1),\dots,T(e_n)\}$ is a Lorentz orthonormal basis and $T$ is linear. \tcbline
\begin{proof}
Suppose that $T$ is a Lorentz transformation. Then since it preserves the Lorentz inner product and $e_1,\dots,e_n$ is trivially Lorentz orthonormal, we know that $T(e_1),\dots,T(e_n)$ are Lorentz orthonormal and we are done. Now let $x=\sum_{k=1}^nx_ke_k$ and $T(x)=\sum_{k=1}^nb_kT(e_k)$. Then $$-b_1=\langle T(x),T(e_1)\rangle_L=\langle x,e_1\rangle_L=-x_1$$ and $$b_k=\langle T(x),T(e_k)\rangle_L=\langle x,e_k\rangle_L=-x_k$$ for $k\in\{2,\dots,n\}$ thus $T$ is linear. \\~\\
Suppose now that $T(e_1),\dots,T(e_n)$ is Lorentz orthonormal and $T$ is linear. Then 
\begin{align*}
\langle T(x),T(y)\rangle_L&=T(x)^TJT(y)\\
&=\left(\sum_{k=1}^nx_kT(e_k)\right)^TJ\left(\sum_{k=1}^ny_kT(e_k)\right)\\
&=\sum_{k=1}^n\sum_{j=1}^nx_ky_jT(e_k)^TJT(e_j)\\
&=\sum_{k=1}^n\sum_{j=1}^nx_ky_je_kJe_j\\
&=\left(\sum_{k=1}^nx_ke_k\right)^TJ\left(\sum_{k=1}^ny_ke_k\right)\\
&=\langle x,y\rangle_L
\end{align*}
\end{proof}
\end{prp}

\begin{defn}{Lorentz Group}{} The Lorentz group is the group of Lorentz orthogonal $n+1,n+1$ matrices denoted $$O(1,n)=\{A\in M_{n+1\times n+1}(\R)\}|A^TJA=J\}$$
\end{defn}

\begin{prp}{}{} $O(1,n)$ is a group. \tcbline
\begin{proof}
\end{proof}
\end{prp}

\begin{prp}{}{} Let $A\in M_{m\times n}(\R)$. Then the following are equivalent. 
\begin{itemize}
\item The map $T(x)=Ax$ is a Lorentz transformation
\item $\|T(x)\|_L=\|x\|_L$ for all $x\in\R^n$
\item $A$ is Lorentz orthogonal. 
\end{itemize}
\end{prp}

\begin{crl}{}{} Every Lorentz Transformation corresponds to an element of $O(1,n)$ and every element of $O(1,n)$ gives rise to a Lorentz Transformation. \tcbline
\begin{proof}
Let $T$ be a lorentz transformation. \\~\\
Now let $T$ be a hyperbolic isometry. Define a Lorentz transformation $S$ by $S|_{H^n}=T$
\end{proof}
\end{crl}

\subsection{Positive Lorentz Transformations}
\begin{defn}{Lorentz Transformation}{} A map $\phi:\R^n\to\R^n$ is a positive Lorentz transformation if the following are true
\begin{itemize}
\item $\phi$ is a Lorentz transformation
\item $x$ is time-like if and only if $T(x)$ is time-like for $x\in\R^n$. 
\end{itemize}
\end{defn}

\begin{defn}{Positive Lorentz Group}{} The positive Lorentz group is the set of all elements in $O(1,n)$ which maps positive time-like vectors bijectively to positive time-like vectors, denoted $O^+(1,n)$
\end{defn}

\begin{prp}{}{} $O^+(1,n)$ is a group. \tcbline
\begin{proof}
\end{proof}
\end{prp}

\begin{lmm}{}{} Let $A\in O(1,n)$. Then $A\in O^+(1,n)$ if and only if $a_{1,1}>0$. 
\end{lmm}

This is extremely powerful since it gives a fairly easy way to check whether a Lorentz orthogonal matrix is an isometry. 

\begin{crl}{}{} Every Positive Lorentz Transformation corresponds to an element of $O^+(1,n)$ and every element of $O^+(1,n)$ gives rise to a Positive Lorentz Transformation. \tcbline
\begin{proof}
Let $T$ be a lorentz transformation. \\~\\
Now let $T$ be a hyperbolic isometry. Define a Lorentz transformation $S$ by $S|_{H^n}=T$
\end{proof}
\end{crl}

\begin{lmm}{}{} For every set of linearly independent $a,b,c\in H^n$, there exists an element $A\in O^+(1,n)$ such that 
\begin{align*}
Aa&=(1,0,\dots,0)\\
Ab&=(b_1,b_2,0,\dots,0)\text{ with }b_1>0\\\
Ac&=(c_1,c_2,c_3,0\dots,0)\text{ with }c_1>0
\end{align*} \tcbline
\begin{proof}
Let $a,b,c,z_4,\dots,z_{n+1}$ be a basis for $H^n$. We construct a Lorentz orthonormal basis for $\R^{n+1}$. Since $a\in H^n$ implies $\|a\|=i$, we can choose $v_1=a$. We find the basis by induction, where $v_k$ is constructed by $a,b,c,z_4,\dots,z_k$. Now let $w_2=b+\langle v_1,b\rangle_L v_1$. Observe that 
\begin{align*}
\langle w_2,v_1\rangle_L&=\langle b,v_1\rangle_L+\langle v_1,b\rangle_L\langle v_1,v_1\rangle_L\\
&=\langle b,v_1\rangle_L-\langle v_1,b\rangle_L\tag{$\langle v_1,v_1\rangle_L=-1$}\\
&=0
\end{align*}
Thus $w_2$ is Lorentz orthogonal to $v_1$. Now just choose $v_2=\frac{w_2}{\|w_2\|_L}$. Note that this is similar to the Gram-schimdt procedure in Euclidean Space. In general, suppose that $v_1,\dots,v_k$ are now made orthogonal. Define $$w_{k+1}=a-\sum_{i=1}^k\langle z_{k+1},v_i\rangle_Lv_i$$ Then $w_{k+1}$ will be orthogonal to all of $v_1,\dots,v_k$ thus we can choose $v_{k+1}=\frac{w_{k+1}}{\|w_{k+1}\|}$. Thus a Lorentz orthonormal basis $v_1,\dots,v_{n+1}$ is formed. \\~\\
With respect to this basis, we have the desired results. 
\end{proof}
\end{lmm}

\subsection{Hyperbolic Space}
\begin{defn}{Hyperbolic Space}{} The $n$ dimensional hyperbolic space is the space $$H^n=\{x\in\R^{n+1}|\|x\|_L=i\text{ and }x_1>0\}$$ along with the metric hyperbolic metric $$d_{H^n}(x,y)=\cosh^{-1}(-\langle x,y\rangle_L)$$
\end{defn}

In order to prove that $H^n$ is a metric space, we need to first develop the notion of lines and triangles and angles in $H^n$. 

\begin{defn}{Classification of Subspaces}{} We say that a vector subspace of $\R^n$ is 
\begin{itemize}
\item time-like if $V$ has at least one time-like vector
\item space-like if every nonzero vector in $V$ is space-like
\item light-like otherwise
\end{itemize}
\end{defn}

To check the type of subspace, we first find whether there is one time-like vector, we then check whether there is one light-like vector. 

\begin{prp}{}{} There is a parametrization between $\R^2$ and $H^2$ given by $$f(t,\theta)=(\cosh(t),\cos(\theta)\sinh(t),\sin(\theta)\sinh(t))$$ for $t\in[0,\infty)$ and $\theta\in[0,2\pi]$ where $\R^2$ is in polar coordinates and $H^2$ in cartesian coordinates. \tcbline
\begin{proof}

\end{proof}
\end{prp}

\begin{defn}{Hyperbolic Lines and Lorentz Planes}{} A Lorentz plane is a $2$ dimensional vector subspace of $\R^{n+1}$ that contains a timelike vector. A hyperbolic line is the intersection of $H^n$ with any Lorentz Plane. 
\end{defn}

\begin{defn}{Collinearity}{} Three points $x,y,z\in H^n$ are hyperbolically collinear if and only if there is a hyperbolic line $L$ of $H^n$ with $x,y,z\in L$. 
\end{defn}

We need the following theorem to show that $d_{H^n}$ is a metric. 

\begin{lmm}{}{} For every $P\neq Q\in H^n$, there exists a unique hyperbolic line $L$ containing $P$ and $Q$. 
\end{lmm}

\begin{lmm}{}{} Every line through the origin in $\R^{n+1}$ intersects $H^n$ in at most $1$ point. \tcbline
\begin{proof}
Let $y=\lambda x$ for some $x,y\in H^n$. Then by definition of $H^n$, we must have $$\langle x,x\rangle_L=\langle y,y\rangle_L=-1$$ But the Lorentz inner product is linear. Thus $\langle y,y\rangle_L=\lambda^2\langle x,x\rangle_L$ and thus $\lambda=\pm1$. Since both $x_1$ and $y_1$, the first component of $x$ and $y$ must be positive, we must have $\lambda=1$ and we are done. 
\end{proof}
\end{lmm}

\begin{defn}{Hyperbolic Triangles}{} A hyperbolic triangle, denoted $\triangle PQR$, consists of three distinct, non-collinear points $P,Q,R\in H^n$, and the finite hyperbolic line segments joining each pair of points, and the finite area enclosed by these lines. 
\end{defn}

\begin{defn}{Hyperbolic Angles}{} Let $\triangle PQR$ be a hyperbolic triangle in $H^2\subset\R^3$. Define the hyperbolic angle $a$ at $P$ to be $a\in[0,\pi]$ such that $$\cos(a)=\frac{\langle P\times_LR,P\times_LQ\rangle_L}{\|P\times_LR\|_L\|P\times_LQ\|_L}$$
\end{defn}

To prove the triangle inqeuality for $d_{H^n}$ we need the following theorem. 

\begin{thm}{}{} Let $x,y,z\in H^n$ be distinct. Let $\alpha=d_{H^n}(z,y)$, $\beta=d_{H^n}(x,z)$, $\gamma=d_{H^n}(x,y)$ and $a$ be the hyperbolic angle of $\triangle xyz$ at $x$. Then $$\cosh(\alpha)=\cosh(\beta)\cdot\cosh(\gamma)-\sinh(\beta)\cdot\sinh(\gamma)\cdot\cos(a)$$ \tcbline
\begin{proof}
WLOG we can assume $x,y,z$ takes the form $x=(1,0,\dots,0)$, $y=(\cosh(t_1),\sinh(t_1),0,\dots,0)$ and $z=(\cosh(t_2),\cos(\theta)\sinh(t_2),\sin(\theta)\sinh(t_2),0,\dots,0)$. This is because Loretnz transformation preserves the Loretnz inner product and thus the distance function remains unchanged. Since the trailing zeroes make no contributions to the calculations, we assume we are working on $H^2$. Now note that $$x\times_L\begin{pmatrix}a_1\\a_2\\a_3\end{pmatrix}=\begin{pmatrix}0\\-a_3\\a_2\end{pmatrix}$$ Thus we have 
\begin{align*}
\cos(a)&=\frac{\langle x\times_Ly,x\times_Lz\rangle_L}{\|x\times_Ly\|_L\|x\times_Lz\|_L}\\
&=\frac{1}{\abs{\sinh(t_1)}\abs{\sinh(t_2)}}\left\langle\begin{pmatrix}0\\0\\\sinh(t_1)\end{pmatrix},\begin{pmatrix}0\\-\sin(\theta)\sinh(t_2)\\\cos(\theta)\sinh(t_2)\end{pmatrix}\right\rangle_L\\
&=\cos(\theta)
\end{align*}
Now we have that 
\begin{align*}
\cosh(\gamma)&=-\langle x,y\rangle_L=\cosh(t_1)\\
\cosh(\beta)&=-\langle x,z\rangle_L=\cosh(t_2)\\
\cosh(\alpha)&=-\langle z,y\rangle_L=\cosh(t_1)\cosh(t_2)-\cos(\theta)\sinh(t_1)\sinh(t_2)
\end{align*}
Combining these and using the fact that $\cos(a)=\cos(\theta)$ gives our result. 
\end{proof}
\end{thm}

\begin{thm}{}{} The hyperbolic metric is indeed a metric on $H^n$ and thus the hyperbolic space is a metric space. \tcbline
\begin{proof}~\\
\begin{itemize}
\item We first prove that the hyperbolic metric is well defined. This means that we need to prove that $-\langle x,y\rangle_L\geq 1$. Let $x,y\in H^n$. Denote $x'=(1,x_2,\dots,x_{n+1})$ and $y'=(1,y_2,\dots,y_{n+1})$. Then we have that $$x_1^2=1+x_2^2+\dots+x_n^2=\|x'\|^2$$ and likewise for $y_1^2$. Note that the norm on the right is the usual Euclidean norm. By Cauchy Schwarz we have that $\langle x',y'\rangle^2\leq\|x'\|^2\|y'\|^2=(x_1y_1)^2$ which implies that $$\abs{1+x_2y_2+\dots+x_ny_n}\leq\abs{x_1y_1}=x_1y_1$$ Rearraging, we have that $\langle x,y\rangle_L\leq -1$ and thus $$-\langle x,y\rangle_L\geq 1$$
\item We want to show that $d_{H^n}(x,y)\geq 0$ with equality if and only if $x=y$. Trivially the image of $\cosh^{-1}$ is greater than or equal to $0$ and $\cosh^{-1}(u)=0$ means that $u=1$ and $-\langle x,y\rangle_L=1$. From the above proof, in order for this to be true, we need $\langle x',y'\rangle^2=\|x'\|^2\|y'\|^2$ which is true if and only if $x$ is a multiple of $y$. But we know that every line passing through the origin only intersects $H^n$ in at most one point. Thus $x=y$ and we are done. 
\item We want to showt that $d_{H^n}(x,y)=d_{H^n}(y,x)$ for all $x,y\in H^n$. But clearly the Lorentz inner product is symmetric, thus we are done. 
\item To show the triangle inequality, let $x,y,z\in H^n$. Let $\alpha=d_{H^n}(z,y)$, $\beta=d_{H^n}(x,z)$, $\gamma=d_{H^n}(x,y)$ and $a$ be the hyperbolic angle of $\triangle xyz$ at $x$. Using the above theorem, we find that $$\cosh(\alpha)\leq\cosh(\gamma)\cosh(\beta)+\sinh(\beta)\sinh(\gamma)=\cosh(\beta+\gamma)$$ Since $\cosh(x)$ is increasing on $[0,\infty)$, this implies that $\alpha\leq\beta+\gamma$. Thus we are done. 
\end{itemize}
\end{proof}
\end{thm}

\begin{prp}{}{} Let $R\in\text{Isom}(H^n,d_H)$. Then there exists a unique $A\in O^+(1,n)$ with $R=T_A|_{H^n}$, where $T_A$ is the linear map on $\R^{n+1}$ given by $T(x)=Ax$. 
\end{prp}

\begin{crl}{}{} There is a group isomorphism between Isom$(H^n)$ and $O^+(1,n)$. 
\end{crl}

Becareful that all isometries of $H^n$ is not the Loretnz group but only of the positive Lorentz group. 

\subsection{Hyperbolic Lines}
\begin{lmm}{}{} If $x\in\R^n$ with $\langle x,x\rangle_L<0$, and $w\in\R^n\setminus\{0\}$ with $\langle x,w\rangle_L=0$, then $\langle w,w\rangle_L>0$. In other words, any vector that is Lorentz orthogonal with a time-like vector must be a space-like vector. \tcbline
\begin{proof}
Let $x'=x-x_1e_1$ and $w'=w-w_1e_1$. Then $\langle x',x'\rangle_L\geq 0$ and $\langle x,x\rangle_L=-x_1^2+\langle x',x'\rangle_L$. This means that $x_1\neq 0$. If $w_1=0$, then since $w\neq 0$, we must have $\langle w,w\rangle_L=\langle w',w'\rangle_L>0$ and we are done. \\~\\
Now suppose that $w_1\neq 0$. Then $x_1w-w_1x$ has zero $e_1$ component. I claim that this vector is non-zero. Indeed if $x_1w=w_1x$, then $$0=\frac{x_1}{w_1}\langle x,w\rangle_L=\langle x,\frac{x_1}{w_1}w\rangle_L=\langle x,x\rangle_L<0$$ Now we have that 
\begin{align*}
0\leq\|x_1w-w_1x\|_L^2&=\langle x_1w-w_1x,x_1w-w_1x\rangle_L\\
&=x_1^2\langle w,w\rangle_L+w_1^2\langle x,x\rangle_L
\end{align*}
Bu this means that $$\langle w,w\rangle_L\geq-\frac{w_1^2}{x_1^2}\langle x,x\rangle_L>0$$ and we are done. 
\end{proof}
\end{lmm}

In layman terms, this means that the orthogonal subspace of a time-like vector is a subspace with all vectors being space-like. 

\begin{defn}{}{} Let $L_1,L_2$ be two distinct lines in $H^2$ with Lorentz plane $\Pi_1$ and $\Pi_2$ respectively. Let $v\in\Pi_1\cap\Pi_2\setminus\{0\}$. Let $V=\Pi_1\cap\Pi_2$. 
\begin{itemize}
\item If $V$ is time like, $L_1$ and $L_2$ intersect at a point $x$ and $V=\R v=\Pi_1\cap\Pi_2$
\item If $V$ is space like, $L_1$ and $L_2$ are parallel and diverge
\item If $V$ is light like, $L_1$ and $L_2$ are ultraparallel
\end{itemize}
\end{defn}

The following theorem shows that Euclid's Postulate does not hold in hyperbolic space. 

\begin{thm}{}{} Let $x\in H^2$. Let $L$ be a line in $H^2$. Then there are infinitely many lines in $H^2$ which pass through $x$ and do not intersect $L$. 
\end{thm}

\subsection{Hyperbolic Triangles and Angles}
\begin{prp}{}{} The hyperbolic angles is invariant under hyperbolic isometries. 
\end{prp}

\begin{prp}{}{} Let $\triangle PQR$ be a hyperbolic triangle. Let $a,b,c$ be the angles at $P,Q,R$ respectively. Then $$a+b+c=\pi-\text{Area}(\triangle PQR)$$
\end{prp}

\subsection{Normal Form Theorem}
Lecture did not have time to thoroughly prove things and so do I. Every Lorentz Transformation must have one of the following four forms: \\~\\
Lorentz Translation: $$\begin{pmatrix}
\cosh(\beta) & \sinh(\beta) & 0\\
\sinh(\beta) & \cosh(\beta) & 0\\
0 & 0 & 1
\end{pmatrix}$$This transformation does not have a fixed point in $H^2$. Orientation is preserved. \\~\\
Lorentz Glide: $$\begin{pmatrix}
\cosh(\beta) & \sinh(\beta) & 0\\
\sinh(\beta) & \cosh(\beta) & 0\\
0 & 0 & -1
\end{pmatrix}$$This transformation does not have a fixed point in $H^2$. Orientation is also reversed\\~\\
Rotation about the point $(1,0,0)$: $$\begin{pmatrix}
1 & 0 & 0\\
0 & \cosh(\beta) & \sinh(\beta)\\
0 & \sinh(\beta) & \cosh(\beta)\\
\end{pmatrix}$$This transformation has one fixed point, namely $(1,0,0)$ and is orientation preserving. \\~\\
Reflection along $x_2=0$: $$\begin{pmatrix}
1 & 0 & 0\\
0 & -1 & 0\\
0 & 0 & 1
\end{pmatrix}$$This transformation fixes the line $x_2=0$ and is orientation reversing. \\~\\

\pagebreak
\section{Projective Geometry}
\subsection{Projective Space}
\begin{lmm}{}{} Let $\F$ be a field. The relation $\sim$ in $\F^{n+1}$ where $(x_0,\dots,x_n)\sim(y_0,\dots,y_n)$ if and only if $y_i=\lambda x_i$ for all $i\in\{1,\dots,n\}$ with $\lambda\in\F$ is an equivalence relation. 
\end{lmm}

\begin{defn}{Projective Space}{} The equivalence relation $\sim$ on $\F^{n+1}$ induces the projective space with elements in it being $1$ dimensional subspaces of $\F^n$, written as $$\Prj(\F^{n+1})=\frac{\F^{n+1}\setminus\{0\}}{\sim}$$
We use $\Prj^n$ to denote $\Prj(\R^{n+1})$. Also define the dimension of $\Prj^n$ to be $n$. 
\end{defn}
We use $\R^{n+1}$ as our vector space since every finite dimensional vector space is isomorphic to $\R^{n}$ for some $n$. 

\begin{prp}{}{} There is a bijection between $\mathbb{P}^n$ and the set of lines through the origin in $\R^{n+1}$. 
\end{prp}

\begin{prp}{}{} We have that $$\Prj^n\cong\R^n\cup\R^{n-1}\cup\dots\cup\R\cup\{\infty\}$$ \tcbline
\begin{proof}
The proof is done by induction. We first consider $\Prj(\R)$. Recall that $x\sim y$ if and only if $y=tx$ for some $t\neq 0$. This means that $[x]$ represents the same point as long as $x\in\R\setminus\{0\}$. We also simply identify $[0]$ with $\infty$. \\~\\
We also consider $\Prj(\R^2)$ for better illustration. All elements of $\Prj(\R^2)$ are of the form $[x_0:x_1]$. We consider the case that $x_0\neq 0$. Since $x_0\neq 0$, we can divide out $x_0$ to get the same coordinate $[1:x_1]\in\Prj(\R^2)$. For each $x_1\in\R$, $[1:x_1]$ represents different coordinates in $\Prj(\R^2)$ thus we can easily identify it with $\R$. Now if $x_0=0$, $[0:x_1]$ would represent the same coordinate for any $x_1\in\R$. This is precisely the same situation as $\Prj(\R)$. Thus we can virtually ignore the first coordinate and identify it with $\Prj(\R)$. Thus we have shown that $\Prj(\R^2)=\R^2\cup\Prj(\R)$. \\~\\
Now through the induction hypothesis, we just have to show that $\Prj(\R^n)=\R^n\cup\Prj(\R^{n-1})$. But this is done in a similar fashion. We can use $[1:x_1:\dots:x_n]$ for $x_1,\dots,x_n\in\R$ to identify $\R^n$ and $\Prj(\R^{n-1})$ with $[0:x_1:\dots:x_n]$. 
\end{proof}
\end{prp}

\begin{thm}{}{} We have that $$\Prj(\R^n)=\frac{S^n}{\pm}$$ where $\pm$ is the equivalence relation of antipodal points. 
\end{thm}

\subsection{Projective Linear Subspaces}
\begin{defn}{Projective Linear Subspaces}{} Let $U\subseteq V$ be a vector subspace. Define the projective linear subspace to be $$\Prj(U)=\frac{U\setminus\{0\}}{\sim}\subseteq\Prj(V)$$ with its dimension defined to be $\dim(\Prj(U))=\dim(U)-1$. We define $\dim(\emptyset)=-1$ for conventions. 
\end{defn}

\begin{defn}{Classification of Subspaces}{} Let $U\subseteq V$ be a vector subspace where $\dim(V)=n$. We say that $\Prj(U)$ is
\begin{itemize}
\item A single point if $\dim(U)=1$, or $\dim(\Prj(U))=0$
\item A line if $\dim(U)=2$, or $\dim(\Prj(U))=1$
\item A plane if $\dim(U)=3$, or $\dim(\Prj(U))=2$
\item A hyperplane if $\dim(U)=n-1$, or $\dim(\Prj(U))=n-2$
\end{itemize}
\end{defn}

In particular, note that lines in $\R^{n+1}$ become points. Let $x\in\R^{n+1}$ be fixed. Take the line $y=tx$ for $t\in\R$ as an example. Observe that in the projective space. $x\sim y$ if and only if $y=tx$ for some $t\in\R$. Naturally $y=tx$ becomes a point. 

\begin{defn}{Projective Cone and Span}{} Let $V$ be a vector subspace and let $U\subset\Prj(V)$ be a subset. Define the projective cone of $U$ to be $$\tilde{U}=\bigcup_{v\in U}v$$ Define the span of $U$ to be $$\langle U\rangle=\Prj(\text{span}(\tilde{U}))$$ the smallest projective linear subspace of $\Prj(V)$ containing $U$. 
\end{defn}

\begin{thm}{Dimemsion Theorem}{} Let $E,F\subset\Prj^n$ be projective linear subspaces. Then $$\dim(E\cap F)=\dim(E)+\dim(F)-\dim\langle E,F\rangle$$ \tcbline
\begin{proof}
Let 
\end{proof}
\end{thm}

\begin{thm}{}{} Any two distinct lines $L_1$ and $L_2$ in $\Prj^2$ intersect in a point. \tcbline
\begin{proof}
Note that $L_1$ and $L_2$ are projections of two planes $A_1,A_2$ through the origin. Since $A_1,A_2$ have dimension $2$ and are distinct. Thus their intersection must have dimension $1$ and their span has dimension $3$. But $\langle L_1,L_2\rangle=\Prj(\text{span}(A_1,A_2))=\Prj(\R^3)$ and thus $\dim(\langle L_1,L_2\rangle)=2$. By the above theorem, we must have $\dim(L_1\cap L_2)=0$ and thus it is exactly a point. 
\end{proof}
\end{thm}

\subsection{Projective Transformations}
\begin{defn}{Projective Transformations}{} A projective transformation of $\Prj^n$ is a map $T|_A:\Prj^n\to\Prj^n$ defined by $$T([x])=[Ax]$$ where $A\in\text{GL}(n+1,\R)$. 
\end{defn}

\begin{defn}{Projective General Linear Group}{} The projective general linear group of a vector space $V$ over $k$ with dimension $n$ is the group of all invertible linear transformations unique up to scalar multiplication. That is, we say that $T_1\sim T_2$ if $T_1=\lambda T_2$ for some $\lambda\in k$ and $\lambda\neq0$. It is denoted as $\text{PGL}(n+1,k)$. 
\end{defn}

\begin{defn}{Projective Frame of Reference}{} A projective frame of reference for $\Prj^n$ is an ordered set of $n+2$ points, $P_0,\dots,P_{n+1}\in\Prj^n$ such that any $n+1$ points are linearly independent. The standard frame of reference is just $[e_1],\dots,[e_{n+1}],[e_1+\dots+e_{n+1}]$. 
\end{defn}

\begin{prp}{}{} There is a bijection between projective transformations of $\Prj^n$ and projective frames of references of $\Prj^n$ as follows: $$\phi(T)=\{T([e_1]),\dots,T([e_{n+1}]),T([e_1+\dots+e_{n+1}])\}$$ where $T$ is the projective transformation. 
\end{prp}

This means that specifying any $n+2$ points in $\Prj^n$ such that they form a projective frame of reference induces a unique projective transformation. 

\subsection{Perspectivities}
\begin{defn}{Perspectivities}{} Let $\Pi_1,\Pi_2$ be two hyperplanes in $\Prj^n$. A perspectivity $f:\Pi_1\to\Pi_2$ from a point $O\in\Prj^n$ but not in $\Pi_1$ or $\Pi_2$ is a map given by $$f(P)=\Pi_2\cap\langle O,P\rangle$$
\end{defn}

\begin{lmm}{}{} Perspectivities are well defined. \tcbline
\begin{proof}
We want to show that $f(P)$ is a point. We appeal to the dimension theorem. We have that $$\dim(\Pi_2\cap\langle O,P\rangle)=\dim(O)+\dim(\Pi_2)-\dim(\langle\Pi_2\langle O,P\rangle\rangle)$$ Since $O$ is necessarily not in $\Pi_1$, we have $\langle O,P\rangle$ is a projective line. Moreover, since $O$ is not in $\Pi_2$, we must have that the span of the hyperplane $\Pi_2$ and a line $\langle O,P\rangle$ must be the projective space itself. Thus we have that $$\dim(\Pi_2\cap\langle O,P\rangle)=0$$ This means that $f(P)$ must be a point in projective space. 
\end{proof}
\end{lmm}

\begin{defn}{Cross Ratio}{} Let $P,Q,R,S$ be distinct and ordered points on a projective line in $\Prj^n$. Define the cross ratio between the four projective points to be the ratio $$(P,Q;R,S)=\left(\frac{p-r}{p-s}\right)\left(\frac{q-s}{q-r}\right)$$ where the ratio between vectors lying on the same projective line is defined to be the $\lambda$ in $\frac{\lambda v}{v}$. 
\end{defn}

\begin{prp}{}{} Projective linear maps perserve perspectivity. \tcbline
\begin{proof}
By linearity, we must have $$\frac{T(\lambda v)}{T(v)}=\frac{\lambda v}{v}$$
\end{proof}
\end{prp}

\begin{lmm}{}{} Perspectivities are projective linear maps. 
\end{lmm}

\subsection{Important Theorems}
\begin{defn}{Triangles in Projective Space}{} Let $P,Q,R\in\Prj^n$ be distinct. The triangle $\triangle PQR$ in $\Prj^n$ is defined to be the three points and the three sides spanned by three pair of points. 
\end{defn}

\begin{defn}{In Perspective from a Point}{} Two triangles $\triangle PQR$ and $\triangle P'Q'R'$ in $\Prj^n$ are said to be in persepective from a point $O$ if the lines passing through $PP'$, $QQ'$ and $RR'$ intersect at a $O$. 
\end{defn}

\begin{defn}{In Perspective from a Line}{} Two triangles $\triangle PQR$ and $\triangle P'Q'R'$ in $\Prj^n$ are said to be in persepective from a line $L$ if the points $PQ\cap P'Q'$, $QR\cap Q'R'$ and $PR\cap P'R'$ are collinear. 
\end{defn}

Note that this definition makes sense since any two lines in projective space must meet at one point. 

\begin{thm}{Desargues' Theorem}{} If $\triangle PQR$ and $\triangle P'Q'R'$ are two distinct triangles in $\Prj^n$ which are in perspective from a point, then they are also in perspective from a line. 
\end{thm}

\begin{thm}{Pappus' Theorem}{} Let $L$ and $L'$ be distinct projective lines in $\Prj^2$. Let $P,Q,R$ in $L$ but not in $L'$ and $P',Q',R'$ in $L'$ but not in $L$ all be distinct points. Then the intersection points $PQ'\cap P'Q$, $PR'\cap P'R$ and $QR'\cap Q'R$ are collinear. 
\end{thm}

\subsection{Axiomatic Projective Geometry}
Just as Euclidean geometry has $5$ axioms to follow, we can also establish axioms for projective geometry. 

\begin{defn}{Axiomatic Projective Geometry}{} An axiomatic projective plane $(P,L,I)$ consists of a set $P$ called the set of points, a set $L$, the set of lines and a relation $I\subseteq P\times L$, the incidence relation. For $l_1,l_2\in L$, define $$l_1\cap l_2=\{p\in P|p\in l_1, p\in l_2\}$$ These sets must satisfy the following four axioms:
\begin{enumerate}
\item Every line contains at least three distinct points: $$\forall l\in L\exists\text{ distinct }x,y,z\in P\text{ such that }(x,l)\in I, (y,l)\in I, (z,l)\in I$$
\item Every point is contained in at least three distinct lines: $$\forall x\in P\exists\text{ distinct }l,m,n\in L\text{ such that }(x,l)\in I, (x,m)\in I, (x,n)\in I$$
\item Any two points span a unique line: $$\forall x\neq y\in P\exists!l\in L\text{ such that }(x,l)\in I, (y,l)\in I$$
\item Any two distinct lines intesect at a unique point: $$\forall l\neq m\in I\exists!x\in P\text{ such that }(x,l)\in I, (x,m)\in I$$
\end{enumerate}
\end{defn}
\end{document}