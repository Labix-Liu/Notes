\section{Natural Numbers}
\subsection{Order of Natural Numbers}
\begin{defn}{Natural Numebers}{} The set of natural numbers $\N$, formulated in ZFC set theory via peano axioms, is the set $$\N=\{0,1,2,\dots\}$$ of natural numbers. Addition and multiplication is also defined. 
\end{defn}

\begin{defn}{Order}{} Let $a,b\in\mathbb{N}$. We say $a<b$ if there exists some $c\in\mathbb{N}$ and $c\neq0$ so that $a+c=b$. $<$ is a relation in the set $\mathbb{N}$. 
\end{defn}

\begin{prp}{Trichotomy}{} Let $a,b\in\mathbb{N}$. Then either $a=b$ or $a<b$ or $b<a$. 
\end{prp}
\begin{proof} From set theory, we have that $a\in b$ or $a=b$ or $b\in a$, which corresponds to $a<b$, $a=b$, $b<a$ respectively. 
\end{proof}

\begin{prp}{}{} Suppose that $a,b,c\in\mathbb{N}$. The relation $<$ has the below properties. 
\begin{itemize}
\item $a<b$ and $b<c\implies a<c$
\item $a<b\implies a+c<b+c$
\item $a<b\implies ac<bc$
\end{itemize}
\end{prp}
\begin{proof} We prove the three using the definition of $<$. 
\begin{itemize}
\item Suppose $a<b$ and $b<c$. There exists some $x,y\in\mathbb{N}$ such that $a+x=b$ and $b+y=c$. Then $a+x+y=c$ thus $a<c$
\item Suppose $a<b$. Then $a+c=b$ for some $c\in\mathbb{N}$. Then $b+d=(a+c)+d=(a+d)+c$ thus $a+d<b+d$. 
\item Suppose $a<b$. Then $a+d=b$ for some $d\in\mathbb{N}$. Then $ac+dc=bc$ thus $ac<bc$
\end{itemize}
\end{proof}

\begin{defn}{Partial Order}{} Let $a,b\in\mathbb{N}$. We say $a\leq b$ if either $a<b$ or $a=b$. 
\end{defn}

\begin{thm}{}{} The relation $\leq$ in the natural numbers are partial order. 
\end{thm}
\begin{proof} Recall from set theory that a partial order is a relation such that it is reflexive, antisymmetric and transitive. 
\begin{itemize}
\item Since we have $a\leq a$, $\leq$ is reflexive. 
\item $a\leq b$ and $b\leq a\implies a=b$ by the trichotomy of natural numbers. 
\item $a\leq b$ and $b\leq c\implies a\leq c$ from the properties of the relation $<$. 
\end{itemize}
\end{proof}

\begin{thm}{}{} The set of natural numbers is totally ordered. 
\end{thm}
\begin{proof} For any two numbers in $\mathbb{N}$, we have the trichotomy, thus we have either $a\leq b$ or $b\leq a$ for all $a,b\in\mathbb{N}$. 
\end{proof}

\pagebreak
\section{Integers}
\subsection{Introduction to Integers}
\begin{defn}{Integers}{} For every natural number $n$ except $0$, we introduce a number $-n$ such that $n+(-n)=0$. It is called the additive inverse of $n$. The set of all natural numbers and additive inverses is the set $$\mathbb{Z}=\{\dots,-3,-2,-1,0,1,2,3,\dots\}$$
\end{defn}

\begin{defn}{Subtraction}{} We define subtraction of $a,b\in\Z$ to be $a-b$, which is $a+(-b)$. 
\end{defn}

\begin{prp}{}{} The set of integers is totally ordered. 
\end{prp}

\subsection{Divisibility}
We begin our study of number theory with divisibility. 
\begin{defn}{}{}[Divisibility] Let $a,b\in\Z$. We define the relation $$a|b$$ if and only if there exists some $k\in\Z$ such that $b=ak$. We say that $a$ divides $b$ in this case. 
\end{defn}
The definition is vey simple. The intuition is straight forward as well. Savour this moment as the subject increases its difficulty exponentially.  

\begin{prp}{}{} Let $d,m,n\in\Z$. The relation $|$ has the following properties and thus is a partial order in $\mathbb{N}$. 
\begin{itemize}
\item (Reflexivity) $n|n$
\item (Antisymmetry) $m|n$ and $n|m\implies m=n$
\item (Transitivity) $d|n$ and $n|m\implies d|m$
\item (Linearity) $d|n$ and $d|m\implies d|(an+bm)$ for any $a,b\in\Z$
\item $1|n$
\item $n|0$
\end{itemize}
\end{prp}
\begin{proof} We prove antisymmetry and transitivity and leave the others for the reader. Let $m,n,d\in\Z$. 
\begin{itemize}
\item (Antisymmetry) If $m|n$ and $n|m$ then there exists some $k_1,k_2\in\mathbb{N}$ such that $n=k_1m$ and $m=k_2n$ thus $n=k_1k_2n$. Then $k_1k_2=1\implies k_1=k_2=1$ and $m=n$
\item (Linearity) If $d|n$ and $n|m$ then there exists $k_1k_2\in\mathbb{N}$ such that $n=k_1d$ and $m=k_2n$. Then $m=k_2k_1d$ thus $d|m$
\end{itemize}
\end{proof}

These properties will come up again and again and will be the foundation of number theory. It is safe to say that number theory is built upon the notion of divisibility. 

\subsection{The Division Algorithm}
This section is dedicated to develop the Euclidean algorithm, a means to find the greatest common divisor. The gcd is a central notion in number theory as well. 
\begin{defn}{}{}[Greatest Common Divisor] Suppose that $m,n\in\Z$. A number $d\in\mathbb{N}$ such that 
\begin{itemize}
\item $d\geq 0$
\item $d|m$ and $d|n$
\item $e|a$ and $e|b\implies e|d$
\end{itemize}
is called the greatest common divisor of $m$ and $n$, denoted $\gcd{(m,n)}$.
\end{defn}

In contrast to the greatest common divisor, we also have the lowest common multiple. Although they work as a pair, we often see the notion of gcd come up more than lcm. 

\begin{defn}{}{}[Lowest Common Multiple] Suppose that $m,n\in\Z$. A number $l\in\mathbb{N}$ such that 
\begin{itemize}
\item $l\geq 0$
\item $m|l$ and $n|l$
\item $m|e$ and $n|e\implies l|e$
\end{itemize}
is called the lowest common multiple of $m$ and $n$, denoted $\lcm{(m,n)}$.
\end{defn}

Beware that both of these definitions does not imply the uniqueness of such a number. However, with a little work, we will see that both of them are indeed unique. Readers should think about whether the existence of these numbers is guaranteed as well. 

\begin{prp}{}{} Let $m,n\in\Z$. $\gcd(m,n)$ and $\lcm(m,n)$ are unique. 
\end{prp}
\begin{proof}
By the third property of both numbers, we must have if $c,d$ are $\gcd(m,n)$/$\lcm(m,n)$, then $c|d$ and $d|c$ thus $c=d$ and $\gcd(m,n)$/$\lcm(m,n)$ is unique. 
\end{proof}

We will see more on gcd and lcm when we deal with factorization. For now, we turn our heads to the division algorithm. This algorithm proves to us that upon dividing two integers, as long as they are not divisible by one or the other, you can always guarantee a remainder smaller than the divident. 

\begin{thm}{}{}[The Division Algorithm] Let $a\in\N$ and $b\in\Z$ with $b\neq 0$. Then there exists unique $q,r\in\Z$ such that $$b=aq+r$$ with $0\leq r<a$. 
\end{thm}
\begin{proof}
We prove existence first by considering three cases. \\
Cases 1: $b$ is divisible by $a$.  If $b$ is divisible by $a$ then there exists $k\in\Z$ such that $b=ka$ thus $k=q$ and $r=0$. \\~\\
Case 2: $b$ is positive and $a$ does not divide $b$. Let $$S=\{b-ka\in\N|k\in\N\}$$ Then $S\subseteq\N$ thus we can apply the well-ordering principle to $S$. Let $r$ be the least natural number in $S$. Then $r\in S$ implies $r=b-ka$ for some $k\in\N$. Thus $b=ka+r$ for some $k$ and $r$. We show that $r<a$. Suppose for a contradiction that $r\geq a$. Then $u=r-a\in\N$ and $$b=ka+r\implies b=ka+(u-a)\implies b=(k-1)a+u$$ thus $u\in S$ and $u<r$, contradicting the fact that $r$ is the least element in $S$. Thus $r\leq a$. If $r=a$, then $$b=ka+a\implies b=(k+1)a$$ which means that $a|b$ which is false in our case. Thus we must have $r<a$. \\~\\
Case 3: $b$ is negative and $a$ does not divide $b$. Then apply the exact same argument to the number $-b$ to get $(-b)=ka+r$ and $b=-ka-r$. Let $k'=-k-1$ and $r'=-r+a$. Then $$b=-ka-r=k'a+a+r'-a=k'a+r'$$ Since we have $0\leq r<a$, we have $-a<-r\leq 0$ and $0<r'\leq a$. Again $r'\neq a$ or else $a|b$ which contradicts our assumption. \\~\\
We now prove uniqueness. Suppose that $b=aq_1+r_1$ and $b=aq_2+r_2$. Then $r_1-r_2=a(q_2-q_1)$. We know that $-a<r_1-r_2<a$ thus $-a<a(q_2-q_1)<a$ and $-1<q_2-q_1<1$ which is impossible for integers $q_1,q_2$ unless $q_1=q_2$. If $q_1=q_2$ then $r_1=r_2$ and we are done. 
\end{proof}

The division algorithm does not require $b$ to be larger than $a$. In fact, if $a$ is larger than $b$, then the division algorithm simply gives $a$ itself as the remainder. Before we reach our conclusion, we need one more proposition. 

\begin{prp}{}{} Suppose that $m\geq n>0$ are natural numbers with $m=qn+r$ for some $q,r\in\mathbb{N}$. Then $$\gcd{(m,n)}=\gcd{(n,r)}$$
\end{prp}
\begin{proof}
Suppose that $d=\gcd(m,n)$. Then we know that $d<n$ from definition. We want to show that $d$ satisfies the three results of a gcd but in terms of $n$ and $r$. Since $d|n$ and $d|m$, by linearity we must have $d|r$. \\~\\
Now suppose for a contradiction that there exists $e$ such that $e$ is a common divisor of $n$ and $r$ and $e>d$. Then $e|n$ and $e|r$ by definition thus $e|m$ by linearity. $e|m$ and $e|n$ implies that $e$ is a larger common divisor of $m$ and $n$ than $d$. However this is not possible since $d$ is assumed to be the largest among the common divisors. This is a contradiction thus $d=\gcd(n,r)$ and we are done. 
\end{proof}

\begin{thm}{}{}[Euclid's Algorithm] Suppose that $m\geq n>0$ are natural numbers. We have the following inequalities. 
$$m=nq_1+r_1\text{ with }0<r_1<n$$
$$n=r_1q_2+r_2\text{ with }0<r_2<n$$
$$r_1=r_2q_3+r_3\text{ with }0<r_3<n$$
$$\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots$$
$$r_{k-2}=r_{k-1}q_k+r_k\text{ with }0<r_k<r_{k-1}$$
$$r_{k-1}=r_kq_{k-1}$$
From this, we have $r_k|r_{k-1}$, $r_k|r_{k-2}\dots r_k|n$ and $r_k|m$. 
\end{thm}
\begin{proof} The first part of the results is due to the repeated use of the division algorithm. For the second part, we have $$\gcd(m,n)=\gcd(n,r_1)=\gcd(r_1,r_2)=\dots=\gcd(r_{k-1},r_k)=r_k$$ and we are done. 
\end{proof}

\begin{lmm}{}{}[Bezout's Lemma] Let $a,b\in\Z$ such that they are not both $0$. Then there exists $x,y\in\Z$ such that $$ax+by=\gcd(a,b)$$
\end{lmm}
\begin{proof} Reconstruct $x$ and $y$ using the Euclidean Algorithm. This is possible since $\gcd(m,n)=r_k$ and every $r_1,\dots,r_{k-1}$ has a factor of $r_k$ in it. 
\end{proof}

\begin{lmm}{}{} Let $a,b\in\Z$ such that they are not both $0$. Then the equation $$ax+by=\gcd(a,b)$$ has an infinite number of integer solutions. 
\end{lmm}
\begin{proof} Using Bezout's Lemma, we conclude that $(x_0,y_0)$ is a solution to the equation. But then $$(x_0-bt,y+at)$$ are also solutions for $t\in\Z$ since $$a(x_0-bt)+b(y+at)=ax+by=\gcd(a,b)$$
\end{proof}

\begin{crl} Let $a,b\in\Z$ such that they are not both $0$ and $d\in\Z$. Then $d$ divides $a$ and $b$ if and only if $d|\gcd(a,b)$. 
\end{crl}

\subsection{Unique Factorization}
\begin{defn}{}{}[Prime Numbers] We say that $n\in\mathbb{N}$ is a prime number if and only if it has exactly two factors, which is $1$ and $n$. Else $n$ is composite. 
\end{defn}

\begin{lmm}{}{} Every integer is divisible by a prime. 
\end{lmm}

\begin{lmm}{}{} Every integer $n>1$ can be written as a product of primes. 
\end{lmm}

\begin{thm}{}{} There is an infinite number of primes. 
\end{thm}

\begin{prp}{}{}[Euclid's Lemma] Suppose that $p,m,n\in\mathbb{N}$, with $p$ prime and $m,n>1$. Suppose that $p|mn$. Then $p$ divides at least one of $m$ or $n$. 
\end{prp}

\begin{prp}{}{} Suppose that $p$ is a prime such that $p|a_1a_2\cdots a_k$. Then $p|a_i$ for some $i\in\{1,2,\dots,k\}$
\end{prp}

\begin{thm}{}{}[Fundamental Theorem of Arithmetic] Suppose that $n\neq 0$ is a natural number. Then there exists exactly one prime factorization for every $n$, meaning that the decomposition $$n=\prod_{k=1}^np_k^{s_k}$$ where $p_k$ is prime exists and is unique. 
\end{thm}

\begin{thm}{}{} Suppose that $m,n\in\mathbb{N}$. Suppose that 
$$m=p_1^{\alpha_1}p_2^{\alpha_2}\cdots p_r^{\alpha_r}$$
$$n=p_1^{\beta_1}p_2^{\beta_2}\cdots p_q^{\beta_q}$$
with $p_1=2$, $p_2=3$, $p_3=5\dots$. Without loss of generality $r\leq q$. Then $$\gcd{(m,n)}=p_1^{\min{(\alpha_1,\beta_1)}}p_2^{\min{(\alpha_2,\beta_2)}}\cdots p_q^{\min{(\alpha_q,\beta_q)}}$$
$$\lcm{(m,n)}=p_1^{\max{(\alpha_1,\beta_1)}}p_2^{\max{(\alpha_2,\beta_2)}}\cdots p_q^{\max{(\alpha_q,\beta_q)}}$$
\end{thm}

\begin{thm}{}{} Suppose that $m$ and $n$ are natural numbers. Then $$\gcd{(m,n)}\times\lcm{(m,n)}=m\times n$$
\end{thm}
\begin{proof} Since $\min\{a,b\}\cdot\max\{a,b\}=ab$, from the above theorem, we have that $\gcd{(m,n)}\times\lcm{(m,n)}=m\times n$ and we are done. 
\end{proof}

\pagebreak
\section{Rational Numbers}
We now produce multiplicative inverses to form the the set of rationals. 
\subsection{Introduction to Rationals}
\begin{defn}{}{} For every integer $n\in\Z$ except $0$ and $1$, we introduce a number $n^{-1}$ such that $n\cdot n^{-1}=1$. It is called the multiplicative inverse of $n$. The set of all combination of integers and multiplicative inverses is the set $$\Q=\{\frac{a}{b}|a,b\in\Z,b\neq 0\}$$ Numbers in $\Q$ are called rational numbers. 
\end{defn}

\begin{defn}{Divsion}{} We define division of $a$ and $b$ to be $\frac{a}{b}$, which is $a\cdot b^{-1}$. 
\end{defn}

\begin{defn}{Reduced Form}{} Suppose that $a\in\mathbb{Q}$. $a=\frac{r}{s}$ is a reduced form if
\begin{itemize}
\item $s>0$
\item $\gcd{(r,s)}=1$
\end{itemize}
\end{defn}

\begin{thm}{}{} For every $x\in\mathbb{Q}$, $x$ has exactly one reduced form. 
\end{thm}

\subsection{Arithmetic and Order of Rationals}
\begin{defn}{}{} We define equality $\frac{a}{b}=\frac{c}{d}$ if and only if $ad=bc$. 
\end{defn}

\begin{defn}{}{}[Arithmetic of Rationals] We define the four basic operators on rationals as follows. 
\begin{itemize}
\item $\frac{a}{b}+\frac{c}{d}=\frac{ad+bc}{bd}$
\item $\frac{a}{b}-\frac{c}{d}=\frac{ad-bc}{bd}$
\item $\frac{a}{b}\cdot\frac{c}{d}=\frac{ac}{bd}$
\item $\frac{a}{b}/\frac{c}{d}=\frac{ad}{bc}$
\end{itemize}
\end{defn}

\begin{prp}{}{} The additive inverse of $\frac{a}{b}$ is $-\frac{a}{b}$. Its multiplicative inverse is $\frac{b}{a}$
\end{prp}

\pagebreak
\section{Real Numbers}
\subsection{Dedekind Cuts}
\begin{defn}{Dedekind Cuts}{} A dedekind cut is a partition of $\mathbb{Q}$ into two subsets $A,B$ such that
\begin{itemize}
\item $A$ is non-empty
\item $A\neq\mathbb{Q}$
\item $x,y\in\mathbb{Q}$ and $x<y$ and $y\in A\implies x\in A$
\item $x\in A\implies$ there exists a $y\in A$ such that $y>x$
\end{itemize}
We use $A$ to denote this cut since $B$ is determined by $A$
\end{defn}

\begin{defn}{Order}{} If $A,B$ are dedekind cuts then we say that $A<B$ if and only if $A\subset B$. 
\end{defn}

\begin{defn}{Real Numbers}{} We define the set of real numbers $\mathbb{R}$ as the set of all dedekind cuts of $\mathbb{Q}$. 
\end{defn}

\begin{prp}{}{} Define addition, subtraction, multiplication, division as follows. Then the resulting set is also a dedekind cut. 
\begin{itemize}
\item $A+B=\{a+b:a\in A\text{ and }b\in B\}$
\item $A-B=\{a-b:a\in A\text{ and }b\in \mathbb{Q}\setminus B\}$
\item $A\times B=\{a\times b:a\in A\text{ and }b\in B\}$ if $A,B\geq 0$ or $A,B\leq 0$. If at one of $A,B<0$ then use the identity $-(-A\times B)$ or $-(A\times -B)$ depending on whether $A<0$ or $B<0$ respectively. 
\item $A/B=\{a/b:a\in A\text{ and }b\in \mathbb{Q}\setminus B\}$ if $A,B\geq 0$ or $A,B\leq 0$. Use the similar approach as multiplication when one of $A,B<0$. 
\end{itemize}
\end{prp}

\begin{prp}{}{} The set of all rational numbers $\mathbb{Q}$ is a subset of the real numbers $\mathbb{R}$. 
\end{prp}
\begin{proof} We define $A=\{x\in\mathbb{Q}:x<q\}$ for every $q\in\mathbb{Q}$. Thus the set $A$ satisfies a dedekind cut. 
\end{proof}

\begin{defn}{Irrational Numbers}{} Any dedekind cut which is not a rational number is called an irrational number. 
\end{defn}

\begin{thm}{}{} There exists an irrational number. \tcbline
\begin{proof} We want to show that there is an irrational number represented by a dedekind cut such that its square is 2. Consider the set $A=\{x\in\mathbb{Q}:x<0\text{ or }x^2<2\}$. $A$ is non-empty since $0\in A$. $A\neq\mathbb{Q}$ since $3\notin A$. Suppose $p\in A$. We then need to show that $q\in A$ whenever $q<p$. When $0\leq q<p$, we have $0\leq q^2<p^2$ from ordering of rationals. When $q<0$, then $q\in A$ by definition of $A$. Thus this is true. Now we need to show that there is always a rational $q$ larger than $p$ which is in $A$. Choose $q=\frac{2p+2}{p+2}$, then $p<q$ and $q^2<2$. Thus $A$ is a dedekind cut. \linebreak\linebreak
Now consider $A\times A$. We have $A\times A\leq 2$ since for all $x,y\in A$, we have $x^2<2$ and $y^2<2$ and thus $xy<2$ whenever $xy\geq 0$. Thus the set $A\times A=\{r\in\mathbb{Q}:r<0\text{ or }r=xy\text{ for some }x,y\in A\text{ and }x,y>0\}$ is less than or equal to 2. We know that $A\times A$ is a dedekind cut. But we want to know if $A\times A$ represents the number $2$. Suppose that $u\in A\times A$. Then we know that from $A$, there exists a number $v\in A$ such that $u<v^2<2$. And this applies for every $u$. Then we know that $A\times A=2$ since $A\times A=\{x\in\mathbb{Q}:x<2\}$, which is our definition of rational numbers with dedekind cut. \linebreak\linebreak
We have proved that there exists a dedekind cut such that its square is 2. But is that dedekind cut irrational? We now represent $A$ with $\sqrt2$. Suppose that $\sqrt2$ is rational. Then we can write it is as $\frac{m}{n}$ in reduced form. Then we have $2n^2=m^2$. Then $2|m^2$ thus $2|m$. Let $m=2k$ for some $k\in\mathbb{N}$. Then $2k^2=n^2$ which similarly implies that $2|n$. This contradicts the fact that $\frac{m}{n}$ is in reduced form, thus $\sqrt{2}$ is in fact not rational, and is an irrational number. 
\end{proof}
\end{thm}

\begin{prp}{}{} Suppose that $A,B,C$ are dedekind cuts. 
\begin{itemize}
\item (O1) $A<B$ or $A=B$ or $B<A$
\item (O2) $A<B$ and $A<C\implies A<C$
\item (O3) $A<B\implies A+C<B+C$
\item (O4) $A<B$ and $z>0\implies AC<BC$
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $x$, $y$, $z\in\mathbb{R}$. 
\begin{itemize}
\item (A1) $x+y\in\mathbb{R}$
\item (A2) $(x+y)+z=x+(y+z)$
\item (A3) $\exists 0\in\mathbb{R}$ such that $x+0=0=0+x$
\item (A4) $x+y=y+x$
\item (A5) $\exists(-x)\in\mathbb{R}$ such that $x+(-x)=0=(-x)+x$
\item (M1) $xy\in\mathbb{R}$
\item (M2) $(xy)z=x(yz)$
\item (M3) $\exists 1\in\mathbb{R}$ such that $x\cdot1=x=1\cdot x$
\item (M4) $xy=yx$
\item (M5) $\exists (x^{-1})\in\mathbb{R}$ such that $x(x^{-1})=1=(x^{-1})x$
\item (D1) $x(y+z)=xy+xz$
\item (O1) $x<y$ or $x=y$ or $y<x$
\item (O2) $x<y$ and $y<z\implies x<z$
\item (O3) $x<y\implies x+z<y+z$
\item (O4) $x<y$ and $z>0\implies xz<yz$
\end{itemize}
\end{prp}

The absolute value is an important function when it comes to defining useful concepts such as distances in the field of real. 

\begin{defn}{}{}[The Absolute Value] The absolute value of a real number $x$ is defined by
$$|x|=\begin{cases}
x & \text{if $x\geq0$} \\
-x & \text{if $x\leq0$}
\end{cases}$$
\end{defn}

The absolute value has some properties that are extremely useful in certain circumstances, notably number 4 and 5. 

\begin{prp}{}{} The absolute Value has the folowing properties
\begin{enumerate}
\item $|x|\geq0$
\item $|xy|=|x||y|$
\item $\abs*{\frac{x}{y}}=\frac{|x|}{|y|}$
\item $|x+y|\leq|x|+|y|$
\item $\abs*{|x|-|y|}\leq|x-y|$
\end{enumerate}
\end{prp}
\begin{proof}  I left out the proofs of (2) and (3) since they are simplay obtained via case by case analysis. 
\begin{enumerate}
\item When $x\geq0$ we have $\abs{x}=x\geq0$. When $x<0$ we have $\abs{x}=-x>0$
\item We start by squaring the left hand side of the inequality. 
\begin{align*}
\abs{x+y}^2&=(x+y)^2 \\
&=x^2+2xy+y^2 \\
&\leq \abs{x}^2+2\abs{x}\abs{y}+\abs{y}^2 \\
&=(\abs{x}+\abs{y})^2
\end{align*}
Since the both sides of the inequality is non-negative, we can take the square root on both sides, thus obtaining $|x+y|\leq|x|+|y|$. 
\item Choose $x$ to be $x-y$ in (4) and we obtain $\abs{x}-\abs{y}\leq\abs{x-y}$. Similarly, choosing $y$ to be $y-x$ in (4), we find that $\abs{y}-\abs{x}\leq\abs{y-x}=\abs{x-y}$. Thus we have $\abs*{|x|-|y|}\leq|x-y|$. 
\end{enumerate}
\end{proof}

\subsection{The Binomial Theorem}
\begin{defn}{The Binomial Coefficient}{} Let $n,r\in\mathbb{N}$ with $n>0$. We define the binomial coefficient $\binom{n}{r}$ to mean the number $\frac{n!}{r!(n-r)!}$ when $r\leq n$. When $r>m$ then $\binom{n}{r}=0$. 
\end{defn}

\begin{prp}{}{} Let $n,r\in\mathbb{N}$ with $0<r<n$, we have $\binom{n}{r}=\binom{n}{n-r}$. 
\end{prp}

\begin{prp}{}{} Let $n,r\in\mathbb{N}$ with $0<r<n$, we have $\binom{n}{r-1}+\binom{n}{r}=\binom{n+1}{r}$. 
\end{prp}

\begin{thm}{The Binomial Theorem}{} Suppose $a,b\in\mathbb{R}$. Then $$(a+b)^n=\sum_{k=0}^n\binom{n}{k}a^{n-k}b^k$$
\end{thm}

\begin{thm}{}{}[Vandermonde's Theorem] Suppose that $a,b,n\in\mathbb{N}$. Then $$\binom{a+b}{n}=\sum_{k=0}^{n}\binom{a}{k}\binom{b}{n-k}$$
\end{thm}

\pagebreak
\section{Complex Numbers}
\subsection{Introduction to Complex Numbers}
\begin{defn}{Complex Numbers}{} Define the number $i=\sqrt{-1}$. Define $z=a+bi$ to be a complex number. Every complex number is uniquely determined by an ordered pair $(a,b)\in\mathbb{R}^2$. $a=\Re(z)$ is called the real part of $z$. $b=\Im(z)$ is called the imaginary part of $z$. The set of all complex numbers is denoted $\C$. 
\end{defn}

\begin{defn}{Equality in Complex Numbers}{} We define the relation equality in $\C$ as $z_1=z_2$ with $z_1=a+bi$ and $z_2=c+di$ if and only if $a=c$ and $b=d$. 
\end{defn}

\begin{defn}{Addition and Multiplication}{} Let $z=a+bi$, $w=c+di$. We define the $+:\C\times\C\to\C$ and $\cdot:\C\times\C\to\C$ in $\C$ as follows. 
\begin{itemize}
\item $z+w=(a+c)+(b+d)i$
\item $zw=(ac-bd)+(ad+bc)i$
\end{itemize}
\end{defn}

\begin{prp}{}{} The set of complex numbers $(\C,+,\cdot)$ is a field. In particular, let $x,y,z\in\C$. Then\\~\\
$(\C,+)$ is a commutative group. 
\begin{itemize}
\item $x+y\in\C$
\item $(x+y)+z=x+(y+z)$
\item $\exists 0\in\C$ such that $x+0=0=0+x$
\item $\exists(-x)\in\C$ such that $x+(-x)=0=(-x)+x$
\item $x+y=y+x$
\end{itemize}
$(\C/\{0\},\cdot)$ is a commutative group. 
\begin{itemize}
\item $xy\in\C$
\item $(xy)z=x(yz)$
\item $\exists 1\in\C$ such that $x\cdot1=x=1\cdot x$
\item $\exists (x^{-1})\in\C$ such that $x(x^{-1})=1=(x^{-1})x$ when $x\neq0$
\item $xy=yx$
\end{itemize}
The distributive laws hold. 
\begin{itemize}
\item $x(y+z)=xy+xz$
\item $(x+y)z=xz+yz$
\end{itemize}
\end{prp}

\subsection{Conjugates}
\begin{defn}{Conjugation}{} For every complex number $z=a+bi$ there exists a conjugate $\bar{z}=a-bi$
\end{defn}

\begin{prp}{}{} Suppose that $z,w\in\C$. 
\begin{itemize}
\item $\overline{\overline{z}}=z$
\item $\overline{z+w}=\overline{z}+\overline{w}$
\item $\overline{zw}=\overline{z}\overline{w}$
\end{itemize}
\end{prp}


\subsection{Modulus and Argument}
\begin{defn}{Modulus}{} Let $z\in\C$. Define the modulus of $z=a+bi$ to be $$\abs{z}=\sqrt{a^2+b^2}$$
\end{defn}

\begin{prp}{}{} Suppose that $z,w\in\C$. Then the following are true for the modulus. 
\begin{itemize}
\item $\abs{z}^2=\abs{z}\abs{\bar{z}}$
\item $\abs{\bar{z}}=\abs{z}$
\item $\abs{zw}=\abs{z}\abs{w}$
\item $z\bar{z}=\abs{z}^2$
\item $\abs{z+w}\leq\abs{z}+\abs{w}$
\item $\abs{z-w}=\abs*{\abs{z}-\abs{w}}$
\end{itemize}
\end{prp}

\begin{defn}{Argument}{} Let $z=a+bi$. Define the argument of $z$ to be $$\arg(z)=\{\theta\in\R|z=\cos(\theta)+i\sin(\theta)\}$$~\\
The principal argument of $z$ is defined to be the $\theta\in\arg(z)$ such that $-\pi<\theta\leq\pi$, denoted Arg$(z)$. 
\end{defn}

\begin{prp}{}{} Suppose that $z,w\in\C$. Then the following are true for the argument of a complex number. 
\begin{itemize}
\item $\arg(z)=\{\text{Arg}(z)+2\pi k|k\in\Z\}$
\item $\arg{(zw)}=\arg{(z)}+\arg{(w)}=\{\theta+\phi|\theta\in\arg(z), \phi\in\arg(w)\}$
\item $\arg{(\bar{z})}=-\arg{(z)}$
\end{itemize}
\end{prp}

\begin{defn}{Polar Form}{} Using the modulus and the argument, a complex number can be uniquely determined by $\abs{z}$ and $\arg{z}$. It can be written as $z=r(\cos{(\theta)}+i\sin{(\theta)})$ where $r=\abs{z}$ and $\cos{(\theta)}=\frac{a}{a^2+b^2}\text{  and  }\sin{(\theta)}=\frac{b}{a^2+b^2}$ with $-\pi<\theta\leq\pi$ This is the polar form of a complex number. 
\end{defn}

\begin{prp}{}{} Suppose that $z=r(\cos{(\theta)}+i\sin{(\theta)})$ and $w=s(\cos{(\phi)}+i\sin{(\phi)})$. 
\begin{itemize}
\item $zw=rs(\cos{(\theta+\phi)}+i\sin{(\theta+\phi)})$
\item $\frac{1}{z}=\frac{1}{r}(\cos{(-\theta)}+i\sin{(-\theta)})$
\item $\frac{z}{w}=\frac{r}{s}(\cos{(\theta-\phi)}+i\sin{(\theta-\phi)})$
\item $\bar{z}=r(\cos{(-\theta)}+i\sin{(-\theta)})$
\end{itemize}
\end{prp}

\begin{thm}{De Moivre's Theorem}{} Suppose that $r\in\mathbb{Q}$. Then $(\cos{\theta}+i\sin{\theta})^r=\cos{(r\theta)}+i\sin{(r\theta)}$
\end{thm}

\subsection{Roots of Complex Numbers}
\begin{defn}{$n$th Roots}{} Let $w\neq0$ be a complex number and $n$ a positive integer. A number $z$ is called the $n$th root of $w$ if and only if $z^n=w$. 
\end{defn}

\begin{thm}{Roots of Unity}{} Suppose that $z=re^{i\theta}$. Then the $n$th roots of $z$ are $$r^{\frac{1}{n}}\left[\cos{\left(\frac{(\theta+2\pi k)i}{n}\right)}+i\sin{\left(\frac{(\theta+2\pi k)i}{n}\right)}\right]$$ where $k=0,1,\dots,n-1$. 
\end{thm}

\pagebreak
\section{Algebraic Inequalities}
\begin{thm}{Bernoulli's Inequality}{} For all $x\geq-1$ and $n\in\N$, $$(1+x)^n\geq1+nx$$\tcbline
\begin{proof} We prove the inequality by induction on $n$. In the case of $n=1$, we have $1+x\geq1+x$, which is true for all $x$. Now suppose that the inequality works for some $n\in\N$. We have 
\begin{align*}
(1+x)^{n+1}&\geq(1+x)(1+nx) \tag{Induction Hypothesis and $x\geq-1$} \\
&=1+(n+1)x+nx^2 \\
&\geq1+(n+1)x \tag{since $x^2\geq0$}
\end{align*}
Thus we have the Bernoulli's Inequality by the principle of mathematical induction. 
\end{proof}
\end{thm}

\begin{thm}{Weierstrass' Inequality}{} Let $a_1,\dots,a_n$ be positive numbers. Then when $n\geq2$, $$(1+a_1)\dots(1+a_n)>1+a_1+\dots+a_n$$
\end{thm}

\begin{thm}{AMGM}{} Let $a_1,\dots,a_n$ be positive numbers. Then $$\frac{a_1+\dots+a_n}{n}\geq(a_1a_2\dots a_n)^\frac{1}{n}$$
\end{thm}

\begin{thm}{Cauchy-Schwarz Inequality}{} Let $x_1,\dots,x_n,y_1\dots,y_n\in\R$. Then $$\left(\sum_{k=1}^nx_ky_k\right)\leq\left(\sum_{k=1}^nx_k^2\right)\left(\sum_{k=1}^ny_k^2\right)$$
\end{thm}

\begin{thm}{Tchbychef's Inequality}{} Let $x_1,\dots,x_n,y_1\dots,y_n\in\R$ such that $x_1\leq x_2\leq\dots\leq x_n$ and $y_1\leq y_2\leq\dots\leq y_n$. Then $$n\left(\sum_{k=1}^n x_ky_k\right)\geq\left(\sum_{k=1}^nx_k\right)\left(\sum_{k=1}^ny_k\right)$$
\end{thm}

\pagebreak
\section{Basics of Matrices}
\subsection{Matrices and its Operations}
\begin{defn}{Matrix}{} A rectangular array of $m\times n$ real numbers, called the elements, or entries, $$A=
\begin{pmatrix}
a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{m1}&a_{m2}&\cdots&a_{mn}
\end{pmatrix}$$ is called an $m\times n$ matrix over $\R$. For $i=1,\dots,m$, let $$r_i=
\begin{pmatrix}
a_{i1}&a_{i2}&\cdot&a_{in}
\end{pmatrix}$$ and for $j=1,\dots,n$, let $$c_j=
\begin{pmatrix}
a_{1j}\\
a_{2j}\\
\vdots\\
a_{mj}
\end{pmatrix}$$ then $r_i$ is called the $i$th row of $A$ and $c_j$ is called the $j$th row of $A$. The element of $A$ at the intersection of the $i$th row and $j$th column is called the $(i,j)$th entry of $A$. The set of all $m\times n$ matrices over $\R$ is denoted by $M_{m\times n}(\R)$. We sometimes denote $A$ as $(a_{i,j})_{m\times n}$
\end{defn}

\begin{defn}{Matrix Addition}{} Let $A,B$ be $m\times n$ matrices. We define the binary operation $+:M_{m\times n}(\R)\times M_{m\times n}(\R)\to M_{m\times n}(\R)$ to be $$A+B=
\begin{pmatrix}
a_{11}+b_{11}&a_{12}+b_{12}&\cdots&a_{1n}+b_{1n}\\
a_{21}+b_{21}&a_{22}+b_{22}&\cdots&a_{2n}+b_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{m1}+b_{m1}&a_{m2}+b_{m2}&\cdots&a_{mn}+b_{mn}
\end{pmatrix}$$
\end{defn}

\begin{prp}{}{} Let $A,B,C\in M_{m\times n}(\R)$. Then 
\begin{itemize}
\item $(A+B)+C=A+(B+C)$
\item $(A+B)=(B+A)$
\item $A+0=0+A=A$
\item There exists a unique $M\in M_{m\times n}(\R)$ such that $A+M=M+A=0$
\end{itemize}
\end{prp}
\begin{proof} Addition is associative, commutative and has an identity in $\R$. 
\end{proof}

\begin{defn}{Scalar Multiplication}{} Let $A=(a_{i,j})_{m\times n}$ and $\lambda\in\R$. We define the scalar multiple $\cdot:\R\times M_{m\times n}(\R)\to M_{m\times n}(\R)$ as $\lambda A=(\lambda a_{i,j})_{m\times n}$. 
\end{defn}

\begin{prp}{}{} Let $A,B\in M_{m\times n}(\R)$ and $\lambda,\mu\in\R$. Then
\begin{itemize}
\item $(\lambda\mu)A=\lambda(\mu A)$
\item $\lambda(A+B)=\lambda A+\lambda B$
\item $(\lambda+\mu)A=\lambda A+\mu A$
\end{itemize}
\end{prp}
\begin{proof} Simple proof by using the definition of scalar multiplication directly. 
\end{proof}

\begin{defn}{Matrix Multiplication}{} Let $A\in M_{m\times p}(\R)$, $B\in M_{p\times n}(\R)$. We define matrix multiplication as $\cdot:M_{m\times p}(\R)\times M_{p\times n}(\R)\to M_{m\times n}(\R)$ where $$A\cdot B=(c_{i,j})_{m\times n}$$ with $$c_{i,j}=\sum_{k=1}^pa_{ik}b_{kj}$$
\end{defn}

\begin{prp}{}{} Let $A,B,C$ be matrices over $R$, with matrix multiplication assumed possible below. 
\begin{itemize}
\item $(AB)C=A(BC)$
\item $(A+B)C=AB+AC$
\item $A(B+C)=AB+AC$
\end{itemize}\tcbline
\begin{proof} Once again an easy proof exploiting the definition of matrix multiplication
\end{proof}
\end{prp}


\begin{defn}{Invertible Matrices}{} A square matrix $A$ is said to be invertible or non-singular if there is a square matrix $B$ such that $AB=BA=I$. In this case $B$ is the inverse of $A$. A matrix that is not-invertible is a singular matrix. 
\end{defn}

\begin{thm}{}{} If $A$ is invertible, then it has a unique inverse. 
\end{thm}
\begin{proof} Suppose that $A$ is invertible. Then there exists $B$ such that $AB=I$. Thus the inverse is exactly $B$. Suppose that $C$ is also an inverse of $A$. Then $AB=I=AC$. Thus $BAB=BAC$ implies $B=C$. 
\end{proof}

\begin{defn}{Upper Triangular Matrices}{} A matrix is called upper triangular if all of its entries below the main diagonal are zero. 
\end{defn}

\begin{defn}{Diagonal Matrices}{} A square matrix is said to be a diagonal matrix if $d_{ij}=0$ whenever $i\neq j$
\end{defn}

\begin{defn}{Transpose}{} Let $A=(a_{ij})_{m\times n}$.  The transpose of $A$ is the $n\times m$ matrix denoted by $A^T$ obtained by interchanging the row and columns of A, that is, $A^T=(a_{ji})_{n\times m}$
\end{defn}

\subsection{Elementary Matrices}
\begin{defn}{}{}[Recombine Matrix] The $n\times n$ recombine matrix is given by $$R_{i,j,a}=
\begin{pmatrix}
1&&&&&&\\
&\ddots&&&&&\\
&&1&&a&&\\
&&&\ddots&&&\\
&&&&1&&\\
&&&&&\ddots&\\
&&&&&&1
\end{pmatrix}$$ where the diagonals are all $1$ and other elements that are not shown is $0$.
\end{defn}

\begin{defn}{}{}[Scale Matrix] The $n\times n$ scale matrix is given by $$R_{i}(a)=
\begin{pmatrix}
1&&&&&&\\
&\ddots&&&&&\\
&&1&&&&\\
&&&a&&&\\
&&&&1&&\\
&&&&&\ddots&\\
&&&&&&1
\end{pmatrix}$$ where the diagonals are all $1$ except for the $i,i$th element and other elements that are not shown is $0$.
\end{defn}

\begin{defn}{}{}[Transposition Matrix] The $n\times n$ transposition matrix is given by $$R_{i,j}=
\begin{pmatrix}
1&&&&&&&&\\
&\ddots&&&&&&&\\
&&0&&&&1&&\\
&&&1&&&&&\\
&&&&\ddots&&&&\\
&&&&&1&&&\\
&&1&&&&0&&\\
&&&&&&&\ddots&\\
&&&&&&&&1
\end{pmatrix}$$ where the diagonals are all $1$ except for the $i,i$th and $j,j$th element and other elements that are not shown is $0$.
\end{defn}

\begin{thm}{}{} The inverse of the three elementary matrices exists and are also their respective elementary matrices. 
\end{thm}
\begin{proof} Note that $$R_{i,j,a}R_{i,j,-a}=I$$ and $$R_i(a)R_i(a^{-1})=I$$ and $$R_{i,j}R_{i,j}=I$$
\end{proof}

\subsection{Row Operations}
\begin{defn}{}{}[Row Operations] Let $A_{m\times n}$ with rows $r_1,\dots,r_m$. There are three types of row operations available on $A$. 
\begin{itemize}
\item For some $i\neq j$, add a multiple of $r_j$ to $r_i$
\item Interchange $r_i$ and $r_j$
\item Multiply a row by a non-zero scalar
\end{itemize}
\end{defn}

\begin{thm}{}{} The three row operations are in fact matrix left multiplications of the elementary matrices. Namely, the recombine matrix, the scale matrix and the transposition matrix corresponds to the above row operations respectively. 
\end{thm}
\begin{proof} It suffices to check for yourselves that each row operation corresponds to an elementary matrix by simple matrix multiplication. 
\end{proof}

\begin{thm}{}{} Let $A_{m\times n}$ be a matrix and $P_{m\times m}$ a product of $m\times m$ elementary matrices. Then the equations $Ax=0_m$ and $(PA)x=0_m$ has the same solution set. 
\end{thm}
\begin{proof} Since $P$ is invertible, we have that $Ax=0_m$ if and only if $(PA)x=0_m$. Thus they have the same solutions. 
\end{proof}

\begin{defn}{}{}[Upper Echelon Form] A matrix satisfying the below properties is said to be in upper echelon form. 
\begin{itemize}
\item All zero rows are below all non-zero rows. 
\item The first non-zero entry of a row is to the right of the first non-zero entry of the row above. 
\item The first non-zero entry of every row is $1$
\end{itemize}
\end{defn}

\begin{defn}{}{}[Row-reduced Form] We say that a matrix in upper echelon form is in row-reduced form if above and below every first non-zero entry of a row, all entries are $0$. 
\end{defn}

\begin{defn}{}{}[Reduction Procedure] Let $A=(a_{ij})_{m\times n}$. Start with $a_{11}$. 
\begin{enumerate}
\item If $a_{ij}$ and all entries below are $0$, move on pivot to the right to $a_{i,j+1}$ and repeat step 1, or terminate if $j=n$
\item If $a_{ij}=0$ but $(a_kj)\neq 0$ for some $k>i$, apply $R_{i,j}$
\item If $a_{ij}\neq 1$, apply $R_{a_{i,j}^{-1}}$
\item If for any $k\neq i$, $a_{kj}\neq 0$, apply $R_{k,i,-a_{kj}}$
\item If $i=m$ or $j=n$ then terminate, else move pivot to $i+1,j+1$ and go back to step 1. 
\end{enumerate}
\end{defn}

\begin{thm}{}{} Every matrix is row equivalent to one and only one matrix in row reduced form. 
\end{thm}
\begin{proof} The above reduction procedure provides the existence of a row reduced form. We prove by induction on the number of columns of $A$ the uniqueness of the matrix. Let $A$ be an $m-\times n$ matrix. When $n=1$, there are only two possible row reduced forms. $a_{i1}=0$ for all $i>1$, and $a_{11}$ i either $0$ or $1$. We have $a_{11}=0$ if and only if the original matrix is the $0$ matrix. So any non-zero matrix $m\times 1$ has only one possible row reduced form. \linebreak\linebreak
Now suppose that $n>1$, and the theorem is true for smaller $n$. Let $A'$ be the $m\times(n-1)$ matrix obtained by deleting the last column from $A$. This means that $A=(A'|\vb{k})$. By induction the row reduced form of $A'$ is unique. Now if any sequence of row operations that places $A$ into row reduced form, it also places $A'$ into row reduced form, so if $B$ and $C$ are two row reduced form of $A$, they differ only by the last column. Now since row operations conserve the set of solutions to $A\vb{x}=0$, we have that if $\vb{c}$ is the solution, then $B\vb{c}=0$ and $C\vb{c}=0$ and $(B-C)vb{c}=0$. Since the first $n-1$ columns of $B$ and $C$ are the same, if $B\neq C$, we have $B-C$ is of the form $(\vb{0}_{m,n-1}|\vb{u})$ and $\vb{u}\neq0$. Since the last column is nonzero, there must be at least one element in $\vb{u}$ nonzero, meaning there is at least one row in the form $(\vb{0}|p)$ for some $p\neq0$. \linebreak\linebreak
Now $(B-C)\vb{c}=0\implies (\vb{0}|\vb{u})\vb{c}=0$ and thus $(\vb{0}|p)\cdot\vb{c}=0$ and $pc_n=0$. If $p\neq 0$ then naturally $c_n=0$ by cancellation law. This implies that there is a leading one in the $n$th column of $B$. Because if this is not true, any choice of $\alpha\in\R$ and setting $c_n=\alpha$ could lead to a solution to $B\vb{c}=0$, contradicting the fact that $c_n=0$. \linebreak\linebreak
Now the leading one in the $n$th column must occur in the first zero row of $A'$ in both $B$ and $C$. And since other every other entry in the column of a leading one is zero, we finally have that the $n$th column of $B$ and $C$ is equal. Thus $B=C$. 
\end{proof}

\subsection{Determinants}
We borrow notations from group theory. 
\begin{defn}{}{}[Odd Even Permutations] A permutation is said to be even, and to have sign $+1$ if $\phi$ is a composition of an even number of transpositions, and $\phi$ is said to be odd, and to have sign $-1$ if $\phi$ is a composition of an odd number of transpositions. 
\end{defn}

\begin{defn}{}{}[Determinants] The determinant of a $n\times n$ matrix $A=(a_{ij})$ is the scalar quantity $$\det(A)=\sum_{\phi\in S_n}\sign(\phi)a_{1\phi(1)}a_{2\phi(2)}\dots a_{n\phi(n)}$$
\end{defn}

\begin{lmm}{}{} $\det(I_n)=1$. 
\end{lmm}
\begin{proof} 
\begin{align*}
\det(I_n)&=\sum_{\phi\in S_n}\sign(\phi)a_{1\phi(1)}a_{2\phi(2)}\dots a_{n\phi(n)}\\
&=a_{11}a_{22}\dots a_{nn}\\
&=1
\end{align*}
\end{proof}

\begin{prp}{}{} If $A$ has two equal rows then $\det(A)=0$
\end{prp}

\begin{prp}{}{} Applying elementary row operations does the following to the determinant of a matrix. 
\begin{itemize}
\item $\det(R_{i,j,a}A)=\det(A)$
\item $\det(R_{i}(a)A)=a\det(A)$
\item $\det(R_{i,j})=-\det(A)$
\end{itemize}
\end{prp}

\begin{prp}{}{} If $A=(a_{ij})_{n\times n}$ is upper triangular, then $$\det(A)=a_{11}a_{22}\dots a_{nn}$$
\end{prp}

\begin{prp}{}{} Let $A=(a_{ij})_{n\times n}$, $B=(b_{ij})_{n\times n}$. Then 
\begin{itemize}
\item $\det(A^T)=\det(A)$
\item $\det(AB)=\det(A)\det(B)$
\end{itemize}
\end{prp}

\subsection{Inverses of Matrices}
\begin{defn}{}{}[Minor] Let $A\in M_{n\times n}(\R)$. The minor $M_{ij}$ of the element $a_{ij}$ of $A$ is the determinant of the submatrix obtained by deleting the $i$th row and the $j$th column of $A$. $$M_{ij}=
\begin{vmatrix}
a_{1,1} & \cdots & a_{1,j-1} & a_{1,j+1} & \cdots & a_{1,n}\\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
a_{i-1,1} & \cdots & a_{i-1,j-1} & a_{i-1,j+1} & \cdots & a_{i-1,n}\\
a_{i+1,1} & \cdots & a_{i+1,j-1} & a_{i+1,j+1} & \cdots & a_{i+1,n}\\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
a_{m,1} & \cdots & a_{m,j-1} & a_{m,j+1} & \cdots & a_{m,n}\\
\end{vmatrix}$$
\end{defn}

\begin{defn}{}{}[Cofactor] The cofactor of $a_{ij}$ in $A\in M_{n\times n}(\R)$ is defined as $$A_{ij}=(-1)^{i+j}M_{ij}$$
\end{defn}

\begin{defn}{}{}[Adjoint] Let $A\in M_{n\times n}(\R)$. The adjoint of $A$ is defined as $$\adj(A)=(A_{ij})_{m\times n}^T$$
\end{defn}

\begin{thm}{}{} Let $A\in M_{n\times n}(\R)$. Then $$A(\adj(A))=(\adj(A))A=\det(A)I$$
\end{thm}

\begin{prp}{}{} Let $A\in M_{n\times n}(\R)$. Then $\det(A)=\sum_{k=1}^{n}a_{ik}A_{ik}$ for $i=1,2,3$ and $\det(A)=\sum_{k=1}^{n}a_{kj}A_{kj}$ for $j=1,2,3$
\end{prp}

\begin{thm}{}{}[Inverse of a Matrix] Let $A\in M_{n\times n}(\R)$. Then $A$ is invertible if and only if $\det(A)\neq0$. In this case, $$A^{-1}=\frac{1}{det(A)}\adj(A)$$
\end{thm}

\begin{prp}{}{} Let $A\in M_{n\times n}(\R)$. If $A$ is invertible then $\det(A^{-1})=\frac{1}{det(A)}$
\end{prp}

\begin{thm}{}{} Suppose that $A,B$ are invertible. 
\begin{itemize}
\item $AB$ is also invertible and $(AB)^{-1}=B^{-1}A^{-1}$
\item $A^n$ is also invertible where $n\in\mathbb{N}$ and $(A^n)^{-1}=(A^{-1})^n$
\item $A^{-1}$ is also invertible and $(A^{-1})^{-1}=A$
\item $A^T$ is also invertible and $(A^T)^{-1}=(A^{-1})^T$
\end{itemize}
\end{thm}

\subsection{System of Linear Equations}
\begin{defn}{}{}[System of Linear Equations] We say that $$
\begin{cases}
a_{11}x_1+a_{12}x_2+\dots+a_{1n}x_n=b_1\\
a_{21}x_1+a_{22}x_2+\dots+a_{2n}x_n=b_2\\
\vdots\\
a_{m1}x_1+a_{m2}x_2+\dots+a_{mn}x_n=b_m
\end{cases}$$ is a system of linear equations in $n$ unknowns $x_1,\dots,x_n$. 
\end{defn}

\subsection{System of Linear Equations}
\begin{defn}{}Solution Sets{}[] A solution of a system of equations is a list of numbers $x_1,\dots,x_n$ that makes all of the equations true simultaneously. The solution set of a system of equations is the collection of all solutions. 
\end{defn}

\begin{defn}{Consistent Systems}{}[] We say that the system of linear equations is consistent if it has at least one solution. We say that the system of linear equations is inconsistent if it has no solutions. 
\end{defn}

\begin{defn}{Homogenous Systems}{}[] We say that the system of linear equations is a homogenous system if all the constant coefficients are $0$. 
\end{defn}

\begin{defn}{Representation of System of Linear Equations}{} The matrix $$A=
\begin{pmatrix}
a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&\vdots&\vdots\\
a_{m1}&a_{m2}&\cdots&a_{mn}
\end{pmatrix}$$ and $$B=
\begin{pmatrix}
b_1\\
b_2\\
\vdots\\
b_n
\end{pmatrix}$$ can represent a system of linear equations by $Ax=B$ with $x=
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_n
\end{pmatrix}$. $A$ is said to be the coefficient matrix. $X$ is said to be a solution if it satisfies the above $m$ equations simultaneously. 
\end{defn}