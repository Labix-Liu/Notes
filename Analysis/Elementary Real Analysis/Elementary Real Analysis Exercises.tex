\documentclass[a4paper]{article}

\input{C:/Users/liula/Desktop/Latex/Headers.tex}

\pagestyle{fancy}
\fancyhf{}
\rhead{Labix}
\lhead{Elementary Real Analysis Exercises}
\rfoot{\thepage}

\title{Elementary Real Analysis Exercises}

\author{Labix}

\date{\today}
\begin{document}
\maketitle
\begin{abstract}
A collection of quality exercises and self-created exercises section by section based on my notes Elementary Real Analysis
\end{abstract}
\textbf{References}
\begin{itemize}
\item Principles of Mathematical Analysis Third Edition by Walter Rudin
\item University of Warwick MA131 Analysis Lecture Notes by Keith Ball
\item Imperial College London Math40002 Lecture Notes
\end{itemize}
\pagebreak
\tableofcontents
\pagebreak

\section{Developing the Real Numbers}
\subsection{Properties of the Real Numbers}


\subsection{The Completeness Axiom}
\begin{qtn}{}{}
\thetcbcounter.\;\; Determine if the supremum and infinum exists for the set $\{\frac{1}{n}:n\in\N\}$. Find it if it exists. \tcbline
\begin{proof}
The supremum is $1$ since for all $\frac{1}{n}$ in the set, $\frac{1}{n}\leq 1$. Suppose that $a<1$ is an upper bound of the set, then necessarily $\frac{1}{1}>a$ thus $a$ cannot be an upper bound. \\~\\
The infinum is $0$ since for all $\frac{1}{n}$ in the set, $\frac{1}{n}>0$. Suppose that $a>0$ is a lower bound of the set, then there must exist $n\in\N$ such that $\frac{1}{n}<a$ by choosing any $n>\frac{1}{a}$. Thus $a$ cannot be a lower bound. 
\end{proof}
\end{qtn}

\subsection{Density of the Reals}

\pagebreak
\section{Sequences}
\subsection{Sequences and Convergences}
\begin{qtn}{}{}
\thetcbcounter.\;\; Prove that the sequence $a_n=\frac{1}{n}$ converges to $0$. \tcbline
\begin{proof}
Let $\epsilon>0$. Choose $N>\frac{1}{\epsilon}$. Then when $n>N$, we must have $\frac{1}{n}<\frac{1}{N}<\epsilon$. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Prove that the sequence $a_n=\frac{\frac{4n^3+6}{n^3-2}+2^{\frac{1}{n}}}{\cos(\frac{1}{n})}$ converges and find its limit. \tcbline
\begin{proof}
By the algebra of sequences, we have that $a_n\to \frac{4+1}{1}=5$. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Prove that $a_n=\frac{\sin(n)}{n}$ converges and find its limit. \tcbline
\begin{proof}
Note that $0\leq\abs{a_n}\leq\frac{1}{n}$ thus by sandwich theorem, $\abs{a_n}\to 0$. Thus $a_n\to 0$. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Define a sequence by $x_1=1$ and $x_{n+1}=5-\frac{1}{x_n}$. Show that $x_n$ converges and find its limit. \tcbline
\begin{proof}
Consider the equation $x=5-\frac{1}{x}$. This equation has roots $x=\frac{5\pm\sqrt{21}}{2}$. Since $(x_n)_{n\in\N}$ is bounded between $1$ and $5$, we reject the root larger than $5$ and claim that $x=\frac{5-\sqrt{21}}{2}$ is the limit. 
\end{proof}
\end{qtn}

\subsection{Subsequences}

\subsection{Further Developing the Real Numbers}

\pagebreak

\section{Series}
\subsection{Series and Convergences}
\subsection{Series with Non-negative Terms}
\subsection{Alternating Series}
\subsection{Absolute and Conditional Convergence}
\subsection{Rearrangement of Series}

\pagebreak

\section{Limits and Continuity}
\subsection{Limits}
\subsection{Continuity}
\subsection{Uniform Continuity}
\begin{qtn}{}{}
\thetcbcounter.\;\; Let $f,g:I\subset\R\to\R$ be uniformly continuous and bounded. Prove that the product $fg:I\to\R$ is also uniformly bounded. \\\hspace*{\fill}\cite{R0004}\tcbline
\begin{proof}
Suppose that $\abs{f(x)}\leq M$ and $\abs{g(x)}\leq N$ for all $x\in I$. Let $\epsilon>0$. There exists $\delta_1,\delta_2>0$ such that $\abs{x-y}<\delta_1$ implies $\abs{f(x)-f(y)}<\epsilon$ and $\abs{x-y}<\delta_2$ implies $\abs{g(x)-g(y)}<\epsilon$. Take $\delta=\min\{\delta_1,\delta_2\}$. Now we have 
\begin{align*}
\abs{f(x)g(x)-f(y)g(y)}&=\abs{f(x)g(x)-f(x)g(y)+f(x)g(y)-f(y)g(y)}\\
&\leq\abs{f(x)}\abs{g(x)-g(y)}+\abs{g(y)}\abs{f(x)-f(y)}\\
&\leq M\epsilon+N\epsilon\\
&=(M+N)\epsilon
\end{align*}
Rescaling $\epsilon$ with $\frac{1}{M+N}$ gives our result. 
\end{proof}
\end{qtn}

\subsection{Power Series}
\pagebreak
\section{Differentiation}
\subsection{Properties of the Derivative}
\subsection{Inverses}
\subsection{Local Extremums and Concavity}
\subsection{L'Hopital's Rule}
\subsection{Taylor Series}
\subsection{The Exponential Series}
\subsection{Second Derivatives}
\pagebreak
\section{Integration}
\subsection{Darboux Integral}
\begin{qtn}{}{}
\thetcbcounter.\;\; Define $f:[0,1]\to\R$ by $f(x)=x^3$ and let $P=\{0,\frac{1}{10}, \frac{2}{5},1\}$. Compute $L(f,P)$ and $U(f,P)$. \\\hspace*{\fill}\cite{R0003}\tcbline
\begin{proof}
Notice that $f$ is an increasing function. Thus $m_k=f(x_k)$ and $M_k=f(x_{k+1})$ and 
\begin{align*}
L(f,P)&=0\cdot\frac{1}{10}+\frac{1}{10^3}\cdot\frac{3}{10}+\left(\frac{2}{5}\right)^3\cdot\frac{3}{5}\\
&=0.0387
\end{align*}
and 
\begin{align*}
U(f,P)&=\frac{1}{10^3}\cdot\frac{1}{10}+\left(\frac{2}{5}\right)^3\cdot\frac{3}{10}+1\cdot\frac{3}{5}\\
&=0.6193
\end{align*}
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Let $f:[0,1]\to\R$ be defined $f(x)=x$. Show that $f$ is integrable and compute $\int_0^1f$ using the definition of the integral. \\\hspace*{\fill}\cite{R0003}\tcbline
\begin{proof}
Let $\epsilon>0$. I want to show that there exists a partition such that $U(f,P)-L(f,P)<\epsilon$. Choose $n$ such that $n>\frac{1}{\epsilon}$. Define a partition with equal subinterval lengths. That is $P=\{0,\frac{1}{n},\frac{2}{n},1\}$. Then 
\begin{align*}
U(f,P)-L(f,P)&=\sum_{k=1}^n\left(f\left(\frac{k}{n}\right)-f(\frac{k-1}{n})\right)\frac{1}{n}\\
&=\sum_{k=1}^n\frac{1}{n}\cdot\frac{1}{n}\\
&=\frac{1}{n}\\
&<\epsilon
\end{align*}
Thus $f$ is integrable. Now computing $U(f,P)$ and $L(f,P)$ separately, we find that $L(f,P)=\frac{1}{2}-\frac{1}{2n}$ and $U(f,P)=\frac{1}{2}+\frac{1}{2n}$. Thus we have that $$\int_0^1f(x)\,dx=\frac{1}{2}$$ as $n\to\infty$. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Let $f:[a,b]\to\R$ be a Riemann integrable function. Let $\alpha>0$ and $\beta\in\R$. Define $g(x)=f(\alpha x+\beta)$ on the interval $I=\left[\frac{a-\beta}{\alpha},\frac{b-\beta}{\alpha}\right]$. Show that $g$ is Riemann integrable. \\\hspace*{\fill}\cite{R0003}\tcbline
\begin{proof}
Let $\epsilon>0$. Then there exists a partition $P$ such that $U(f,P)-L(f,P)<\epsilon$. Note that $\alpha>0$ means that the supremum and infinum is preserved and sclaed by $\alpha$. This means that $U(f,P)-L(f,P)=\sum_{k=1}^n(M_k-m_k)(x_k-x_{k-1})$ and 
\begin{align*}
U(g,P)-L(g,P)&=\sum_{k=1}^n(\alpha M_k+\beta-\alpha m_k-\beta)(\alpha x_k+\beta-\alpha x_{k-1}-\beta)\\
&=\alpha^2\sum_{k=1}^n(M_k-m_k)(x_k-x_{k-1})\\
&<\alpha^2\epsilon
\end{align*}
Thus rescaling $\epsilon$ with $\frac{1}{\alpha^2}$ gives our result. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Let $f:[0,1]\to\R$ be given by $$f(x)=\begin{cases}
1-\frac{1}{q} & \text{if }x=\frac{p}{q} \text{ with } p,q\in\N \text{ coprime}\\
1 & \text{otherwise}
\end{cases}$$ 
Prove that $f$ is Riemann integrable and calculate $\int_0^1f(x)\,dx$. \\\hspace*{\fill}\cite{R0002}\tcbline
\begin{proof}
Fix $\epsilon>0$. I claim that there is only a finite number of points $x_1,\dots,x_n$ between $0$ and $1-\epsilon$. This is because as $q\to\infty$, $1-\frac{1}{q}\to 1$ which means by convergence there is only a finite number of points outside the $\epsilon$ bounds for convergence. \\~\\
Define the partition $P$ by including $[x_k-\frac{\epsilon}{2n},x_k+\frac{\epsilon}{2n}]$ where $k\in\{1,\dots,n\}$. Connect these disjoint intervals with the complement in $[0,1]$ to form the partition. Denote $N$ the number of intervals in $P$. Now we have that 
\begin{align*}
U(f,P)-L(f,P)&=\sum_{k=1}^N(M_k-m_k)\abs{x_k-x_{k-1}}\\
&=\sum_{k=1}^n(M_k-m_k)\abs{x_k-x_{k-1}}+\sum_{k=1}^{N-n}(M_k-m_k)\abs{x_k-x_{k-1}}\tag{Left sum is constructed intervals}\\
&<\sum_{k=1}^n(M_k-m_k)\abs{x_k-x_{k-1}}+1-(1-\epsilon)\sum_{k=1}^{N-n}\abs{x_k-x_{k-1}}\tag{$M_k=1$, $1-m_k<\epsilon$}\\
&<\sum_{k=1}^n\abs{x_k-x_{k-1}}+\epsilon\sum_{k=1}^{N-n}\abs{x_k-x_{k-1}}\tag{$M_k-m_k<1$}\\
&<\epsilon+\epsilon\sum_{k=1}^{N-n}\abs{x_k-x_{k-1}}\tag{Total length of partition is $\epsilon$}\\
&<\epsilon+\epsilon\tag{Length of partition $<1$}
&=2\epsilon
\end{align*}
Rescaling $\epsilon$ gives the desired result. \\~\\
For the value of the integral, notice that for any partition $P$, $U(f,P)=\sum_{k=1}^nM_k\abs{x_k-x_{k-1}}=1$ Thus $$\int_0^1f(x)\,dx=\inf_P\{U(f,P)\}=1$$
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Let $f,g:[a,b]\to\R$. Assume that $f$ is integrable and $f(x)=g(x)$ for all but finitely many points $c_1,\dots,c_n\in[a,b]$. Show that $g$ is Riemann integrable and calculate $\int_a^bg(x)\,dx$ in terms of $\int_a^bf(x)\,dx$. \\\hspace*{\fill}\cite{R0002}\tcbline
\begin{proof}
I will prove the proposition with one point of disagreement. Suppose that $f$ and $g$ disagree at $c\in[a,b]$. Then $h(x)=f(x)-g(x)$ is the zero function except at point $c$. Without loss of generality take $h(c)>0$. Let $\frac{\epsilon}{h(c)}>0$. Take $P$ be any partition such that the interval containing $c$, denoted $I_c$, has length $\abs{I_c}<\frac{\epsilon}{h(c)}$. Then 
\begin{align*}
U(f,P)-L(f,P)&=U(f,P)\\
&=\sum_{k=1}^nM_k\abs{x_k-x_{k-1}}\\
&=h(c)\abs{I_c}\\
&<\epsilon
\end{align*}
Thus now $h$ is integrable. Clearly since $L(f,P)=0$ for all $P$, $\int_a^bh(x)\,dx=\sup_P\{L(f,P)\}=0$. By the linearity of Riemann integrals, we have that $g(x)=f(x)-h(x)$ is integrable and $$\int_a^bg(x)\,dx=\int_a^bf(x)\,dx$$
\end{proof}
\end{qtn}

\subsection{Riemann Integral}


\subsection{Properties of the Riemann Integral}
\begin{qtn}{}{}
\thetcbcounter.\;\; Suppose that $f\geq 0$, $f$ is continuous on $[a,b]$ and $\int_a^bf(x)\,dx=0$. Prove that $f(x)=0$ for all $x\in[a,b]$. \\\hspace*{\fill}\cite{R0001}\tcbline
\begin{proof}
Suppose for a contradiction that there exists $c\in(a,b)$ such that $f(c)>0$. Then there must exists some number $\epsilon>0$ such that $f(c)>\epsilon>0$. By continuity, there exists $\delta>0$ such that $\abs{x-c}<\delta$ implies $$\abs{f(x)-f(c)}<\epsilon$$ This means that there is a range of $x$ such that $f(x)$ is non zero by continuity. But then $$\int_a^bf(x)\,dx\geq\int_{c-\delta}^{c+\delta}f(x)\,dx\geq\int_{c-\delta}^{c+\delta}\epsilon=\epsilon\cdot 2\delta>0$$ Thus $f(x)=0$ for $c\in(a,b)$. By continuity the end points must also be $0$ else it is not continuous. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; If $f(x)=0$ for all irrational $x$, $f(x)=1$ for al rational $x$, prove that $f$ is not integrable on $[a,b]$ for any $a<b$. \tcbline
\begin{proof}
We have that for any partition $P$, $$\sum_{k=1}^nm_k\abs{I_k}=0\;\;\;\text{and}\;\;\;\sum_{k=1}^nM_k\abs{I_k}=b-a$$ since any interval must contain at least one rational number and one irrational number. Clearly the sums does not vary along with the partition thus the infinum of the upper sum will never be equal to the supremum of the lower sum when $a<b$. Thus $f$ is not integrable. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Let $f,g:[a,b]\to\R$ be continuous functions. Assume that $g(x)\geq 0$ for all $x\in[a,b]$. Show that there exists $t\in[a,b]$ such that $$\int_a^bf(x)g(x)\,dx=f(t)\int_a^bg(x)\,dx$$ \\\hspace*{\fill}\cite{R0002}\tcbline
\begin{proof}
Since $f$ is continuous in $[a,b]$, $f$ attains its maximum and minimum, say $M$ and $m$ respectively. Then $mg(x)\leq f(x)g(x)\leq Mg(x)$ for all $x\in[a,b]$. This implies that $$\int_a^bmg(x)\,dx\leq\int_a^bf(x)g(x)\,dx\leq\int_a^bMg(x)\,dx$$~\\
If $g(x)$ is the zero function then we are done. Suppose not. I will show that $\int_a^bg(x)\,dx>0$. There exists $c\in[a,b]$ such that $g(c)\neq 0$. Choose $\epsilon>0$ such that $g(c)>\epsilon$. Then by continuity there exists $\delta>0$ such that $\abs{x-c}<\delta$ implies $g(x)>\epsilon$ for all $x$ in this interval. Denote $(c-\delta,c+\delta)$ by $I$. Then $$\int_Ig(x)\,dx\geq\int_I\epsilon\,dx=\epsilon\abs{I}$$ We also have that $$\int_a^bg(x)\,dx=\int_a^{c-\delta}+\int_{c+\delta}^bg(x)\,dx+\int_Ig(x)\,dx$$ Thus $\int_a^bg(x)\,dx>0$. \\~\\
Rearranging the inequality gives $$m\leq\frac{\int_a^bf(x)g(x)\,dx}{\int_a^bg(x)\,dx}\leq M$$ Since $f$ is continuous, it attains all values between $m$ and $M$. Thus there must exists $t\in[a,b]$ such that $$f(t)=\frac{\int_a^bf(x)g(x)\,dx}{\int_a^bg(x)\,dx}$$ as required. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Let $f:[a,b]\to\R$ and $g:[a,b]\to\R$ be continuous functions. Show that if $\int_a^bf=\int_a^bg$ then there exists $c\in[a,b]$ such that $f(c)=g(c)$. \tcbline
\begin{proof}
Consider the new function $h(x)=f(x)-g(x)$. It is integrable by algebra of integrable functions. By the intermediate value theorem, there exists $c\in[a,b]$ such that $$h(c)=\frac{1}{b-a}\int_a^bh(x)\,dx$$ But this implies that $$f(c)-g(c)=\frac{1}{b-a}\int_a^bf(x)-g(x)\,dx=0$$ Thus we have that $f(c)=g(c)$. 
\end{proof}
\end{qtn}

\subsection{Fundamental Theorem of Calculus}
\begin{qtn}{}{}
\thetcbcounter.\;\; Compute the derivative of the following functions: 
\begin{enumerate}[label=\alph*)]
\item $$f(x)=\int_0^{x^2}\sin(t^3)\,dt$$
\item $$g(x)=\int_{x^3}^{2+x^6}e^{-t^2}\,dt$$
\end{enumerate}~\\\hspace*{\fill}\cite{R0002}\tcbline
\begin{proof}~\\
\begin{enumerate}[label=\alph*)]
\item Let $u=x^2$. Then applying the fundamental theorem of caluclus to $f(u)$ gives 
\begin{align*}
f'(x)&=f'(u)u'(x)\tag{Chain Rule}\\
&=\sin(u^3)\cdot(2x)\\
&=2x\sin(x^6)
\end{align*}
\item Define $g_1(x)=\int_{0}^{2+x^6}e^{-t^2}\,dt$ and $g_2(x)=\int_0^{x^3}e^{-t^2}\,dt$. Let $u(x)=2+x^6$. By the fundamental theorem of calculus, 
\begin{align*}
g_1'(x)&=g_1'(u)u'(x)\tag{Chain Rule}\\
&=e^{-u^2}6x^5\\
&=6x^5e^{-(2+x^6)^2}
\end{align*}
Similarly, we have
\begin{align*}
g_2'(x)&=g_2'(v)v'(x)\tag{$v(x)=x^3$}\\
&=e^{-v^2}3x^2\\
&=3x^2e^{-x^6}
\end{align*}
Combining the both with the linearity property gives $$g(x)=6x^5e^{-(2+x^6)^2}-3x^2e^{-x^6}$$
\end{enumerate}
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Given a function $f$ on $[a,b]$, define the total variation of $f$ to be $$Vf=\sup_P\left\{\sum_{k=1}^n\abs{f(x_k)-f(x_{k-1}}\right\}$$ where $P$ is a any partition of $[a,b]$. Prove using the FTC that if the function is is differentiable and continuous derivatives, then $$Vf\leq\int_a^b\abs{f'}$$ Use the mean value theorem to establish the reverse inequality and conclude that $Vf=\int_a^b\abs{f'}$. \\\hspace*{\fill}\cite{R0004}\tcbline
\begin{proof}
Firstly, by the FTC, we have that $\abs{\int_{x_{k-1}}^{x_k}f'}=\abs{f(x_k)-f(x_{k-1}}\leq\int_{x_{k-1}}^{x_k}\abs{f'}$. This is possible since $f$ is known to be continuous and differentiable. Summing up the partitions, we have that $$\abs{\int_a^bf'}=\sum_{k=1}^n\abs{f(x_k)-f(x_{k-1})}\leq\int_a^b\abs{f'}$$ Since this is true for any arbitrary partition, we have that $$Vf=\sup_P\left\{\sum_{k=1}^n\abs{f(x_k)-f(x_{k-1}}\right\}\leq\int_a^b\abs{f'}$$ For the second part, since $f$ is differentiable and continuous at $(a,b)$, by the mean value theorem we have that $$\abs{f(x_k)-f(x_{k-1})}=\abs{f'(c_k)}\abs{(x_k-x_{k-1})}$$ for some $c_k\in(x_{k-1},x_k)$. Summing them all up gives $$Vf=\sup_P\left\{\sum_{k=1}^nf'(c_k)\abs{x_k-x_{k-1}}\right\}$$ The right hand side will be greater than or equal to $\sup_P\{\sum_{k=1}^nm_k\abs{x_k-x_{k-1}}\}$, the lower sum of the partition where $m_k$ is the infinum of $\abs{f'}$ between $x_{k-1}$ and $x_k$. Thus $$Vf\geq\sup_P\{L(f,P)\}=\int_a^b\abs{f'}$$ and we are done. 
\end{proof}
\end{qtn}

\subsection{Improper Integrals}
\begin{qtn}{}{}
\thetcbcounter.\;\; Show that for any $s\in\R$, there exists non-negative functions $a(\epsilon)$, $b(\epsilon)$ such that $a(\epsilon)\to 0$ and $b(\epsilon)\to 0$ as $\epsilon\to 0$ and $$\lim_{\epsilon\to0^+}\left[\int_{-1}^{-a(\epsilon)}\frac{1}{x}\,dx+\int_{b(\epsilon)}^1\frac{1}{x}\,dx\right]=s$$\\\hspace*{\fill}\cite{R0002}\tcbline
\begin{proof}~\\
Since $a(\epsilon)$, $b(\epsilon)$ are non zero, 
\begin{align*}
\lim_{\epsilon\to0^+}\left[\int_{-1}^{-a(\epsilon)}\frac{1}{x}\,dx+\int_{b(\epsilon)}^1\frac{1}{x}\,dx\right]&=\lim_{\epsilon\to0^+}\left[-\int_{a(\epsilon)}^{1}\frac{1}{x}\,dx+\int_{b(\epsilon)}^1\frac{1}{x}\,dx\right]\\
&=\lim_{\epsilon\to0^+}\left[\ln(a(\epsilon))+\ln(b(\epsilon))\right]\\
&=\lim_{\epsilon\to0^+}\ln\left(\frac{a(\epsilon)}{b(\epsilon)}\right)\\
\end{align*}
Fix $s$, then by continuity of limits, $$\lim_{\epsilon\to 0^+}\frac{a(\epsilon)}{b(\epsilon)}=e^s$$ Then just take $b(\epsilon)=b_0\epsilon$ and $a(\epsilon)=e^sb_0\epsilon$. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Define $$f(x)=\int_x^{x+1}\sin(t^2)\,dt$$
\begin{enumerate}[label=\alph*)]
\item Prove that $\abs{f(x)}<\frac{1}{x}$ if $x>0$. \\
Hint: Put $t^2=u$ and integrate by parts to show that $f(x)$ is equal to $$\frac{\cos(x^2)}{2x}-\frac{\cos[(x+1)^2]}{2(x+1)}-\int_{x^2}^{(x+1)^2}\frac{\cos(u)}{4u^{3/2}}\,du$$
\item Prove that $$2xf(x)=\cos(x^2)-\cos[(x+1)^2]+r(x)$$ where $\abs{r(x)}<\frac{c}{x}$ and $c$ is a constant. 
\item Find the upper and lower limits of $xf(x)$ as $x\to\infty$
\item Does $\int_0^\infty\sin(t^2)\,dt$ converge?
\end{enumerate}~\\\hspace*{\fill}\cite{R0001}\tcbline
\begin{proof}~\\
\begin{enumerate}[label=\alph*)]
\item Substituting $t^2=u$, we find that 
\begin{align*}
f(x)&=\int_{x^2}^{(x+1)^2}\frac{\sin(u)}{2\sqrt{u}}\,du\\
&=\frac{1}{2}\left(\left[-\frac{\cos(u)}{\sqrt{u}}\right]_{x^2}^{x^2+1}-\int_{x^2}^{(x+1)^2}\frac{\cos(u)}{2\sqrt{u}}\right)
\end{align*}
This is possible since $-\cos(u)$ and $\frac{1}{\sqrt{u}}$ are continuous and differentiable at $(0,\infty)$. Now
\begin{align*}
f(x)&=\frac{\cos(x^2)}{2x}-\frac{\cos[(x+1)^2]}{2(x+1)}-\int_{x^2}^{(x+1)^2}\frac{\cos(u)}{4u^{3/2}}\,du\\
&\leq\frac{\cos(x^2)}{2x}-\frac{\cos[(x+1)^2]}{2(x+1)}+\int_{x^2}^{(x+1)^2}\frac{1}{4u^{3/2}}\,du\tag{Monotonicity}\\
&=\frac{\cos(x^2)}{2x}-\frac{\cos[(x+1)^2]}{2(x+1)}-\frac{1}{2}\left[\frac{1}{\sqrt{u}}\right]_{x^2}^{(x+1)^2}\\
&=\frac{\cos(x^2)}{2x}-\frac{\cos[(x+1)^2]}{2(x+1)}-\frac{1}{2}\left(\frac{1}{x+1}-\frac{1}{x}\right)\\
&=\frac{\cos(x^2)}{2x}-\frac{\cos[(x+1)^2]}{2(x+1)}+\frac{1}{2x(x+1)}\\
\abs{f(x)}&\leq\abs{\frac{\cos(x^2)}{2x}}+\abs{\frac{\cos[(x+1)^2]}{2(x+1)}}+\abs{\frac{1}{2x(x+1)}}\tag{Triangle Inequality}\\
&\leq\frac{1}{2x}+\frac{1}{2(x+1)}+\frac{1}{2x(x+1)}\\
&=\frac{2x+2}{2x(x+1)}\\
&=\frac{1}{x}
\end{align*}
\end{enumerate}
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Does $$\int_0^\infty\sin(x^2)\,dx$$ converge? Use integration by parts and the absolute comparison test. \\\hspace*{\fill}\cite{R0002}\tcbline
\begin{proof}~\\
We have that $$\int_0^1\sin(x^2)\,dx+\lim_{t\to\infty}\int_1^t\sin(x^2)\,dx=\lim_{t\to\infty}\int_0^t\sin(x^2)\,dx$$ The first part of the sum consists of a continuous function and thus is integrable. We just have to show that the latter half converges. Using integration by parts, we have that 
\begin{align*}
\lim_{t\to\infty}\int_1^t\sin(x^2)\,dx&=\lim_{t\to\infty}\int_1^t\frac{1}{2x}\cdot 2x\sin(x^2)\,dx\tag{$x\neq 0$}\\
&=\lim_{t\to\infty}\left[-\frac{\cos(x^2)}{2x}\right]_1^t-\lim_{t\to\infty}\int_1^t\frac{\cos(x^2)}{2x^2}\,dx\\
&=\lim_{t\to\infty}-\frac{\cos(t^2)}{2t}+\frac{\cos(1)}{2}-\lim_{t\to\infty}\int_1^t\frac{\cos(x^2)}{2x^2}\,dx
\end{align*}
Once again, the front half converges since $\abs{\cos(t^2)}\leq 1$ and $\frac{1}{2t}\to 0$ as $t\to\infty$. Consider $g(x)=\frac{1}{2x^2}$. Clearly $\abs{\frac{\cos(x^2)}{2x^2}}\leq g(x)$. Now
\begin{align*}
\lim_{t\to\infty}\int_1^t\frac{1}{2x^2}\,dx&=\lim_{t\to\infty}\left[-\frac{1}{2x}\right]_1^t\\
&=\lim_{t\to\infty}-\frac{1}{2t}+\frac{1}{2}\\
&=\frac{1}{2}
\end{align*}
Thus by the absolute comparison test, $\lim_{t\to\infty}\int_1^t\frac{\cos(x^2)}{2x^2}\,dx$ converges and thus the entire integral converges. 
\end{proof}
\end{qtn}

\subsection{More on Taylor Series}

\pagebreak
\section{Sequences of Functions}
\subsection{Uniform Convergence}
\begin{qtn}{}{}
\thetcbcounter.\;\; Consider the sequence of functions $$h_n(x)=\frac{x}{1+x^n}$$ for $n\in\N$ on the domain $[0,\infty)$. 
\begin{enumerate}[label=\alph*)]
\item Find the pointwise limit $h$ of $(h_n)_{n\in\N}$ on $[0,\infty)$. 
\item Prove that $(h_n)_{n\in\N}$ does not converge to $h$ uniformly on $[0,\infty)$. 
\item Prove that there exists a subset of $[0,\infty)$ over which the convergence is uniform. 
\end{enumerate}~\\\hspace*{\fill}\cite{R0002}\tcbline
\begin{proof}~\\
\begin{enumerate}[label=\alph*)]
\item When $\abs{x}<1$, we have that $x^n\to 0$. Thus $h(x)=x$ for $x\in[0,1)$. When $x=1$, then $h_n(x)=\frac{1}{2}$ thus $h(1)=\frac{1}{2}$. When $x>1$, $\frac{1}{1+x^n}\to 0$. Thus $h(x)=0$ for $x>1$. Combining these three cases gives $$h(x)=\begin{cases}
x & \text{if }0\leq x<1\\
\frac{1}{2} & \text{if } x=1\\
0 & \text{if }x>1
\end{cases}$$
\item $h_n(x)$ is clearly continuous for all $n$. If the sequence is uniformly convergent then $h$ should be continuous since it preserves continuity. But $h$ has a discontinuity at $x=1$. Thus $(h_n)_{n\in\N})$ is not uniformly convergent. 
\item $h_n(x)$ converges uniformly on $[0,a]$ for any $a<1$ and $(b,\infty)$ for any $b>1$. Firstly, consider the interval $[0,a]$. We have that 
\begin{align*}
\abs{\frac{x}{1+x^n}-x}&=\abs{\frac{x^{n+1}}{1+x^n}}\\
&\leq x^{n+1}\\
&\leq a^{n+1}
\end{align*}
Clearly $a^{n+1}\to 0$ as $n\to\infty$. Thus the sequence $\frac{x}{1+x^n}\to x$ regardless of $x$. Now consider $[b,\infty)$, we have that 
\begin{align*}
\abs{\frac{x}{1+x^n}}&=\abs{\frac{x}{x^n}}\\
&=\abs{\frac{1}{x^{n-1}}}\\
&\leq\frac{1}{b^{n-1}}
\end{align*}
Clearly $\frac{1}{b^{n-1}}\to 0$ as $n\to\infty$. Thus the sequence $\frac{x}{1+x^n}\to 0$ regardless of $x$. 
\end{enumerate}\tcbline
\end{proof}
\end{qtn}

\subsection{Uniform Convergence and Integrability}

\subsection{Uniform Convergence and Differentiability}
\begin{qtn}{}{}
\thetcbcounter.\;\; Consider the sequence of functions $$g_n(x)=\frac{x^n}{n}$$ for $x\in[0,1]$ and $n\in\N$. 
\begin{enumerate}[label=\alph*)]
\item Does $g_n(x)$ converge pointwise for $x\in[0,1]$? Does it converge uniformly? Determine the limit $g(x)$ if it exists and differentiate it to find $g'(x)$. 
\item Does $(g_n'(x))_{n\in\N}$ converge pointwise and/or uniformly on $[0,1]$? If $\lim_{n\to\infty}g_n'=h$, then is $h$ equal to $g'$?
\end{enumerate}\tcbline
\begin{proof}~\\
\begin{enumerate}[label=\alph*)]
\item Clearly it is pointwise convergence since $\abs{\frac{x^n}{n}}\leq\frac{1}{n}$ for all $n$. Thus $g_n(x)$ pointwise converges to $g(x)=0$. It is also uniformly convergent since the fact that $\abs{\frac{x^n}{n}}\leq\frac{1}{n}$ does not change as $x$ changes. Thus $g'(x)=0$. 
\item We have that $g_n'(x)=x^{n-1}$. As $n\to\infty$, we see that the exponent goes to infinity thus it is not pointwise convergent to any function. Since it is not pointwise convergent, it is not uniformly convergent. Then clearly $h\neq g'$ and we proved the importance of uniform convergence in the theorem that allows exchange of the limit operator and the differential operator. 
\end{enumerate}
\end{proof}
\end{qtn}

\subsection{Series of Functions}
\begin{qtn}{}{}
\thetcbcounter.\;\; Show that $$g(x)=\sum_{k=1}^\infty\frac{\cos(2^kx)}{2^k}$$ is continuous. \\\hspace*{\fill}\cite{R0002}\tcbline
\begin{proof}~\\
Define $g_k(x)=\frac{\cos(2^kx)}{2^k}$. Clearly $\sum_{k=1}^ng_k(x)$ is continuous since it is a finite sum of continuous functions. We also have that 
\begin{align*}
\abs{g_k(x)}&=\abs{\frac{\cos(2^kx)}{2^k}}\\
&\leq\frac{1}{2^k}
\end{align*}
Also we see that $\sum_{k=1}^\infty\frac{1}{2^k}=1$. The conditions for the Weierstrass M-test are fulfilled and we have that $g(x)=\sum_{k=1}^\infty g_k$ is uniformly convergent. Then since uniform convergence preserves continuity, we have that $g(x)$ is continuous. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Show that $$h(x)=\sum_{k=1}^\infty\frac{x^k}{k^2}$$ is continuous. \\\hspace*{\fill}\cite{R0002}\tcbline
\begin{proof}~\\
Define $h_k(x)=\sum_{k=1}^nf_k(x)$ where $f_k(x)=\frac{x^k}{k^2}$. When $\abs{x}\leq 1$, we have that $f_k(x)\leq\frac{1}{k^2}$. And we know that $\sum_{k=1}^\infty\frac{1}{k^2}$ is convergent. Thus we can apply the Weierstrass M-test and $h_k(x)$ converges uniformly to $h(x)$. $h_k(x)$ is a sequence of continuous functions since it is a sum of continuous functions. Thus $h(x)$ is continuous by the fact that uniform convergence preserve continuity. 
\end{proof}
\end{qtn}

\begin{qtn}{}{}
\thetcbcounter.\;\; Let $$h(x)=\sum_{k=1}^\infty\frac{1}{x^2+k^2}$$
\begin{enumerate}[label=\alph*)]
\item Show that $h$ is continuous on $\R$
\item Is $h$ differentiable? If so, is the derivative function $h'$ continuous?
\end{enumerate}~\\\hspace*{\fill}\cite{R0002}\tcbline
\begin{proof}~\\
\begin{enumerate}[label=\alph*)]
\item Define $h_n(x)=\sum_{k=1}^nf_k$ where $f_k(x)=\frac{1}{x^2+k^2}$. Then clearly $\abs{f_k(x)}\leq\frac{1}{k^2}$ and that $\sum_{k=1}^\infty\frac{1}{k^2}$ converges, thus $h_n(x)$ uniformly converges to $h(x)$. Since $h_n(x)$ is a sum of continuous functions, $h(x)$ is also continuous using the fact that uniform convergence preserves continuity. 
\item Clearly $h_n\in C^1$ for all $n\in\N$. Since its derivative $$h_n'(x)=-2x\sum_{k=1}^n\frac{1}{(x^2+k^2)^2}$$ is continuous. We need to show that $h_n'(x)$ is unifomrly convergent. Note that 
\begin{align*}
\abs{h_n'(x)}&\leq\sum_{k=1}^n\frac{2\abs{x}}{(x^2+k^2)^2}\\
&\leq2\sum_{k=1}^n\frac{x^2+k^2}{(x^2+k^2)^2}\tag{Cauchy-Schwarz}\\
&\leq2\sum_{k=1}^n\frac{1}{k^2}
\end{align*}
Thus we have that $$\abs{h_n(x)-h_m(x)}\leq\sum_{k=m+1}^n\frac{1}{n^2}$$. Thus we have that $(h_n(x))_{n\in\N}$ is uniformly Cauchy and thus is uniformly convergent. This means that $h(x)$ is differentiable. Since $h_n'(x)$ are continuous, we have that $h_n'(x)\to h'(x)$ is continuous. 
\end{enumerate}
\end{proof}
\end{qtn}

\pagebreak
\section{References}
\bibliographystyle{plain}
\bibliography{C:/Users/liula/Desktop/Latex/ref.bib}







\end{document}