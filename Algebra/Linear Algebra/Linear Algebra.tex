\documentclass[a4paper]{article}

\input{C:/Users/liula/Desktop/Latex/Headers V1.2.tex}

\pagestyle{fancy}
\fancyhf{}
\rhead{Labix}
\lhead{Linear Algebra}
\rfoot{\thepage}

\title{Linear Algebra}

\author{Labix}

\date{\today}
\begin{document}
\maketitle
\begin{abstract}
Linear algebra is at the heart of mathematics. Almost all areas of mathematics make some use of the notion of vector spaces and its properties. It began with the study of systems of linear equations. \\~\\

Nowadays as we understand vector spaces independently from systems of linear equations, we will also treat the material differently. The first three chapter begins with the basis definitions: vector spaces and the maps between them called linear maps. Eigenvalues and eigenspaces will be an important invariant for vector spaces. Together with the aid of matrices, we will have a good grasp of how to write a given vector space in a simpler form. Chapter 4 will then improve on the further simplifying a given matrix so that we can read information from each easily. \\~\\

The rest of the chapters will focus on particular properties of vector spaces and linear maps. They each correspond to an important class of matrices. For examples, quadratic form corresponds to matrices equivalent up to congruency while orthogonality of basis vectors give orthogonal matrices. 
\end{abstract}
\pagebreak
\tableofcontents
\pagebreak

\section{Vector Spaces}
\subsection{Introduction to Vector Spaces}
\begin{defn}{Vector Space}{} Let $\F$ be a field. A vector space over $\F$ is an abelian group $V$ together with a function $\cdot:\F\times V\to V$ such that the following are true. 
\begin{itemize}
\item $\lambda\cdot(v+w)=\lambda\cdot v+\lambda\cdot w$ for all $v,w\in V$ and $\lambda\in\F$
\item $(\lambda\mu)\cdot v=\lambda\cdot(\mu\cdot v)$ for all $v\in V$ and $\lambda,\mu\in\F$
\item $(\lambda+\mu)\cdot v=\lambda\cdot v+\mu\cdot v$ for all $v\in V$ and $\lambda,\mu\in\F$
\item $1_\F\cdot v=v$ for all $v\in V$. 
\end{itemize}
\end{defn}

\begin{prp}{}{} Let $a\in\mathbb{F}$ and $u\in V$ be a vector space over $\mathbb{F}$. 
\begin{itemize}
\item $a\cdot \vb{0}_V=\vb{0}_V$
\item $0\cdot \vb{u}=\vb{0}_V$
\item $(-a)\vb{v}=-(a\vb{v})=a(-\vb{v})$
\item $a\vb{v}=\vb{0}_V\implies a=0$ or $\vb{v}=\vb{0}_V$
\end{itemize}\tcbline
\begin{proof} ~\\
\begin{itemize}
\item $a(\vb{0}_V)=a(\vb{0}_V+\vb{0}_V)=a\vb{0}_V+a\vb{0}_V$. Adding the additive inverse of $a\vb{0}_V$ on both sides gives our result. 
\item $0\vb{u}=(0+0)\vb{u}=0\vb{u}+0\vb{u}$. Adding the additive inverse of $0\vb{u}$ on both sides gives our result. 
\item Naturally $-(a\vb{v})$ is the inverse of $a\vb{v}$. Consider $a\vb{v}+a(-\vb{v})$. $a\vb{v}+a(-\vb{v})=a(\vb{v}-\vb{v})=a\vb{0}_V=0_V$. Thus $a(-\vb{v})$ is also the inverse of $a\vb{v}$ and $-(a\vb{v})=a(-\vb{v})$. The same could be done to the third item with the other distributive law. 
\item Suppose that $a\neq0$. Then $\vb{v}=(a^{-1}a)\vb{v}=a^{-1}(a\vb{v})=0$. 
\end{itemize}
\end{proof}
\end{prp}

\begin{prp}{}{} The additive identity, multiplicative identity, additive inverse of a vector space is unique. \tcbline
\begin{proof} Suppose that $\vb{e}$ and $\vb{f}$ are additive identities. Then $\vb{e}+\vb{f}=\vb{e}$ and $\vb{e}+\vb{f}=\vb{f}$. Thus $\vb{e}=\vb{f}$. Suppose that $\vb{e}$ and $\vb{f}$ are multiplicative identities. Then $\vb{e}\vb{f}=\vb{e}$ and $\vb{e}\vb{f}=\vb{f}$ and $\vb{e}=\vb{f}$. Let $a\in V$. Suppose that $b,c\in V$ are additive inverses of $a$. Then 
\begin{align*}
a+b=a+c&\implies b+a+b=b+a+c\\
&\implies(b+a)+b=(b+a)+c\\
&\implies b=c
\end{align*}
\end{proof}
\end{prp}

\subsection{Basis and Dimension}
\begin{defn}{Linearly Independent}{} We say that a set of vectors $\{v_1,\dots,v_n\}$ of a vector space $V$ over $\F$ are linearly independent if $$\sum_{k=1}^na_kv_k=0$$ for $a_1,\dots,a_n\in\F$ implies $a_1=\dots=a_n=0$. 
\end{defn}

\begin{defn}{Span}{} We say that a set of vectors $\{v_1,\dots,v_n\}$ of a vector space $V$ over $\F$ spans $V$ if for all $v\in V$, there exists $a_1,\dots,a_n\in\F$ such that $$v=\sum_{k=1}^na_kv_k$$
\end{defn}

\begin{defn}{Basis}{} We say that a set of vectors $\{v_1,\dots,v_n\}$ of a vector space forms a basis for $V$ if they are linearly independent and spans $V$. 
\end{defn}

\begin{defn}{Dimension}{} We say that the dimension of a vector space $V$ is the number of elements in a basis of $V$. If a basis has $n$ elements, then we say that $\dim(V)=n$. \\~\\
If $n$ is a finite number, then we say that $V$ is finite dimensional. 
\end{defn}

We have yet to shown that the dimension of a vector space is well defined since we do not know whether the cardinality of any two bases are the same. Therefore we have the following important theorem for finite dimensional vector space. 

\begin{thm}{Steinitz Exchange Lemma}{} Let $U,W$ be finite subsets of a finite dimensional vector space $V$. If $U$ is a set of linearly independent vectors and $W$ spans $V$, then 
\begin{itemize}
\item $\abs{U}\leq\abs{W}$
\item There exists a set $W'\subset W$ with $\abs{W'}=\abs{W}-\abs{U}$ such that $U\cup W'$ spans $V$. 
\end{itemize} \tcbline
\begin{proof}
Take $U=\{u_1,\dots,u_m\}$ and $W=\{w_1,\dots,w_n\}$. We will show that after reordering elements of $W$, we will have a set $\{u_1,\dots,u_m,w_m+1,\dots,w_n\}$ that it spans $V$. We proceed by induction on $m$. Suppose that $m=0$. In this case, $\abs{U}\leq\abs{W}$ necesssarily holds and by construction, $W$ already spans $V$. \\~\\
Now suppose that the proposition is true for $m-1$. By the induction hypothesis, we may reorder elements of $W$ so that $\{u_1,\dots,u_{m-1},w_m,\dots,w_n\}$ spans $V$. Since $u_m\in V$, there exists $a_1,\dots,a_n$ such that $$u_m=\sum_{k=1}^{m-1}a_ku_k+\sum_{k=m}^na_kw_k$$ At least one of $a_m,\dots,a_n$ must be nonzero else the equality will contradict the linear independence of $u_1,\dots,u_m$. This must mean that $m\leq n$. \\~\\
Now by reordering $a_mw_m,\dots,a_nw_n$, we may assume that $a_m\neq 0$. Thus we have that $$w_m=\frac{1}{a_m}\left(u_m-\sum_{k=1}^{m-1}a_ku_k-\sum_{k=m+1}^na_kw_k\right)$$ This means that $w_m$ lies in the span of $\{u_1,\dots,u_m,w_{m+1},\dots,w_n\}$. Since this span contains each of the vectors $u_1,\dots,u_{m-1},w_m,\dots,w_n$, by the inductive hypothesis it spans $V$. 
\end{proof}
\end{thm}

Clearly this implies that linearly independent sets of vectors must have cardinality less than sets of vectors that span $V$. By taking the highest cardinality of such linearly indendent set of vectors, and the lowest cardinality of such sets of vectors that span $V$, we necessarily have that they are equal and thus is exactly the dimension of $V$. \\

We will discuss about dimensions and infinite dimensional vector spaces more in functional analysis. For the rest of the notes we will mostly go with finite dimensional vector spaces. \\

We now give a criterion with matrices to find whether a set of vectors span $V$ or whether they are linearly independent. 

\begin{thm}{}{} Let $V$ be a vector space of dimension $n$ and $S=\{v_1,\dots,v_n\}\subset V$. Then 
\begin{itemize}
\item Elements of $S$ are linearly independent if and only if the row echelon form of $\begin{pmatrix}v_1 & \cdots & v_n\end{pmatrix}$ has a leading one in every column
\item Elements of $S$ span $V$ if and only if the row echelon form of $\begin{pmatrix}v_1 & \cdots & v_n\end{pmatrix}$ has no zero rows
\item $S$ is a basis of $V$ if and only if the row echelon form of $\begin{pmatrix}v_1 & \cdots & v_n\end{pmatrix}$ is equal to the identity
\end{itemize}
\end{thm}

\subsection{Vector Subspaces}
\begin{defn}{Vector Subspaces}{} Let $\F$ be a field. Let $V$ be a vector space over $\F$. A vector subspace of $V$ is a subset $U\subseteq V$ such that $U$ is a vector space over $\F$ in its own right. 
\end{defn}

\begin{prp}{Subspace Criterion}{} Let $\F$ be a field. Let $V$ be a vector space over $\F$. Let $U\subseteq V$ be a subset. Then $U$ is a subspace of $V$ if and only if the following is true. 
\begin{itemize}
\item $0_V\in U$
\item For all $u_1,u_2\in U$, $u_1+u_2\in U$
\item For all $u\in U$ and $\lambda\in\F$, $\lambda\cdot u\in U$
\end{itemize} \tcbline
\begin{proof}
Suppose that $U$ is a subspace of $V$. Then necessarily $U$ is closed under vector addition and scalar multiplication and contains the zero vector. \\~\\
Now suppose that the latter conditions are fulfilled by a subset $U$ of $V$. Then it is easy to see that $U$ satisfies all the criteria for being a vector space. 
\end{proof}
\end{prp}

\begin{prp}{}{} If $U_1$ and $U_2$ are subspaces of $V$ then $U_1\cap U_2$ is also a subspace. \tcbline
\begin{proof} Suppose that $\vb{v},\vb{w}\in U_1\cap U_2$. Then $\vb{v},\vb{w}\in U_1$ and $U_2$. Since $U_1,U_2$ are subspaces, $\vb{v}+\vb{w}\in U_1$ and $U_2$ thus $\vb{v}+\vb{w}\in U_1\cap U_2$. The proof is similar for scalar multiplication. 
\end{proof}
\end{prp}

\begin{defn}{Sum of Subspaces}{} Let $U,W$ be subspaces of the vector space $V$. Then define $$U+W=\{\vb{u}+\vb{w}:\vb{u}\in U\text{ and }\vb{w}\in W\}$$
\end{defn}

\begin{prp}{}{} Let $U,W$ be subspaces of a vector space $V$. Then $U+W$ is the smallest subspace of $V$ containing $U$ and $W$. \tcbline
\begin{proof}
We first show that $U+W$ is indeed a subspace of $V$. Suppose that $v\in U+W$. Then there exists $u\in U$ and $w\in W$ such that $v=u+w$. Then since $U$ and $W$ are closed individually under vector addition and scalar multiplication, any product and addition in $U+W$ can be decomposed into a sum of vectors in $U$ and $W$ and thus the new vector will also be able to be decomposed into $U$ and $W$ and thus lie in $U+W$. \\~\\
Now suppose that $S$ is a subspace of $V$ containing $U$ and $W$. This means that any linear combination of elements of $U$ and $W$ are contained in $S$ thus $U+W\subseteq S$. This means that if any subspace containing $U$ and $W$ must also contain $U+W$, which means that $U+W$ is the smallest subspace containing $U$ and $W$. 
\end{proof}
\end{prp}

\begin{defn}{Independent Subspaces}{} Let $W_1,\dots,W_n$ be subspaces of a vector space $V$. We say that $W_1,\dots,W_n$ are independent if no vector of $W_i$ is a linear combination of the remaining subspaces for every $i\in\{1,\dots,n\}$
\end{defn}

\begin{defn}{Direct Sum}{} A vector space is the direct sum of its subspaces $$V=W_1\oplus\dots\oplus W_n$$ if $W_1,\dots,W_n$ are independent and $V=W_1+\dots+W_n$. 
\end{defn}

\begin{crl}{}{} If $V=W_1\oplus\dots\oplus W_n$ then $$\dim(V)=\sum_{k=1}^n\dim(W_k)$$ \tcbline
\begin{proof}
Each basis of $W_k$ are not contained in any other linear combination of all the basis of $W_1,\dots,W_{k-1},W_{k+1},\dots,W_n$. This means that the set of all the basis of $W_1,\dots,W_n$ are linearly independent. Since they each span $W_k$ independently, the set of all the basis of $W_1,\dots,W_n$ will span $W_1\oplus\dots\oplus W_n$ and thus is a basis of $V$. Thus we are done. 
\end{proof}
\end{crl}

\subsection{Row and Column Ranks}
The final section is devoted to matrices as we will soon see that matrices are particularly useful in a lot of things. 
\begin{defn}{Row Space}{} Let $\F$ be a field and let $A\in M_{m\times n}(\F)$ be a matrix. The row space of $A$ is the subspace of $\mathbb{F}^m$, $$\Span\{r_1,\dots,r_m\}$$ where $r_i$ are the rows of $A$. The row rank of $A$ is defined to be the dimension of the row space of $A$. 
\end{defn}

\begin{defn}{Column Space}{} Let $\F$ be a field and let $A\in M_{m\times n}(\F)$ be a matrix. The column space of $A$ is the subspace of $\mathbb{F}^n$, $$\Span\{c_1,\dots,c_m\}$$ where $c_i$ are the columns of $A$. The column rank of $A$ is defined to be the dimension of the column space of $A$. 
\end{defn}

\begin{lmm}{}{} Applying row operations does not change the row space, row rank of a matrix and column rank of a matrix. 
\end{lmm}

\begin{thm}{}{} The row rank of a matrix is equal to the column rank. 
\end{thm}

We can now define the rank of a matrix without problem. 

\begin{defn}{Rank of a Matrix}{} Define the rank of a matrix to be its row rank or column rank. 
\end{defn}

\begin{prp}{}{} Let $\F$ be a field and let $A\in M_{n\times n}(\F)$ be a matrix. Then the following are equivalent. 
\begin{itemize}
\item The rank of $A$ is $n$
\item $A$ is invertible
\item The rows of $A$ form a linearly independent set
\item The columns of $A$ form a linearly independent set
\end{itemize}
\end{prp}

\pagebreak
\section{Preserving the Vector Space Structure}
\subsection{Linear Transformations}
\begin{defn}{Linear Transformation}{} Let $V,W$ be vector spaces over a field $\F$. A linear transformation or linear map $T$ from $V$ to $W$ is a function $$T:V\to W$$ such that 
\begin{itemize}
\item $T(v_1+v_2)=T(v_1)+T(v_2)$ for all $v_1,v_2\in V$
\item $T(kv)=kT(v)$ for all $k\in\mathbb{F}$, $v\in V$
\end{itemize}
\end{defn}

\begin{lmm}{}{} Let $V,W$ be vector spaces over a field $\F$. Let $T:V\to W$ be a linear map. Then the following are true. 
\begin{itemize}
\item $T(0_v)=0_w$
\item $T(-v)=-T(v)$ for all $v\in V$
\end{itemize}\tcbline
\begin{proof} Suppose that $v\in V$. Then $T(0\cdot v)=0\cdot T(v)=0$. Also we have that $T(0-v)=T(0)-T(v)=-T(v)$. 
\end{proof}
\end{lmm}

\begin{prp}{}{} Let $U,V,W$ be vector spaces over a field $\F$. If $T:U\to V$ and $S:V\to W$ are linear then $$S\circ T:U\to W$$ is also linear. \tcbline
\begin{proof} Let $au+bv\in U$. 
\begin{align*}
S\circ T(au+bv)&=S(aT(u)+bT(v))\\
&=a(S\circ T(u))+b(S\circ T(v))
\end{align*}
\end{proof}
\end{prp}

\begin{prp}{Universal Property of Vector Spaces}{} Let $\F$ be a field. Let $V,W$ be vector spaces over $\F$. Let $B$ be a basis of $V$. Let $f:B\to W$ be a function. Then there exists a unique linear map $T:V\to W$ such that the following diagram commutes: \\~\\
\adjustbox{scale=1.0,center}{\begin{tikzcd}
	B & V \\
	& W
	\arrow["\iota", hook, from=1-1, to=1-2]
	\arrow["f"', from=1-1, to=2-2]
	\arrow["{\exists!T}", dashed, from=1-2, to=2-2]
\end{tikzcd}} \\~\\
\end{prp}

\subsection{Isomorphisms}
\begin{defn}{Isomorphic Linear Maps}{} Let $V,W$ be vector spaces over a field $\F$. Let $T:V\to W$ be a linear. We say that $T$ is an isomorphism if there exists a linear map $S:W\to V$ such that $$S\circ T=\text{id}_V\;\;\;\;\text{ and }\;\;\;\;T\circ S=\text{id}_W$$
\end{defn}

\begin{prp}{}{} Let $V,W$ be vector spaces over a field $\F$. Let $T:V\to W$ be a linear map. Then the following are equivalent. 
\begin{itemize}
\item $T$ is an isomorphism. 
\item $T$ is bijective. 
\item $v_1,\dots,v_n\in V$ is basis for $V$ if and only if $T(v_1),\dots,T(v_n)$ is a basis for $W$. 
\end{itemize}
\end{prp}

\begin{crl}{}{} Every finite dimensional vector space over $\R$ is isomorphic to $\R^n$ for some $n\in\N\setminus\{0\}$. \tcbline
\begin{proof} Direct consequence from the above. 
\end{proof}
\end{crl}

This corollary is especially important since it tells use that we only really need to study all of $\R^n$ to study all of finite dimensional spaces. Once we have our results on $\R^n$, we can translate it via an isomorphism. 

\begin{defn}{The Space of all Linear Maps}{} Let $V,W$ be vector spaces over a field $\F$. Define the space of all linear maps to be the set $$\Hom_\F(V,W)=\{T:V\to W\;|\;T\text{ is a linear map}\}$$ together with the following operations. 
\begin{itemize}
\item For two linear maps $T,S:V\to W$, define its sum to be $T+S:V\to W$ defined by $(T+S)(v)=T(v)+S(v)$ for all $v\in V$
\item For a linear map $T:V\to W$ and a scalar $\lambda\in\F$, define its product to be $\lambda T:V\to W$ defined by $(\lambda T)(v)=\lambda T(v)$ for all $v\in V$. 
\end{itemize}
\end{defn}

\begin{lmm}{}{} Let $V,W$ be vector spaces over a field $\F$. Then $\Hom_\F(V,W)$ is a vector space over $\F$. 
\end{lmm}

\subsection{Kernels and Images}
\begin{defn}{Kernel of Linear Map}{} Let $\F$ be a field. Let $V,W$ be vector spaces over $\F$. Let $T:V\to W$ be a linear map. Define the kernel of $T$ to be the set $$\ker(T)=\{v\in V\;|\;T(v)=0\}$$
\end{defn}

\begin{prp}{}{} Let $T:U\to V$ be a linear map. Then the following are true. 
\begin{itemize}
\item $\im(T)$ is a subspace of $V$
\item $\ker(T)$ is a subspace of $U$
\end{itemize}\tcbline
\begin{proof} Let $T:U\to V$ be a linear map. 
\begin{itemize}
\item We prove that $\im(T)$ is a subspace of $V$. Let $\vb{u},\vb{v}\in\im(T)$ and $a\in\F$. Since $\vb{u},\vb{v}\in\im(T)$, there exists $\vb{u}_0,\vb{v}_0\in U$ such that $T(\vb{u}_0)=\vb{u}$ and $T(\vb{v}_0)=\vb{v}$. Note that $a\vb{u}_0\in U$ and $\vb{u}_0+\vb{v}_0\in U$. Consider $T(a\vb{u}_0)$. We have $T(a\vb{u}_0)=aT(\vb{u}_0)=a\vb{u}$. Thus $a\vb{u}\in\im(T)$. Similarly, $T(\vb{u}_0+\vb{v}_0)=T(\vb{u}_0)+T(\vb{v}_0)=\vb{u}+\vb{v}$. Thus $\vb{u}+\vb{v}\in\im(T)$. By the subspace criterion $\im(T)$ is a subspace of $V$. 
\item We now prove that $\ker(T)$ is a subspace of $U$. Suppose that $u,v\in\ker(T)$ and $a,b\in\F$. Then $T(au+bv)=aT(u)+bT(v)=0$. Thus $au+bv\in\ker(T)$. 
\end{itemize}
\end{proof}
\end{prp}

\begin{defn}{Rank and Nullity}{} Let $T:U\to V$ be a linear map. 
\begin{itemize}
\item $\rank(T)=\dim(\im(T))$ is said to be the rank of $T$. 
\item $\nullity(T)=\dim(\ker(T))$ is said to be the nullity of $T$. 
\end{itemize}
\end{defn}

\begin{thm}{Rank Nullity Theorem}{} Let $T:U\to V$ be a linear map. Then $$\rank(T)+\nullity(T)=\dim(U)$$
\end{thm}

\begin{thm}{}{} Let $T:U\to V$ be a linear map, where $\dim(U)=n$, $\dim(V)=m$. Let $e_1,\dots,e_n$ be a basis of $U$. Then the rank of $T$ is equal to the largest size of a linearly independent subset of $T(e_1),\dots,T(e_n)$. 
\end{thm}

\subsection{Role of Matrices}
\begin{defn}{Matrix of a Linear Map}{} Let $U,V$ be vector spaces over a field $\F$. Let $T:U\to V$ be a linear map where $\dim(U)=n$ and $\dim(V)=m$. Let $\vb{e}_1,\dots,\vb{e}_n$ be a basis of $U$ and $\{\vb{f}_1,\dots,\vb{f}_m\}$ a basis of $V$. Let $$T(\vb{e}_i)=\sum_{k=1}^m\alpha_{ki}\vb{f}_k$$ for $i\in\{1,\dots,n\}$. We then say that the matrix $$\begin{pmatrix}
\alpha_{11} & \alpha_{12} & \cdots & \alpha_{1n}\\
\alpha_{21} & \alpha_{22} & \cdots & \alpha_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
\alpha_{m1} & \alpha_{m2} & \cdots & \alpha_{mn}\\
\end{pmatrix}$$ represents the linear map $T$ under the given bases. 
\end{defn}

\begin{thm}{}{} Let $U,V$ be vector spaces over a field $\F$. Let $T:U\to V$ be a linear map. Choose a basis for $U$ and $V$ and let $A$ represent $T$ under these bases. Then $$T(v)=Av$$
\end{thm}

\begin{thm}{}{} The rank of a matrix equals the rank of any map that it represents. 
\end{thm}

\begin{thm}{}{} The composition of linear maps is represented by the matrix product of its representatives. 
\end{thm}

\begin{thm}{}{} Let $T:V\to W$ be a linear map. Then $T$ is a vector space isomorphism if and only if its matrix is nonsingular. 
\end{thm}

\subsection{Change of Bases}
We have seen that matrices represent a linear map after choosing bases for the domain and codomain vector spaces. However, when does two matrices represent the same linear map but in different bases?

\begin{defn}{Change of Basis}{} Let $V$ be a vector space over a field $\F$. Let $E=\{e_1,\dots,e_n\}$ and $F=\{f_1,\dots,f_n\}$ be two bases of $V$. We say that a linear map $T:V\to V$ is a change of bases from $E$ to $F$ if $T(e_k)=f_k$ for $1\leq k\leq n$. 
\end{defn}

\begin{prp}{}{} Let $V$ be a vector space over a field $\F$. Let $E=\{e_1,\dots,e_n\}$ and $F=\{f_1,\dots,f_n\}$ be two bases of $V$. Write $$e_k=p_{k1}v_1'+\dots+p_{kn}f_n$$ for all $1\leq k\leq n$. Then the matrix $$P=\begin{pmatrix}
p_{11}&\cdots&p_{1n}\\
\vdots&\ddots&\vdots\\
p_{n1}&\cdots&p_{nn}
\end{pmatrix}$$ represents the change of bases linear map from $E$ to $F$. In particular, this means that if $v\in V$ is given in the basis $E$, then $Pv\in V$ represents the same vector in the basis $F$. 
\end{prp}

\begin{lmm}{}{} Let $V$ be a vector space over a field $\F$. Then any change of bases matrix $P$ is invertible. 
\end{lmm}

\begin{thm}{}{} Let $V,W$ be vector spaces over a field $\F$. Let $V,W$ be vector spaces. Let $T:V\to W$ be a linear map. Moreover, choose bases $B$ and $B'$ for $V$ and choose bases $C$ and $C'$ for $W$ respectively. 
\begin{itemize}
\item Let the matrix $A$ represent $T$ under the bases $B$ and $C$. 
\item Let the matrix $A'$ represent $T$ under the bases $B'$ and $C'$. 
\item Let $P$ be the change of bases matrix of $V$ from $B$ to $B'$. 
\item Let $Q$ be the change of bases matrix of $W$ from $C$ to $C'$. 
\end{itemize}
Then we have that $$A'=QAP^{-1}$$
\end{thm}

It is often easier to depict this as a diagram. Recall that we write $V_B$ to indicate the vector space $V$ is given the basis $B$. Using this notation we obtain the following commutative diagram as a result of the above theorem. \\~\\
\adjustbox{scale=1.0,center}{\begin{tikzcd}
	{V_B} & {W_C} \\
	{V_{B'}} & {W_{C'}}
	\arrow["A", from=1-1, to=1-2]
	\arrow["P"', from=1-1, to=2-1]
	\arrow["Q", from=1-2, to=2-2]
	\arrow["{A'}"', from=2-1, to=2-2]
\end{tikzcd}}\\~\\
Even better, we could have equivalently stated in the theorem that this commutative diagram is the result. 

\begin{defn}{Similar Matrices}{} Let $\F$ be a field. We say that two matrices $A,B\in M_{n\times n}(\F)$ are similar if there exists an invertible matrix $P\in M_{n\times n}(\F)$ such that $B=PAP^{-1}$. 
\end{defn}

\begin{lmm}{}{} Let $\F$ be a field. The relation of similarity in matrices is an equivalent relation in $M_{n\times n}(\F)$. 
\end{lmm}

Using the equivalence relation, we can express a one-to-one correspondence as follows. 

\begin{thm}{}{} Let $V$ be a vector space over a field $\F$. Then there is a one-to-one correspondence $$\Hom_\F(V,V)\;\;\overset{1:1}{\longleftrightarrow}\;\;\frac{M_{n\times n}(\F)}{\cong}$$ where $\cong$ is the equivalence relation of similar matrices, given as follows. 
\begin{itemize}
\item For $T:V\to V$ a linear map, choose a basis for $V$ and let $A\in M_{n\times n}$ represent the matrix $T$. Then the forward map sends $T$ to $[A]$
\item For $[A]$ an equivalence class of similar matrices, it is mapped to the linear map $T:V\to V$ given by $T(v)=Av$. 
\end{itemize}
\end{thm}

\pagebreak
\section{Eigenspaces}
Eigenspaces are invariants of a linear map. Every vector in the eigenspace will only be scaled while maintaining its direction. 

\subsection{Eigenvalues and Eigenvectors}
\begin{defn}{Eigenvalues and Eigenvectors}{} Let $V$ be a vector space over a field $\F$. Let $T:V\to V$ be a linear map. An eigenvector of $T$ is a vector $v\in  V$ such that $$T(v)=\lambda v$$ for some scalar $\lambda\in\F$. In this case we say that $\lambda$ is an eigenvalue of $T$. 
\end{defn}

By choosing a basis for $V$ an representing the linear map $T$ by a matrix $A$, we notice that the concept of eigenvalues and eigenvectors can be translated to matrices. \\

Notice that two eigenvectors can have the same eigenvalue. For instance, if $v$ is an eigenvector of $\lambda$ and $a$ is a scalar, then $av$ is also an eigenvector of $\lambda$ because $$T(av)=aT(v)=a(\lambda v)=\lambda(av)$$ In particular, the collection of all eigenvectors of an eigenvalue forms a vector subspace of $V$. 

\begin{defn}{Eigenspace}{} Let $V$ be a vector space over a field $\F$. Let $T:V\to V$ be a linear map. Let $\lambda\in\F$ be an eigenvalue of $T$. Define the eigenspace of $\lambda$ to be the vector subspace $$E_\lambda=\{v\in V\;|\;T(v)=\lambda v\}$$
\end{defn}

\begin{lmm}{}{} Let $V$ be a vector space over a field $\F$. Let $T:V\to V$ be a linear map. Let $\lambda\in\F$ be an eigenvalue of $T$. Then $$E_\lambda=\ker(T-\lambda I)$$ \tcbline
\begin{proof}
Let $v\in V$. We have that 
\begin{align*}
T(v)=\lambda v&\iff T(v)-\lambda v=0\\
&\iff (T-\lambda I)(v)=0\\
&\iff v\in\ker(T-\lambda I)
\end{align*}
and so we conclude. 
\end{proof}
\end{lmm}

\begin{prp}{}{} Let $\lambda_1,\dots,\lambda_r$ be distinct eigenvalues of $T:V\to V$, and let $v_1,\dots,v_r$ be the corresponding eigenvectors. Then $v_1,\dots,v_r$ are linearly independent. 
\end{prp}

As we can see, distinct eigenvalues are linearly independent. Considering the span of each eigenvectors, we can clearly see that each of their spans are independent. 

\begin{prp}{}{} If $\lambda_1,\dots,\lambda_n$ are distinct eigenvalues of a matrix $A$, then $E_{\lambda_1},\dots,E_{\lambda_n}$ are independent. \tcbline
\begin{proof}
Clear from the fact that the basis of eigenspaces of different eigenvalues are linearly independent. 
\end{proof}
\end{prp}

\begin{defn}{Characteristic Polynomial}{} Let $A$ be an $n\times n$ matrix. $$c_A(x)=\det(A-xI_n)$$ is called the characteristic polynomial of $A$. 
\end{defn}

\begin{prp}{}{} Let $A$ be an $n\times n$ matrix. Then $\lambda$ is an eigenvalue of $A$ if and only if $$c_A(\lambda)=0$$
\end{prp}

\begin{defn}{Invariant Subspaces}{} Let $T:V\to V$ be a linear transformation. Let $U$ be a subspace of $V$. We say that $U$ is $T$-invariant if $$v\in U\implies T(v)\in U$$ for all $v\in U$ or equivalently, $T(U)\subseteq U$. 
\end{defn}

\begin{thm}{}{} Eigenspaces is an invariant subspace under its linear transformation. 
\end{thm}

The main result of this subsection, stated that eigenspaces remain invariant under the linear transformation. Clearly this depends on the linear transformation. We will also show that this fact is also unchanged when considering different basis for the linear transformation. 

\subsection{Diagonalization}
We now show a very nice kind of matrices, diagonal matrices that will come into play with linear maps. Our goal is to attempt to classify, by similarity, of every matrix into a diagonal one. We will soon see that this is not possible, and thus giving the last section of these notes meaning. 

\begin{defn}{Diagonalizable Linear Maps}{} Let $V$ be a vector space over a field $\F$. Let $T:V\to V$ be a linear map. We say that $T$ is diagonalizable if there exists a basis of $V$ such that the matrix representing $T$ under the chosen basis is diagonal. 
\end{defn}

We can transfer this concept to matrices as well. 

\begin{defn}{Diagonalizable Matrices}{} Let $\F$ be a field. Let $A\in M_{n\times n}(\F)$ be a matrix. We say that $A$ is diagonalizable if there exists invertible matrices $P\in GL(n,\F)$ such that $$P^{-1}AP$$ is a diagonal matrix. 
\end{defn}

\begin{lmm}{}{} Let $V$ be a vector space over a field $\F$. Let $T:V\to V$ be a linear map. Then $T$ is diagonalizable if and only if any matrix $A\in M_{n\times n}(\F)$ representing $T$ under some basis is diagonalizable. 
\end{lmm}

The compact way of saying this is that $T$ is diagonalizable if and only if its equivalence class of similar matrices contain a diagonal matrix. \\

Ideally, we would like to study linear maps using the diagonal matrices because they appear to be very simple in nature. However, not every equivalence class of similar matrices contain a diagonal matrix and so this method is not feasible for every linear map. However, we can still find conditions for linear maps that are diagonalizable. This is highly related to eigenvalues and eigenspaces. 

\begin{thm}{}{} If the linear map $T:V\to V$ has $n$ distinct eigenvalues where $\dim(V)=n$, then $T$ is diagonalizable. 
\end{thm}

Although not stated in the theorem, this does not mean that linear maps without $n$ distinct eigenvalues are not diagonalizable. However by taking the contrapositive, we see that not every linear map is diagonalizable because clearly, not every linear map has $n$ distinct eigenvalues. 

\begin{thm}{}{} Let $V$ be a vector space over a field $\F$. Let $T:V\to V$ be a linear map. Let $\lambda_1,\dots,\lambda_m$ be the distinct eigenvalues of $T$. Then following are equivalent. 
\begin{itemize}
\item $T$ is diagonalizable
\item $V$ has a basis consisting of eigenvectors of $T$
\item $V=E_{\lambda_1}+\dots+E_{\lambda_m}$
\item $\dim(V)=\dim(E_{\lambda_1})+\dots+\dim(E_{\lambda_m})$
\end{itemize}
\end{thm}

\pagebreak
\section{The Cayley-Hamilton Theorem}
\subsection{The Minimal Polynomial}
\begin{thm}{}{} Let $\F$ be a field. Let $A$ be a $n\times n$ matrix over $\F$. Then there is some non-zero polynomial $p\in\F[x]$ of degree at most $n^2$ such that $p(A)=\vb{0}_n$. \tcbline\begin{proof} Note that $\{I,A,\dots,A^{n^2}\}$ is linerarly dependent in the vector space of $n\times n$ matrices. Thus there exists constant $c_0,\dots,c_{n^2}$ that are not all zero such that $$c_0I+\dots+c_{n^2}A^{n^2}=\vb{0}_n$$ Thus $p(x)=c_0+c_1x+\dots+c_{n^2}x^{n^2}$ is our desired polynomial. 
\end{proof}
\end{thm}

\begin{thm}{}{} Let $A_{n\times n}$ be a matrix over $\F$ representing the linear map $T:V\to V$. Then 
\begin{itemize}
\item There is a unique monic non-zero polynomial $p(x)$ with minimal degree and coefficients in $\F$ such that $p(A)=\vb{0}_n$
\item If $q(x)$ is any polynomial with $q(A)=\vb{0}_n$, then $p|q$
\end{itemize}\tcbline\begin{proof} By the previous theorem, there exists a polynomial such that $p(A)=0$. Divide the polynomial by $c_{n^2}$ gives us the desired monic polynomial. Suppose that $p_1,p_2$ are distinct monic polynomials that are minimal such that $p_1(A)=0$ and $p_2(A)=0$, then $p=p_1-p_2$ is a non zero polynomial with a smaller degree and $p(A)=0$, contradicting the minimality of degree. Thus $p$ is unique. \\~\\
Let $p(x)$ be the minimal polynomial in the above proof. Let $q(A)=0$. By division algorithm there exists some $r$ with smaller degree than $p$ such that $q=sp+r$. If $r$ is non-zero, then $r(A)=q(A)-s(A)p(A)=0$, contradiction of minimality, thus $r=0$ and $p|q$. 
\end{proof}
\end{thm}

\begin{defn}{The Minimal Polynomial}{} The unique monic non-zero polynomial $\mu_A(x)$ of minimal degree with $\mu_A(A)=\vb{0}_n$ is called the minimal polynomial of $A$. 
\end{defn}

\begin{prp}{}{} Similar matrices have the same minimal polynomial. \tcbline\begin{proof} Similar matrices represent the same linear map, thus both have their minimal polynomial same as $T$, the linear map. 
\end{proof}
\end{prp}

\begin{prp}{}{} Let $D$ be a diagonal matrix with $\{d_1,\dots,d_r\}$ its unqiue diagonal entries, then $$\mu_D(x)=(x-d_1)\cdots(x-d_r)$$\tcbline
\begin{proof} For any diagonal matrix, $$p(D)=\begin{pmatrix}p(d_{11}) & 0 & \cdots & 0\\ 0 & p(d_{22}) & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots\\ 0 & 0 & \cdots & p(d_{nn})\end{pmatrix}$$ Thus $p(D)=0$ if and only if $p(d_{kk})=0$ for $k\in\{1,\dots,n\}$. Thus the smallest-degree monic polynomial vanishing at these points is clearly the polynomial above. 
\end{proof}
\end{prp}

\begin{crl}{}{} Every diagonalizable matrix has its minimal polynomial a product of distinct linear factors. \tcbline
\begin{proof} Since diagonalizable matrix is similar to some diagonal matrix and they both have the same minimal polynomial, by the above proposition it is a product of distinct linear factors. 
\end{proof}
\end{crl}

We will later see that in fact, the above criterion is a neccessary and sufficient condition: $A$ is diagonalizable if and only if the minimal polynomial is a product of distinct linear factors. 

\begin{lmm}{}{} Let $\F$ be a field. Let $n,m\in\N\setminus\{0\}$. Let $A\in M_n(\F)$ and $B\in M_m(\F)$. Then $$c_M(x)=c_A(x)c_B(x)$$ and $$\mu_M(x)=\gcd(\mu_A(x),\mu_B(x))$$
\end{lmm}

\subsection{Cayley-Hamilton Theorem}
\begin{thm}{Cayley-Hamilton}{} Let $c_A(x)=\det(A-xI)$ be the characteristic polynomial of the $n\times n$ matrix $A$ over a field $\F$, then $c_A(A)=0$. \tcbline
\begin{proof} Firstly note that if $P(x)=\sum_{i=1}^nP_ix^i$ and $Q(x)=\sum_{j=1}^mQ_jx^j$ are polynomials with matrix coefficients where the matrix is $n\times n$, and $R(x)=\sum_{k=1}^{n+m}R_kx^k$ is the product of the two polynomials with $R_k=\sum_{i+j=k}P_iQ_j$, then if $M$ is a $n\times n$ matrix that commmutes with all of $Q_j$, then we have $$R(M)=P(M)Q(M)$$
This can be seen by expanding the sums out. \\~\\
Now take $Q(x)=A-xI$ and $P(x)=\adj(Q)$. Then we have $P(x)Q(x)=\det(A-xI)I=c_A(x)I$ by property of the adjoint. And since $A$ commutes with all coefficients of the polynomial of $Q$, we have $$c_A(A)I=P(A)Q(A)=P(A)\cdot 0=0$$ Thus $c_A(A)=0$. 
\end{proof}
\end{thm}

\begin{crl}{}{} For any $A_{n\times n}$ over $\F$, we have $\mu_A|c_A$, and $\deg(\mu_A)\leq n$. \tcbline
\begin{proof}
This is clear since $c_A(A)=0$ and $\mu_A$ is the minimal polynomial such that $\mu_A(A)=0$ by division with remainder. Since $\deg(c_A)=n$, $\deg(\mu_A)\leq n$. 
\end{proof}
\end{crl}

This lemma may help with finding out the minimal polynomial. 

\begin{lmm}{}{} Let $\lambda$ be an eigenvalue of $A$. Then $\mu_A(\lambda)=0$. \tcbline
\begin{proof}
Let $v$ be an eigenvector of the eigenvalue $\lambda$ of $A$. Trivially $\mu_A(A)v=0$. But also since $$A^nv=\lambda^nv$$ we have $0=\mu_A(A)v=\mu_A(\lambda)v$. Since $v$ is nonzero we must have $\mu_A(\lambda)=0$. 
\end{proof}
\end{lmm}

In general, to deduce the formula for the minimal polynomial, we follow three steps. \\~\\
Step 1: Find out the eigenvalues of the matrix. \\
Step 2: List out the possibilities of the minimal degree. This is done using the fact that $\mu_A(\lambda)=0$ and $\deg(\mu_A)\leq n$. \\
Step 3: Plug in the matrix to find out which polynomial has its root at $A$. \\~\\
There is another method to find out the formula using the following lemma. 

\begin{lmm}{}{} Let $T:V\to V$ be a linear map. Let $$V=W_1\oplus\dots\oplus W_k$$ be the direct sum of invariant subspaces, meaning $W_1,\dots,W_k$ are invariant subspaces of $T$. Let $\mu_i(x)$ be the minimal polynomial of $T|_{W_i}$. Then $$\mu_T(x)=\lcm(\mu_1,\dots,\mu_k)$$
\end{lmm}

Using this, we derive a better algorithm to find the minimal polynomial: \\~\\
Step 1: Take $v\neq 0$ an eigenvector and set $W=\text{span}\{v,T(v),T^2(v),\dots\}$. Then $W$ is invariant under $T$. Let $d$ be the minimal positive integer such that $v,T(v),\dots,T^d(v)$ are linearly dependent. Then $v,T(v),\dots,T^{d-1}(v)$ are linearly independent. Then we know that $\mu_T(x)$ has degree larger than $d$ since else $\mu_T(x)v$ will never be $0$. Then there is a nontrivial linear dependency relation of the form $$T^d(v)+c_{d-1}T^{d-1}(v)+\dots+c_1T(v)+c_0v=0$$\\
Step 2: Consider the polynomial $$x^d+c_{d-1}x^{d-1}+\dots+c_1x+c_0$$ Then this is precisely the minimal polynomial. 

\pagebreak
\section{The Jordan Canonical Form}
In the last sections, we looked into what kinds of matrices can have "nice" looking matrix under some basis. We now provide a less "nice" looking form of a similar matrix. However, every matrix can be reduced to this relatively "nice" looking form, as long as the field is algebraically closed. This form is called the Jordan Normal Form. 

\subsection{Generalized Eigenspace}
\begin{defn}{Generalized Eigenvectors}{} Let $V$ be a vector space over a field $\F$. Let $T\in\text{End}_k(V)$. Let $\lambda$ be an eigenvalue of $T$. We say that $v\in V$ is a generalized eigenvector of $T$ if $$(T-\lambda\text{id}_V)^kv=0$$ for some $k\in\N\setminus\{0\}$. 
\end{defn}

\begin{defn}{Generalized Eigenspaces}{} Let $V$ be a vector space over a field $\F$. Let $T\in\text{End}_k(V)$. Let $\lambda$ be an eigenvalue of $T$. Define the generalized eigenspace of index $k\in\N\setminus\{0\}$ of $T$ with respect to $\lambda$ to be $$N_k(T,\lambda)=\{v\in V|(T-\lambda I)^kv=0\}=\ker((T-\lambda I)^k)$$
\end{defn}

\begin{prp}{}{} Let $V$ be a vector space over a field $\F$. Let $T\in\text{End}_k(V)$. Let $\lambda$ be an eigenvalue of $T$. Then the following are true. 
\begin{itemize}
\item $N_1(T,\lambda)\subseteq N_2(T,\lambda)\subseteq N_3(T,\lambda)\subseteq\dots$
\item There exists some $k\in\N\setminus\{0\}$ such that $N_k(T,\lambda)=N_{k+1}(T,\lambda)=\cdots$
\end{itemize} \tcbline
\begin{proof}
Let $v\in N_i(T,\lambda)$. This means that $(T-\lambda\text{id}_V)^iv=0$. Applying the linear map $T-\lambda\text{id}_V$ on both sides show that $v\in N_{i+1}(T,\lambda)$. 
\end{proof}
\end{prp}

\begin{defn}{Degree of Eigenvectors}{} Let $V$ be a vector space over a field $\F$. Let $T\in\text{End}_k(V)$. Let $\lambda$ be an eigenvalue of $T$. Define the degree of $\lambda$ to be $$d(\lambda)=\min\{k\;|\;N_k(T,\lambda)=N_{k+1}(T,\lambda)=\cdots\}$$
\end{defn}

\begin{prp}{}{} Let $V$ be a vector space over a field $\F$. Let $T\in\text{End}_k(V)$. Let $\lambda_1,\dots,\lambda_m$ be eigenvalues of $T$. Let $v_i\in N_{d(\lambda_i)}(T,\lambda_i)$ for $1\leq i\leq m$. Then $v_1,\dots,v_m$ are linearly independent. 
\end{prp}

Using generalized eigenspaces, we obtain an improvement of thm3.2.5. 

\begin{thm}{Primary Decomposition Theorem}{} Let $V$ be a vector space of an algebraically closed field $F$. Let $T:V\to V$. Let $\lambda_1,\dots,\lambda_m$ be the full list of distinct eigenvalues of $T$. Then the following are true. 
\begin{itemize}
\item $V$ admits a decomposition into generalized eigenspaces $$V=\bigoplus_{i=1}^mN_{d(\lambda_i)}(T,\lambda_i)$$
\item The generalized eigenspace $N_{d(\lambda_i)}(T,\lambda_i)$ is an invariant subspace of $T$. 
\end{itemize}
\end{thm}

This version of the decomposition is applicable to any in vector space and linear map. In particular, if $d(\lambda_i)=1$ for all $i$, notice that generalized eigenspaces are simply eigenspaces so that the two decompositions coincide. 

\subsection{Jordan Canonical Form}
\begin{defn}{Jordan Chain}{} Let $V$ be a vector space over a field $\F$. Let $T\in\text{End}_k(V)$. Let $\lambda$ be an eigenvalue of $T$. A Jordan Chain of length $k\in\N\setminus\{0\}$ is a sequence of nonzero vectors $v_1,\dots,v_k\in V$ such that the following are true. 
\begin{itemize}
\item $T(v_1)=\lambda v_1$
\item For $2\leq i\leq k$, we have $T(v_i)=\lambda v_i+v_{i-1}$
\end{itemize}
\end{defn}

\begin{lmm}{}{} Let $V$ be a vector space over a field $\F$. Let $T\in\text{End}_k(V)$. Let $\lambda$ be an eigenvalue of $T$. Let $v_1,\dots,v_k\in V$ be a Jordan chain of length $k$ with respect to $T$ and $\lambda$. Then the following are true. 
\begin{itemize}
\item For $1\leq i\leq k$, we have $v_i\in N_i(A,\lambda)$
\item For $2\leq i\leq k$, $(T-\lambda\text{id}_V)(v_i)=v_{i-1}$
\item The vectors $v_1,\dots,v_k$ are linearly independent. 
\end{itemize} \tcbline
\begin{proof}
\end{proof}
\end{lmm}

\begin{prp}{}{} Let $V$ be a vector space over a field $\F$. Let $T\in\text{End}_k(V)$. Let $\lambda$ be an eigenvalue of $T$. Let $v_1,\dots,v_k\in V$ be a Jordan chain of length $k$ with respect to $T$ and $\lambda$. Then the subspace $\F\langle v_1,\dots,v_k\rangle$ is invariant under $T$. \tcbline
\begin{proof}
Note that we just have to find out where $v_1,\dots,v_k$ are mapped to since they are a basis of our subspace. But $T(v_i)=\lambda v_i+v_{i-1}$ for $i\in\{2,\dots,k\}$ which is a linear combination of our basis, we must have that the subspace is invariant. 
\end{proof}
\end{prp}

\begin{defn}{Jordan Block of Degree $k$}{} Let $\F$ be a field. Let $\lambda\in\F$. Define the Jordan block of degree $k$ of $\lambda$ to be the $k\times k$ matrix $$
\gamma_{ij}=\begin{cases}
\lambda & \text{if $j=i$}\\
1_\F & \text{if $j=i+1$}\\
0 & \text{otherwise}
\end{cases}$$
The matrix is denoted by $J_{\lambda,k}$. 
\end{defn}

\begin{defn}{Jordan Basis}{} Let $V$ be a vector space over a field $\F$. Let $T\in\text{End}_\F(V)$. A Jordan basis of $T$ is sequence of Jordan chains $v_1,\dots,v_{k_1},w_1,\dots,w_{k_2},\dots$ for $T$ such that it is a basis for $V$. 
\end{defn}

\begin{prp}{}{} Let $V$ be a vector space over a field $\F$. Let $T\in\text{End}_\F(V)$. Let $v_1,\dots,v_n$ be a Jordan basis for $T$. Then the matrix representing $T$ under the Jordan basis is given by a direct sum of Jordan blocks. 
\end{prp}

\begin{thm}{}{} Let $\F$ be an algebraically closed field. Let $n\in\N\setminus\{0\}$. Let $A\in M_n(\F)$ be a matrix. Then the following are true. 
\begin{itemize}
\item Consider $A$ as a linear transformation $\F^n\to\F^n$. Then a Jordan basis for $A$ exists. 
\item $A$ is similar to a direct sum of Jordan blocks. 
\end{itemize} \tcbline
\begin{proof} We will construct the basis with $3$ methods. They each contribute to part of the Jordan Basis. Firstly, we will obtain a basis of a subspace. We induct on $n$. The case of $n=1$ is trivial. \\~\\
Now suppose $T:V\to V$ is a linear map with $\dim(V)=n$. We want to find a restriction of $T$ that is an automorphism to apply the induction hypothesis. Fix $\lambda$ to be one of the eigenvalues of $T$. This will be used throughout the entire proof. This $\lambda$ is possible because the ground field is algebraically closed. Now I claim that $U=\im(T-\lambda I)$ is invariant under $T$. If this is true, then $T|_U:U\to U$ can be used to apply induction hypothesis. So all we have to show is that $U$ is $T$-invariant and that $\dim(U)<\dim(V)$. The second item must be true by the rank nullity theorem. There must be an eigenvector in $\ker(T-\lambda I)$. Thus $\ker(T-\lambda I)\geq 1$ which implies that $\dim(U)<n$ by the rank nullity theorem. Now we prove that $U$ is invariant. Let $u\in U$, I show that $T(u)\in U$. If $u\in U$, then $u=(T-\lambda I)(v)$ for some $v\in V$, hence $$T(u)=T(T-\lambda I)(v)=(T-\lambda I)(T(v))\in\im(T-\lambda I)=U$$ Thus we have proven that $T|_U:U\to U$. Apply induction hypothesis here to obtain a Jordan Basis for $T_U$. Call that Jordan Basis $e_1,\dots,e_m$. \\~\\
We now construct our second set of vectors. Recall that a Jordan Basis is a string of $l$ Jordan Chains. Let $v_1,\dots,v_k$ denote one of the Jordan Chains. We can extend this Jordan Chain one more by setting $(T-\lambda I)^{k+1}(v_{k+1})=0$. Do the same thing for everey Jordan Chain, and relabel them to $w_1,\dots,w_l$. As a side note, the two set of vectors we have now still form a Jordan Basis because we simply extended every Jordan Chain one more. \\~\\
For the final set of vectors, observe that the first vector of each of the $l$ Jordan Chains are eigenvectors of $T|_U$ with its eigenvalue being $\lambda$. This is because by definition of Jordan Chains, $T|_U(v_1)=\lambda v_1$. Also note that those $l$ vectors are linearly independent. Thus the first vectors of each of the $l$ Jordan Chains span an $l$ dimensional subspace of the eigenspace of $\lambda$. Recall that the eigenspace of $\lambda$ has dimension $\dim(V)-\dim(U)=\dim(\ker(T-\lambda I))$. To minimize notation let $m=\dim(U)$. Thus by extension theorem we can extend the basis of the $l$ dimensional subspace to $\ker(T-\lambda I)$. Call the extension vectors $w_{l+1},\dots,w_{n-m}$. As a side note, these $n-m-l$ vectors each Jordan Chains of length $1$. Thus we have complete our last set of vectors. \\~\\
We have $n$ vectors $$e_1,\dots,e_m,w_1,\dots,w_l,w_{l+1},\dots,w_{n-m}$$ Thus we only need to prove that they are linearly independent. Let $x=\sum_{k=1}^m\beta_me_m$. Let $$\sum_{i=1}^{n-m}\alpha_iw_i+x=0$$ Applying $T-\lambda I$ on both sides give $$\sum_{i=1}^{l}\alpha_i(T-\lambda I)w_i+(T-\lambda I)(x)=0$$ Since $w_{l+1},\dots,w_{n-m}$ is in $\ker(T-\lambda I)$, they become $0$. Now recall that our construction of $w_1,\dots,w_l$ is made by extending our Jordan Chains. So applying $(T-\lambda I)$ moves down our Jordan Chain. This means that $(T-\lambda I)x$ no longer contains the last term of each Jordan Chain and are linear combinations of $\{e_1,\dots,e_m\}\setminus\{\text{Last Term of each Jordan Chain}\}$, while all of the $(T-\lambda I)(w_1),\dots,(T-\lambda I)(w_l)$ are all last members of ecah Jordan Chain. From the fact that $e_1,\dots,e_m$ are a basis, we have $\alpha_1=\dots=\alpha_l=0$. \\~\\
Our sum now becomes $(T-\lambda I)x=0$. Which means that $x\in\ker(T-\lambda I)$. Now our original sum becomes $$\sum_{i=l+1}^{n-m}\alpha_iw_i+x=0$$ By construction, $w_{l+1},\dots,w_{n-m}$ extends a basis of the eigenspace of $T|_U$ for $\lambda$, thus $\alpha_{l+1}=\dots=\alpha_{n-m}=0$. Also since $e_1,\dots,e_m$ is a basis of $U$, we have $\beta_1=\dots=\beta_m=0$. 
Finally by the above corollary, in a Jordan Basis, the matrix of $T$ is a direct sum of Jordan Blocks. 
\end{proof}
\end{thm}

\begin{defn}{The Jordan Canonical Form}{} Let $\F$ be an algebraically closed field. Let $n\in\N\setminus\{0\}$. Let $A\in M_n(\F)$ be a matrix. Define the Jordan canonical form of $A$ to be any choice of direct sum of Jordan blocks given by the above theorem. 
\end{defn}

\begin{lmm}{}{} Let $\F$ be an algebraically closed field. Let $n\in\N\setminus\{0\}$. Let $A\in M_n(\F)$ be a matrix. Then $A$ and $B$ have the same Jordan canonical form up to a reordering of the direct sum in the JCF. 
\end{lmm}

\begin{prp}{}{} Let $\F$ be an algebraically closed field. Let $n\in\N\setminus\{0\}$. Let $A\in M_n(\F)$ be a matrix. Then the following are true. 
\begin{itemize}
\item The number of Jordan Blocks of $J$ with eigenvalue $\lambda$ is equal to $\dim(\ker(A-\lambda I))$
\item The number of Jordan Blocks of $J$ with eigenvalue $\lambda$ of degree at least $i$ is equal to $\dim(N_i(A,\lambda))-\dim(N_{i-1}(A,\lambda))$
\end{itemize}\tcbline\begin{proof} Since similar matrices have the same dimensions for their generalized eigenspaces corresponding to their eigenvalue, WLOG take $A=J=J_{\lambda_1,k_1}\oplus\dots\oplus J_{\lambda_s,k_s}$. However, note that the dimension of $N_i(A\oplus B,\lambda)$ is equal to $\dim(N_i(A,\lambda))+\dim(N_i(B,\lambda))$. So we just have to  prove the theorem for a single Jordan Block. \\~\\
Since $(J_{\lambda,k}-\lambda I)^i$ has a single diagonal line of ones $i$ places above the diagonal for $i<k$, and is $0$ for $i\geq k$, the dimension of its kernel is $i$ for $0\leq i\leq k$ and k for $i\geq k$. 
\end{proof}
\end{prp}

\begin{prp}{}{} Let $\F$ be an algebraically closed field. Let $n\in\N\setminus\{0\}$. Let $A\in M_n(\F)$ be a matrix. Then the Jordan canonical form of $A$ is unique up to reordering of the direct sum of the Jordan blocks. 
\end{prp}

\begin{prp}{}{} Let $V$ be a vector space over an algebraically closed field $\F$. Let $T\in\text{End}_k(V)$. Let $\lambda_1,\dots,\lambda_m$ be the full list of distinct eigenvalues of $T$. Then then following are true. 
\begin{itemize}
\item The characteristic polynomial of $T$ is given by $$c_T(x)=(-1)^n\prod_{k=1}^m(x-\lambda_k)^{a_k}$$ where $a_k$ is the sum of the degrees of the Jordan Blocks of $T$ of eigenvalue $\lambda_k$
\item The minimal polynomial of $T$ is $$\mu_A(x)=\prod_{k=1}^m(x-\lambda_k)^{d(\lambda_k)}$$
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $V$ be a vector space over an algebraically closed field $\F$. Let $T\in\text{End}_k(V)$. Then the following are equivalent. 
\begin{itemize}
\item $T$ is diagonalizable. 
\item $\mu_T(x)$ has no repeated factors. 
\item $d(\lambda)=1$ for all eigenvalues $\lambda$ of $T$. 
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $A,B\in M_{n\times n}(\C)$. Then $A$ and $B$ are similar if and only if the following are true. 
\begin{itemize}
\item $A$ and $B$ have the same set of eigenvalues
\item $\dim(\ker(A-\lambda I)^i)=\dim(\ker(B-\lambda I)^i)$ for all $i$ and eigenvalues $\lambda$ 
\end{itemize}
\end{prp}

To finish this section, we show the process of determining the Jordan Canonical form of a matrix. The steps are usually as follows: \\~\\
Step 1: Find out the nullity of $A-\lambda I$ as this gives us the number of Jordan Blocks with eigenvalue $\lambda$. \\
Step 2: To find out the number of Jordan blocks with eigenvalue $\lambda$ and size at least $i$, we calculate $\dim(\ker(A-\lambda I_n)^i)-\dim(\ker(A-\lambda I)^{i-1})$. \\~\\
To find out the change of basis matrix, meaning the Jordan Chains, we do the following step: \\~\\
Step 3: Find out the last one in the chain $v_k$ by solving $(A-\lambda I)^kv_k=0$ while restricting $v_k$ such that $(A-\lambda I)^{k-1}v_k\neq 0$, and then proceed to find out $v_{k-1}$ by $(A-\lambda I)^{k-1}v_k=v_{k-1}$ and vice versa. \\~\\

There are also extra information that we can use to determine the JCF: \\
The degree of $(x-\lambda)$ in $\mu_A$ indicates the maximum size of Jordan Blocks with eigenvalue $\lambda$. \\
The degree of $(x-\lambda)$ in $c_A$ indicates the total size of used in the JCF of all Jordan Blocks with eigenvalue $\lambda$. \\~\\

We give an example of finding the JCF of a matrix. 

\begin{eg}{}{} Find the Jordan Canonical Form of $$A=\begin{pmatrix}
1 & 0 & 1\\
0 & 1 & 0\\
0 & 1 & 2
\end{pmatrix}$$ \tcbline
\begin{proof}
We begin by finding out the eigenvalues of $A$. We have that $c_A(x)=\det(A-xI)=(1-x)^2(2-x)$. This means that the eigenvalues are $1$ and $2$. Now we begin with step 1. \\~\\
For eigenvalue $1$, we have that row reduced form of $A-I$ is $$\begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 1\\
0 & 0 & 0
\end{pmatrix}$$
Thus the nullity of $A-I$ is $1$. This means that the number of Jordan blocks with eigenvalue $1$ is $1$. Using information from $c_A$, we know that the total size used for the eigenvalue $1$ is $2$. This means that there is exactly one Jordan block of size $2$ in the JCF of $A$. \\~\\
This leaves the fact that the remaining Jordan block of size $1$ being the eigenvalue $2$. \\~\\
With this, we complete the JCF of $A$ with $$J=\begin{pmatrix}
1 & 1 & 0\\
0 & 1 & 0\\
0 & 0 & 2
\end{pmatrix}$$

To compute the basis, or the change of basis matrix $P$, we use step $3$. Since $A$ has one Jordan block for eigenvalue $1$, we need to find one string of Jordan chain. This chain needs to have length $2$ since the size of the Jordan block is $2$. (If there are multiple of Jordan blocks of the same eigenvalues, the end vector of the Jordan chains needs to be linearly indendent). We begin by finding the ending of the chain, $v_2$ by using the fact that $(A-I)^2v_2=0$ and $(A-I)v_2=v_1\neq 0$. We have that $$(A-I)^2=\begin{pmatrix}
0 & 1 & 1\\
0 & 0 & 0\\
0 & 1 & 1
\end{pmatrix}$$
We choose that $v_2=\begin{pmatrix}1\\ 1\\ -1\end{pmatrix}$. Now we have $$v_1=(A-I)v_2=\begin{pmatrix}-1\\ 0\\ 0 \end{pmatrix}$$ Finally, we choose $v_3\in\ker(A-2I)$. But row reducing $A-2I$ gives $$\begin{pmatrix}
1 & 0 & -1\\
0 & 1 & 0\\
0 & 0 & 0
\end{pmatrix}$$
We can choose $v_3=\begin{pmatrix}1\\ 0\\ 1\end{pmatrix}$. This means that $$P=\begin{pmatrix}
-1 & 1 & 1\\
0 & 1 & 0\\
0 & -1 & 1
\end{pmatrix}$$
\end{proof}
\end{eg}

\subsection{The Jordan-Chevalley Decompositions}
\begin{defn}{Jordan-Chevalley Decompositions}{} Let $k$ be a field. Let $V$ be a finite dimensional vector space over $k$. Let $T\in\text{End}(V)$. A Jordan-Chevalley decomposition of $V$ consists of $D,S\in\text{End}(V)$ such that the following are true. 
\begin{itemize}
\item $T=D+S$
\item $D$ is diagonalizable
\item $S$ is nilpotent
\item $SD=DS$
\end{itemize}
\end{defn}

We note here that if we consider vector spaces as a $k$-module, then saying $V$ semisimple is the same as saying $V$ is diagonalizable. 

\begin{prp}{}{} Let $k$ be a field. Let $V$ be a finite dimensional vector space over $k$. Let $T\in\text{End}(V)$. Then $T$ admits a unique Jordan-Chevalley decomposition. \tcbline
\begin{proof}
Let $T=PJP^{-1}$ be the Jordan canonical form of $x$. Then $J$ has non-zero entries exactly at the $(i,i)$th and $(i,i+1)$ positions. Therefore we can decompose $J$ as a sum of a diagonal matrix $D_J$ and a matrix $D_J$ with non-zero entry only on $(i,i+1)$th positions. But $S_J$ must then be nilpotent. Since $J$ is given in block form, we can also consider $D_J$ and $S_J$ as blocks. But diagonal matrices commute with all matrices. Hence $D_JS_J=S_JD_J$. Now let $D=PD_JP^{-1}$ and $S=PS_JP^{-1}$. I claim that $D,S$ consist of ta Jordan-Chevalley decomposition. We have that $T=PJP^{-1}=P(D_J+S_J)P^{-1}=PD_JP^{-1}+PS_JP^{-1}=D+S$. Also, $D$ is diagonalizable since $D=PD_JP^{-1}$ and $D_J$ is diagonalizable. Now we know that $S_J^n=0$ for some $n\in\N$. Then $S^n=PS_J^nP^{-1}=0$ hence $S$ is also nilpotent. Finally, we have that $$DS=PD_JP^{-1}PS_JP^{-1}=PD_JS_JP^{-1}=PS_JD_JP^{-1}=SD$$ hence we proved the existence of such a decomposition. 
\end{proof}
\end{prp}

\begin{prp}{}{} Let $k$ be an algebraically closed field. Let $V$ be a finite dimensional vector space over $k$. Let $T\in\text{End}(V)$. Let $D,S\in\text{End}(V)$ be the Jordan-Chevalley decomposition of $T$. Then there exists $p,q\in k[x]$ such that $p(T)=D$ and $q(T)=S$. 
\end{prp}

\pagebreak
\section{The Dual Space}
\subsection{Dual of a Vector Space}
Recall that for $V$ and $W$ two vector spaces over a field $\F$, the set of all linear maps $\Hom(V,W)$ can be equipped with a vector space structure. We consider the case where $W=\F$. 

\begin{defn}{Linear Forms}{} Let $V$ be a vector space over a field $\F$. A linear form on $V$ is a linear map from $V$ to $\F$. 
\end{defn}

\begin{defn}{Dual Space}{} Let $V$ be a vector space over a field $\F$. Define the dual space $V^\ast$ of $V$ to be the vector space $$V^\ast=\Hom(V,\F)$$ of all linear forms on $V$
\end{defn}

Explicitly, the vector space operations are defined as follows. 
\begin{itemize}
\item For two linear forms $f,g:V\to\F$, define $f+g:V\to\F$ by $(f+g)(v)=f(v)+g(v)$ for all $v\in V$
\item For a linear form $f:V\to\F$ and a scalar $\lambda\in\F$, define $\lambda f:V\to\F$ by $(\lambda f)(v)=\lambda\cdot f(v)$ for all $v\in V$
\end{itemize}

\begin{lmm}{}{} Let $V$ be a finite dimensional vector space. Then $V^\ast$ is also finite dimensional and $\dim(V^\ast)=\dim(V)$. 
\end{lmm}

\begin{defn}{Dual Basis}{} Let $V$ be a vector space. Let $v_1,\dots,v_n$ be a basis of $V$, then the dual basis of $v_1,\dots,v_n$ is the list $\phi_1\dots,\phi_n$ of elements of $V^\ast$, where $\phi_k$ is a linear functional such that $$\phi_k(v_i)=
\begin{cases}
1 & \text{if }k=i\\
0 & \text{if }k\neq i
\end{cases}$$
\end{defn}

\begin{prp}{}{} Let $V$ be a vector space. Then the dual basis of a basis of $V$ is a basis of $V^\ast$
\end{prp}

\begin{defn}{Dual Map}{} Let $V,W$ be vector spaces over a field. Let $T\in\Hom(V,W)$. The dual map of $T$ is the linear map $T^\ast\in\Hom(W^\ast,V^\ast)$ defined by $T^\ast(\phi)=\phi\circ T$ for $\phi\in W^\ast$. 
\end{defn}

\begin{prp}{}{} Let $V,W$ be vector spaces over a field $\F$. Let $T\in\Hom(V,W)$ and $\lambda\in\F$. Then the following are true. 
\begin{itemize}
\item $(S+T)^\ast=S^\ast+T^\ast$
\item $(\lambda T)^\ast=\lambda T^\ast$
\item $(ST)^\ast=T^\ast S^\ast$. 
\end{itemize}
\end{prp}

\begin{thm}{}{} Let $V,W$ be vector spaces over a field $\F$. Let $T:V\to W$ be a linear transformation and let $A$ be a matrix representing $T$ under some choice of bases. Then $A^T$ represents the map $T^\ast$ under the same bases. 
\end{thm}

\subsection{The Double Dual}
\begin{defn}{The Double Dual}{} Let $V$ be a vector space over a field $\F$. Define the double dual of $V$ to be the vector space $$V^{\ast\ast}=\Hom(\Hom(V,\F),\F)$$
\end{defn}

\begin{thm}{}{} Let $V$ be a vector space over a field $k$. Define the function $\text{ev}_V:V\to V^{\ast\ast}$ by $$\text{ev}(v)=\left(\substack{E_v:V^\ast\to\R\\f\mapsto f(v)}\right)$$ Then $\text{ev}$ is an injective linear transformation. Moreover $\text{ev}_V$ is natural in the following sense. If $W$ is another vector space over $k$ and $\phi:V\to W$ is a linear map, then the following diagram commutes: \\~\\
\adjustbox{scale=1,center}{\begin{tikzcd}
	V & W \\
	{V^{\ast\ast}} & {W^{\ast\ast}}
	\arrow["\phi", from=1-1, to=1-2]
	\arrow["{\text{ev}_V}"', from=1-1, to=2-1]
	\arrow["{\text{ev}_W}", from=1-2, to=2-2]
	\arrow["{\phi^{\ast\ast}}"', from=2-1, to=2-2]
\end{tikzcd}}\\~\\
\end{thm}

\begin{lmm}{}{} Let $V$ be a vector space over a field. Then the map $$\text{ev}:V\to V^{\ast\ast}$$ is an isomorphism. 
\end{lmm}

\subsection{Perfect Pairings}

\pagebreak
\section{Linearity in Two Variables}
\subsection{Quadratic Forms}
\begin{defn}{Quadratic Forms}{} A quadratic form in $n$ variables $x_1,\dots,x_n$ over a field $K$ is a polynomial $$q(x_1,\dots,x_n)=\sum_{i=1}^n\sum_{j=1}^na_{ij}x_ix_j$$
\end{defn}

\begin{prp}{}{} Every quadratic form $q(x_1,\dots,x_n)=\sum_{i=1}^n\sum_{j=1}^na_{ij}x_ix_j$ can be represented by a matrix multiplication, namely $$q(x_1,\dots,x_n)=\begin{pmatrix}x_1 & \cdots & x_n\end{pmatrix}\begin{pmatrix}a_{11} & \frac{1}{2}a_{12} & \cdots & \frac{1}{2}a_{1n}\\
\frac{1}{2}a_{21} & a_{22} & \cdots & \frac{1}{2}a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
\frac{1}{2}a_{n1} & \frac{1}{2}a_{n2} & \dots & a_{nn}
\end{pmatrix}\begin{pmatrix}x_1 \\ \vdots \\ x_n\end{pmatrix}$$ In particular, this matrix is symmetric with $a_{ij}=a_{ji}$ for $i,j\in\{1,\dots,n\}$. \tcbline
\begin{proof}
Multiplying out the entries of the matrix multiplication gives the original quadratic form. 
\end{proof}
\end{prp}

\begin{prp}{}{} A change of basis via the change of basis matrix $P$ also changes the symmetric matrix of the quadratic form by $P^TAP$
\end{prp}

\begin{defn}{Congruent Matrices}{} Let $A,B\in M_{n\times n}(\F)$ be two square matrices. We say that $A$ and $B$ are congruent if there exists some invertible matrix $P\in GL(n,\F)$ such that $$B=P^TAP$$
\end{defn}

\begin{prp}{}{} Two symmetric matrices are congruent if and only if they represent the same quadratic form with respect to different bases. 
\end{prp}

Using this we can establish a one-to-one correspondence $$\left\{q:\F^n\to\F\;|\;q\text{ is a quadratic form}\right\}\;\;\overset{1:1}{\longleftrightarrow}\;\;\frac{\left\{M\in M_{n\times n}(\F)\;|\;M\text{ is symmetric}\right\}}{\cong}$$ where $\cong$ denotes the congruence relation. Moreover, within each congruence class of a quadratic form $q$ contains all possible symmetric matrices that represent $q$ in all possible bases. 

\begin{thm}{}{} Let $q(x_1,\dots,x_n)$ be a quadratic form in $n$ variables over a field $K$ whose characteristic is not $2$. Then there exists a basis such that $q(y_1,\dots,y_n)=c_1y_1^2+\dots+c_ny_n^2$ for some $c_1,\dots,c_n\in K$. \tcbline
\begin{proof}
There is a shorter proof for this theorem, but for the sake of the construction of $c_1,\dots,c_n$, we will prove the theorem constructively. Suppose that $q$ is represented by the symmetric matrix $A=(a_{ij})_{n\times n}$ with respect to the basis $b_1,\dots,b_n$. There are three steps in the construction. I use $b_1,\dots,b_n$ to indicate the old basis and $b_1',\dots,b_n'$ to indicate the basis after the step. \\~\\
Step 1: Arrange such that $q(b_1)\neq 0$. There are four cases here. 
\begin{itemize}
\item If $a_{11}\neq 0$, then we are done. 
\item If $a_{11}=0$ but $a_{kk}\neq 0$ for some $1<k\leq n$. Then just set $b_1'=b_k$ and $b_k'=b_1$. At the same time, the matrix for the quadratic form is changed by swapping rows $r_1$ and $r_k$, and then swapping the columns $c_1$ and $c_k$
\item If $a_{kk}=0$ for all $k\in\{1,\dots,n\}$, but there are some $i,j$ such that $a_{ij}\neq 0$, then set $b_i'=b_i+b_j$ since $q(b_i+b_j)=2a_{ij}\neq 0$ and so we reduced this case to the previous two cases.The matrix then becomes $$\begin{pmatrix}
2a_{1k} & a_{12}+a_{k2} & \dots & a_{1k} & \cdots & a_{1n}+a_{kn} \\
 a_{12}+a_{k2} & 0 & \cdots & a_{2k} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
a_{1k} & a_{k2} & \cdots & 0 & \cdots & a_{kn}\\
\vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
a_{1n}+a_{kn} & a_{n2} & \cdots & a_{nk} & \cdots & 0
\end{pmatrix}$$
\item If $a_{ij}=0$ for all $i,j\in\{1,\dots,n\}$ then it is the zero function. 
\end{itemize}
In this step the change of basis matrix is just the elementary matrices. \\~\\

Step 2: Now we modify $b_2,\dots,b_n$ to make them orthogonal to $b_1$. Now set $b_k'=b_k-\frac{a_{1k}}{a_{11}}b_1$. This way, the matrix entry $a_{1k}$ becomes zero. Now the change of basis matrix becomes $$P=\begin{pmatrix}
1 & -\frac{a_{12}}{a_{11}} & \cdots & -\frac{a_{1n}}{a_{11}}\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 0
\end{pmatrix}$$
After this step all the change of basis matrix should be compiled and calculated so that the new matrix for the quadratic form can be formed. \\~\\
Step 3: Since the matrix for the quadratic form is now $$\begin{pmatrix}
a_{11} & 0 & \cdots & 0\\
0 & ? & \cdots & ?\\
\vdots & \vdots & \ddots & \vdots\\
0 & ? & \cdots & ?
\end{pmatrix}$$
We can induct on $n$ by repeating the process of step 1 with the entry $a_{22}$ until we reach $a_{nn}$. 
\end{proof}
\end{thm}

This main theorem of quadratic forms shows that every quadratic form is congruent to a diagonal matrix. It also proves that within every equivalence class of congruent matrices, there must be at least one diagonal matrix. To illustrate the process of reduction, we look an example. 

\begin{eg}{}{} Find a nice basis for the quadratic form $q\left(\begin{pmatrix}x\\y\\z\end{pmatrix}\right)=xy+3yz-5xz$. \tcbline
\begin{proof}
Using the formula, we construct the matrix of $q$ as $$A=\begin{pmatrix}
0 & \frac{1}{2} & -\frac{5}{2}\\
\frac{1}{2} & 0 & \frac{3}{2}\\
-\frac{5}{2} & \frac{3}{2} & 0
\end{pmatrix}$$ ~\\
We start the first change of basis. Notice that $a_{11}=a_{22}=a_{33}=0$ in the matrix while $a_{12}$ is not. Denote the standard basis by $b_1,b_2,b_3$. We perform the first basis change by $\{b_1'=b_1+b_2,b_2'=b_2,b_3'=b_3\}$. This means that the change of basis matrix from new to old is $$P'=\begin{pmatrix}
1 & 0 & 0\\
1 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}$$
To change $A$ into the new matrix $A'$, we simply replace $r_1$ by $r_1+r_2$ and replace $c_1$ by $c_1+c_2$. This gives $$A'=\begin{pmatrix}
1 & \frac{1}{2} & -1\\
\frac{1}{2} & 0 & \frac{3}{2}\\
-1 & \frac{3}{2} & 0
\end{pmatrix}$$
We keep track of changing the basis for clarity. We now have $A'=(P')^TAP'$. \\~\\
The next step is to set the new basis to $b_2''=b_2'-\frac{1}{2}b_1'$ and $b_3''=b_3'+b_1'$. This means that the new basis is now $\{b_1+b_2,\frac{1}{2}(b_2-b_1),b_1+b_2+b_3\}$. The change of basis from this basis back to the old one is now $$P''=\begin{pmatrix}
1 & -\frac{1}{2} & 1\\
1 & \frac{1}{2} & 1\\
0 & 0 & 1
\end{pmatrix}$$ Now the new matrix $A''$ is formed by replacing $r_2$ with $r_2$ by $r_2-\frac{1}{2}r_1$ and $r_3$ with $r_3+r_1$. Noticing that $A''$ must be symmetric, we need to take the new elements and replace the remanining lower traingualr elements so that $A''$ maintains symmetric. Also observe that the replacement of rows is exactly one changes into the new basis from the previous basis, where $b_2''=b_2'-\frac{1}{2}b_1'$ etc. Now we have $$A''=\begin{pmatrix}
1 & 0 & 0\\
0 & -\frac{1}{4} & 2\\
0 & 2 & -1
\end{pmatrix}$$
We now have $A''=(P'')^TAP''$. \\~\\
Now we perform the next change of basis. We set $b_3'''=b_3''-\frac{2}{-\frac{1}{4}}b_2''=b_3''+8b_2''$. Now the new basis is $\{b_1+b_2,\frac{1}{2}(b_2-b_1),-3b_1+5b_2+b_3\}$. The change of basis matrix is now $$P'''=\begin{pmatrix}
1 & -\frac{1}{2} & -3\\
1 & \frac{1}{2} & 5\\
0 & 0 & 1
\end{pmatrix}$$ Similar to the above, we replace $r_3$ with the same transformation and adjust $A'''$ so that it remains symmetric. Thus now we have $$A'''=\begin{pmatrix}
1 & 0 & 0\\
0 & -\frac{1}{4} & 0\\
0 & 0 & 15
\end{pmatrix}$$
This means that we are done with $A'''=(P''')^TAP'''$. 
\end{proof}
\end{eg}

In general, this result of diagonalization is different from that of similar matrices. One should not be confusing reduction of congruent matrices into diagonal matrices and reduction of similar matrices into JCF as well as reduction of diagonalizable matrices into diagonal matrices. 

\subsection{Bilinear Forms}
\begin{defn}{Bilinear Maps}{} Let $V,W$ be vector spaces over a field $\F$. A bilinear map on $V$ and $W$ is a map $\tau:V\times W\to\F$ such that 
\begin{itemize}
\item $\tau(a_1+a_2v_2,w)=a_1\tau(v_1,w)+a_2\tau(v_2,w)$
\item $\tau(v,b_1w_1+b_2w_2)=b_1\tau(v,w_1)+b_2\tau(v,w_2)$
\end{itemize}
\end{defn}

\begin{defn}{Matrix Representing Bilinear Forms}{} Let $\F$ be a field. Let $V,W$ be finite dimensional vector spaces over $\F$. Let $E$ be a basis of $V$ and let $F$ be a basis of $W$. Let $\tau:V\times W\to\F$ be a bilinear map. Define the matrix representing $\tau$ under the basis $E$ and $F$ to be $$A=\begin{pmatrix}
\tau(e_1,f_1) & \cdots & \tau(e_1,f_m)\\
\vdots & \ddots & \vdots\\
\tau(e_n,f_1) & \cdots & \tau(e_n,f_m)
\end{pmatrix}$$
\end{defn}

\begin{prp}{}{} Let $\F$ be a field. Let $V,W$ be finite dimensional vector spaces over $\F$. Let $E$ be a basis of $V$ and let $F$ be a basis of $W$. Let $\tau:V\times W\to\F$ be a bilinear map. Let $A$ be the matrix representing $\tau$ under the given bases. Then for any $v\in V$ and $w\in W$, we have $$\tau(v,w)=v^TAw$$ \tcbline
\begin{proof}
Simple to see by expanding the matrix multiplication. 
\end{proof}
\end{prp}

\begin{prp}{}{} Let $V,W$ be vector spaces over $\F$. Let $\tau:V\times W\to\F$ be a bilinear map. Consider the following data. 
\begin{itemize}
\item Let $A\in M_{n\times m}(\F)$ represent $\tau$ with the bases $E$ of $V$ and $F$ of $W$ respectively. 
\item Let $B\in M_{n\times m}(\F)$ represent $\tau$ with the bases $E'$ of $W$ and $F'$ of $W$ respectively
\item Let $P\in GL(n,\F)$ be the change of basis matrix from $E$ to $E'$
\item Let $Q\in GL(m,\F)$ be the change of basis matrix from $F$ to $F'$
\end{itemize}
Then we have that $$B=P^TAQ$$
\end{prp}

\begin{defn}{Bilinear Forms}{} Let $V$ be a vector space over a field $\F$. A bilinear form on $V$ is a bilinear map $$\tau:V\times V\to\F$$
\end{defn}

\begin{defn}{Types of Bilinear Forms}{} Let $k$ be a field. Let $V$ be a vector space over $k$. Let $\tau:V\times V\to k$ be a bilinear form. 
\begin{itemize}
\item We say that $\tau$ is symmetric if $\tau(v,w)=\tau(w,v)$
\item We say that $\tau$ is skew-symmetric if $\tau(v,w)=-\tau(w,v)$
\item We say that $\tau$ is non-degenerate if $\tau(v,w)=0$ for all $w\in V$ implies that $v=0$. 
\end{itemize}
\end{defn}

\begin{prp}{}{} Let $\F$ be a field. Let $V$ be a vector space over $\F$. Let $\tau:V\times V\to\F$ be a bilinear form. Then the following are true. 
\begin{itemize}
\item $\tau$ is symmetric if and only if the matrix representing $\tau$ over any basis is symmetric. 
\item $\tau$ is skew-symmetric if and only if the matrix representing $\tau$ over any basis is skew-symmetric. 
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $\F$ be a field. Let $V$ be a vector space over $\F$. Let $\tau:V\times V\to\F$ be a bilinear form. Then the following are true. Then the following are equivalent. 
\begin{itemize}
\item $\tau$ is non-degenerate. 
\item The matrix representing $\tau$ over any basis of $V$ is invertible. 
\item Every element of $V^\ast$ is of the form $\tau(v,-)$ for some $v\in V$. 
\item Every element of $V^\ast$ is of the form $\tau(v,-)$ for some unique $v\in V$. 
\end{itemize}
\end{prp}

\begin{defn}{Types of Bilinear Forms}{} Let $k$ be a field. Let $V$ be a vector space over $k$. Let $\tau:V\times V\to k$ be a bilinear form. 
\begin{itemize}
\item We say that $\tau$ is positive definite if $\tau(v,v)>0$ for all $v\in V\setminus\{0\}$. 
\item We say that $\tau$ is positive semi-definite if $\tau(v,v)\geq 0$ for all $v\in V$. 
\item We say that $\tau$ is negative definite if $\tau(v,v)<0$ for all $v\in V$. 
\item We say that $\tau$ is negative semi-definite if $\tau(v,v)\leq 0$ for all $v\in V$. 
\item We say that $\tau$ is indefinite otherwise. 
\end{itemize}
\end{defn}

\begin{prp}{}{} Let $q:V\to K$ be a function. Then the following are equivalent. 
\begin{itemize}
\item $q$ is a quadratic form
\item $q(cv)=c^2v$ for $c\in K$ and $v\in V$ and $\tau(v,w)=\frac{1}{2}(q(v+w)-q(v)-q(w))$ is a bilinear form on $V$
\item $q(v)=\tau(v,v)$ is a symmetric bilinear form on $V$
\end{itemize} \tcbline
\begin{proof}
Let $q:V\to K$ be a function. 
\begin{itemize}
\item $(1)\implies (2)$: Since every term in a quadratic form is quadratic, $q(cv)=c^2q(v)$ is natural. The fact that $\tau(v,w)$ is bilinear is also easy to check. 
\item $(2)\implies (3)$: From $(2)$ we know that $\tau(v,v)=q(v)$ by substituting $v$ in the position of $w$ and thus it clearly is a bilinear form. The position of $w$ and $v$ are also interchangable and thus it is symmetric. 
\item $(3)\implies (1)$: If $\tau(v,v)$ is a symmetric bilinear form then the matrix of $\tau$ is symmetric since $a_{ij}=\tau(e_i,f_j)=\tau(f_j,e_i)=a_{ji}$. Thus $q(v)=\tau(v,v)$ defines a quadratic form. 
\end{itemize}
\end{proof}
\end{prp}

\subsection{Sesquilinear Forms}
\begin{defn}{Sesquilinear Form}{} Let $V$ be a vector space over $\C$. A sesquilinear form on $V$ is a map $\tau:V\times V\to\C$ such that the following are true. 
\begin{itemize}
\item $\tau(\lambda v_1+\mu v_2,w)=\overline{\lambda}\tau(v_1,w)+\overline{\mu}\tau(v_2,w)$
\item $\tau(v,\lambda w_1+\mu w_2)=\lambda\tau(v,w_1)+\mu\tau(v,w_2)$
\end{itemize}
\end{defn}

\begin{defn}{Types of Sesquilinear Forms}{} Let $V$ be a vector space over $\C$. Let $\tau:V\times V\to\C$ be a sesquilinear form. 
\begin{itemize}
\item We say that $\tau$ is Hermitian if $\tau(v,w)=\overline{\tau(w,v)}$ for all $v,w\in V$. 
\end{itemize}
\end{defn}

\pagebreak
\section{Inner Product Spaces}
\subsection{The Norm of a Space}
Throughout this section, $\F$ means either $\R$ or $\C$. In general normed vector spaces only perform well in these two fields. 
\begin{defn}{Norm}{} Let $V$ be a vector space. A norm on $V$ is a function $\|\cdot\|:V\to\F$ such that
\begin{itemize}
\item $\|x\|\geq 0$ for all $x\in V$ with equality if and only if $x=0$
\item $\|\lambda x\|=\abs{\lambda}\|x\|$ for all $x\in V$ and $\lambda\in\F$
\item $\|x+y\|\leq\|x\|+\|y\|$ for all $x,y\in V$
\end{itemize}
\end{defn}

\begin{defn}{Normed Vector Space}{} A normed vector space is a pair $(V,\|\cdot\|)$ where $V$ is a vector space and $\|\cdot\|$ is a norm on $V$. 
\end{defn}

\begin{defn}{Convex Set}{} Let $V$ be a vector space. A subset $K$ of $V$ is convex if $x,y\in K$ implies $$\lambda x+(1-\lambda)y\in K$$ for $0\leq\lambda\leq 1$. 
\end{defn}

\begin{lmm}{}{} For every normed vector space, the unit ball $B_1(0)=\{v\in V|\|v\|\leq 1\}$ is convex. 
\end{lmm}

\begin{prp}{}{} Let $N:V\to\R^+$ be a function that satisfies the first two requirements of a norm. If $N$ also satisfies the fact that $\{x\in V|N(x)\leq 1\}$ is convex, then $N$ is a norm. 
\end{prp}

\subsection{Inner Products}
Inner products are only properly defined for vector spaces over $\R$ and $\C$. From this point onwards we will limit our discussions with $V=\R^n$ or $\C^n$ and $K=\R$ or $\C$. 

\begin{defn}{Inner Product over $\R$}{} Let $V$ be a vector space over $\R$. An inner product on $V$ is a bilinear form $\langle-,-\rangle:V\times V\to\R$ such that the following are true. 
\begin{itemize}
\item Positive definite: $\langle x,x\rangle\geq0$ for all $x\in V$ with equality if and only if $x=0$. 
\item Symmetric: $\langle x,y\rangle=\langle y,x\rangle$ for all $x,y\in V$
\end{itemize}
In this case $V$ is called an inner product space. 
\end{defn}

\begin{prp}{}{} Let $V$ be a finite dimensional vector space over $\R$. Let $\tau:V\times V\to\R$ be a bilinear form. Then the following are equivalent. 
\begin{itemize}
\item $\tau$ is an inner product on $V$. 
\item The matrix representing $\tau$ over any basis of $V$ is symmetric and positive definite. 
\item For any matrix $A$ representing $\tau$ over some basis of $V$, there exists $P\in GL_{\dim(V)}(\R)$ such that $A=P^TP$. 
\end{itemize}
\end{prp}

\begin{defn}{Inner Product over $\C$}{} Let $V$ be a vector space over $\C$. An inner product on $V$ is a sesquilinear form $\langle-,-\rangle:V\times V\to\C$ such that the following are true. 
\begin{itemize}
\item Positive definite: $\langle x,x\rangle\geq0$ for all $x\in V$ with equality if and only if $x=0$. 
\item Hermitian: $\langle x,y\rangle=\overline{\langle y,x\rangle}$ for all $x,y\in V$
\end{itemize}
In this case $V$ is called an inner product space. 
\end{defn}

\begin{lmm}{}{} Let $\F=\R$ or $\C$. Let $(V,\langle-,-\rangle)$ be an inner product space over $\F$. Then $\langle-,-\rangle$ is non-degenerate. 
\end{lmm}

\begin{prp}{}{} Let $\F=\R$ or $\C$. Let $(V,\langle-,-\rangle)$ be an inner product space over $\F$. Then the map $\|-\|:V\to\F$ defined by $$\|v\|=\langle v,v\rangle$$ is a norm for $V$. 
\end{prp}

\begin{prp}{Cauchy-Schwartz Inquality}{} Let $\F=\R$ or $\C$. Let $(V,\langle-,-\rangle)$ be an inner product space over $\F$. Let $\|-\|$ be the induced norm from the inner product. For all $x,y\in V$, we have $$\abs{\langle x,y\rangle}\leq\|x\|\cdot\|y\|$$ with equality if and only if $y=\lambda x$ for some $\lambda\in\F$. \tcbline
\begin{proof} Let $z=x-\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y$. We have $\|z\|\geq0$. 
\begin{align*}
\|z\|^2&=\left\langle x-\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y,x-\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y\right\rangle\\
&=\langle x,x\rangle-2\left\langle x,\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y\right\rangle+\left\langle \frac{\abs{\langle x,y\rangle}}{\|y\|^2}y,\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y\right\rangle\\
&=\langle x,x\rangle-2\frac{\abs{\langle x,y\rangle}^2}{\|y\|^2}+\frac{\abs{\langle x,y\rangle}^2}{\|y\|^4}\langle y,y\rangle\\
&=\langle x,x\rangle-2\frac{\abs{\langle x,y\rangle}^2}{\|y\|^2}+\frac{\abs{\langle x,y\rangle}^2}{\|y\|^2}\\
&=\|x\|^2-\frac{\abs{\langle x,y\rangle}^2}{\|y\|^2}\\
\end{align*}
Thus we have
\begin{align*}
\|x\|^2&\geq\frac{\abs{\langle x,y\rangle}^2}{\|y\|^2}\\
\|x\|&\geq\frac{\abs{\langle x,y\rangle}}{\|y\|}\tag{Since they are all positive}\\
\|x\|\cdot\|y\|&\geq\abs{\langle x,y\rangle}
\end{align*}
Note that we have equality if and only if $\|z\|=0$, meaning $x=\frac{\abs{\langle x,y\rangle}}{\|y\|^2}y$. We are done by taking $\lambda=\frac{\|y\|^2}{\abs{\langle x,y\rangle}}$. 
\end{proof}
\end{prp}
\subsection{The Adjoint of a Linear Map}
\begin{defn}{The Adjoint of a Linear Map}{} Let $V$ be an inner product space. Let $T\in\text{End}(V)$ be a linear map. A map $T^\ast\in\text{End}(V)$ is said to be the adjoint of $T$ if $$\langle T(v), w\rangle=\langle v, T^\ast(w)\rangle$$ for all $v,w\in V$. 
\end{defn}

\begin{prp}{}{} Let $V$ be an inner product space and $T:V\to V$ be a linear map. Then the adjoint of $T$ exists and is unique. \tcbline
\begin{proof}
Let $T$ be a linear map. Then the function $\tau(v,w)=\langle T(v), w\rangle$ is a bilinear form since the inner product is bilinear. But we know that bilinear forms can be represented by a matrix multiplication, namely $\tau(v,w)=v^TAw$ where $A$ is defined as in theorem 1.3.2. Then treating $Aw$ as the linear map $T^\ast(w)$ and since $v^Tw=v\cdot w$, we have that $\tau(v,w)=v\cdot T^\ast(w)$ thus proving existence. Uniqueness follows naturally by construction of the matrix $A$. 
\end{proof}
\end{prp}

\begin{prp}{}{} Let $V$ be an finite dimensional inner product space. Let $T\in\text{End}(V)$ be a linear map. Let $A$ be a matrix representing $T$ under a chosen basis of $V$. Then the transpose $A^T$ of $A$ represents the adjoint $T^\ast$ under the same basis. 
\end{prp}

\begin{defn}{Self-Adjoint}{} Let $V$ be an inner product space. Let $T\in\text{End}(V)$ be a linear map. We say that $T$ is self-adjoint if $T=T^\ast$. 
\end{defn}

\begin{prp}{}{} Let $V$ be an finite dimensional inner product space. Let $T\in\text{End}(V)$ be a linear map. Let $A$ be a matrix representing $T$. Then $T$ is self-adjoint if and only if $A$ is symmetric. \tcbline
\begin{proof}
Let $T$ be self-adjoint. Then $Av\cdot w=v\cdot Aw$ for all $v,w\in V$. 
\end{proof}
\end{prp}

\subsection{The Trace of a Linear Map}
\begin{defn}{The Trace of a Linear Map}{} Let $V$ be an inner product space over $\R$ or $\C$. Let $T:V\to V$ be a linear map. Let $\{e_1,\dots,e_n\}$ be an orthonormal basis. Define the trace of $T$ to be $$\text{tr}(T)=\sum_{i=1}^{\dim(V)}\langle T(e_i),e_i\rangle$$
\end{defn}

TBA: Invariance under choice of basis. 

\begin{defn}{The Trace of a Matrix}{} Let $k=\R$ or $\C$. Let $M=(m_{i,j})\in M_{n\times n}(k)$ be a matrix. Define the trace of $M$ to be $$\text{tr}(M)=\sum_{i=1}^nm_{i,i}$$
\end{defn}

\begin{prp}{}{} Let $V$ be an inner product space over $k=\R$ or $\C$. Let $T:V\to V$ be a linear map. Suppose that $M\in M_{n\times n}(k)$ represent the linear map $T$. Then $$\text{tr}(T)=\text{tr}(M)$$
\end{prp}

\begin{lmm}{}{} Let $k=\R$ or $\C$. Let $A,B=(m_{i,j})\in M_{n\times n}(k)$ be two square matrices. Then the following are true. 
\begin{itemize}
\item $\text{tr}:M_{n\times n}(k)\to k$ is a linear map
\item $\text{tr}(AB)=\text{tr}(BA)$
\end{itemize}
\end{lmm}

\pagebreak
\section{Orthogonality}
\subsection{Orthogonal Vectors}
\begin{defn}{Orthogonal Vectors}{} Let $\F=\R$ (or $\C$). Let $V$ be a vector space over $\F$. Let $\tau:V\times V\to\F$ be a symmetric (Hermitian) bilinear form. We say that $v,w\in V$ are orthogonal if $$\tau(v,w)=0$$
\end{defn}

\begin{defn}{Orthogonal Complement}{} Let $\F=\R$ (or $\C$). Let $V$ be a vector space over $\F$. Let $\tau:V\times V\to\F$ be a symmetric (Hermitian) bilinear form. Let $W\subseteq V$ be a subspace of $V$. Define the orthogonal complement of $W$ to be $$W^{\perp}=\{v\in V|\langle v,w\rangle=0\text{ for all }w\in W\}$$
\end{defn}

\begin{prp}{}{} Let $\F=\R$ (or $\C$). Let $V$ be a vector space over $\F$. Let $\tau:V\times V\to\F$ be a symmetric (Hermitian) bilinear form. Then the following are equivalent. 
\begin{itemize}
\item $\tau$ is non-degenerate. 
\item The matrix representing $\tau$ over any basis of $V$ is invertible. 
\item $V^\perp=\{0\}$. 
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $\F=\R$ (or $\C$). Let $V$ be a vector space over $\F$. Let $\tau:V\times V\to\F$ be a symmetric (Hermitian) bilinear form. Let $W$ be a subspace of $V$. Then the following are equivalent. 
\begin{itemize}
\item $\tau|_{W\times W}$ is non-degenerate on $W$. 
\item $W\cap W^\perp=\{0\}$. 
\item $V=W\oplus W^\perp$. 
\end{itemize}
\end{prp}

\begin{prp}{}{} Let $\F=\R$ (or $\C$). Let $V$ be a vector space over $\F$. Let $\tau:V\times V\to\F$ be a symmetric (Hermitian) bilinear form. Let $W$ be a subspace of $V$. If $\tau|_{W\times W}$ is non-degenerate on $W$, then we have $$W=(W^\perp)^\perp$$
\end{prp}

\subsection{Orthogonality with Inner Products}
With the added benefit of positive definiteness, we can produce bases for inner product spaces that are pairwise orthogonal. 

\begin{thm}{Pythagorean Theorem}{} Let $\F=\R$ or $\C$. Let $(V,\langle-,-\rangle$ be an inner product space over $\F$. Let $\|-\|$ be the induced norm. If $u,v\in V$ are orthogonal, then we have $$\|u+v\|^2=\|u\|^2+\|v\|^2$$ \tcbline
\begin{proof}
We have that 
\begin{align*}
\|u+v\|^2&=\langle u+v,u+v\rangle\\
&=\langle u,u\rangle+\langle u,v\rangle+\langle v,u\rangle+\langle v,v\rangle\\
&=\langle u,u\rangle+\langle v,v\rangle\\
&=\|u\|^2+\|v\|^2
\end{align*}
\end{proof}
\end{thm}

\begin{thm}{Orthogonal Decomposition}{} Let $\F=\R$ or $\C$. Let $(V,\langle-,-\rangle$ be an inner product space over $\F$. Let $\|-\|$ be the induced norm. Let $u,v\in V$ and $v\neq 0$. Set $c=\frac{\langle u,v\rangle}{\|v\|^2}$ and $w=u-\frac{\langle u,v\rangle}{\|v\|^2}v$. Then $\langle w,v\rangle=0$ and $u=cv+w$. \tcbline
\begin{proof}
The fact that $u=cv+w$ is natural so we only have to prove that $\langle w,v\rangle=0$. We have that 
\begin{align*}
\langle w,v\rangle&=\left\langle u-\frac{\langle u,v\rangle}{\|v\|^2}v,v\right\rangle\\
&=\langle u,v\rangle-\left\langle \frac{\langle u,v\rangle}{\|v\|^2}v,v\right\rangle\\
&=\langle u,v\rangle-\frac{\langle u,v\rangle}{\|v\|^2}\langle v,v\rangle\\
&=\langle u,v\rangle-\frac{\langle u,v\rangle}{\|v\|^2}\|v\|^2\\
&=0
\end{align*}
Thus we are done. 
\end{proof}
\end{thm}

\begin{prp}{}{} Let $\F=\R$ or $\C$. Let $(V,\langle-,-\rangle)$ be an inner product space over $\F$. Let $v_1,\dots,v_n\in V\setminus\{0\}$ be pairwise orthogonal. Then $v_1,\dots,v_n$ are linearly independent.  \tcbline
\begin{proof}
We want to show that $v_n=\sum_{k=1}^{n-1}a_kv_k$ implies $a_1=\dots=a_{n-1}=0$. For $i\neq n$, we have that $$0=\langle v_n, v_i\rangle=\sum_{k=1}^{n-1}a_k\tau(v_k,v_i)=a_i\langle v_i,v_i\rangle$$ Since $\langle-,-\rangle$ is positive definite, $\langle v_i,v_i\rangle\neq 0$. Then $a_i=0$. 
\end{proof}
\end{prp}

\begin{defn}{Orthonormal Basis}{} Let $\F=\R$ or $\C$. Let $(V,\langle-,-\rangle)$ be an inner product space over $\F$. Let $b_1,\dots,b_n\in V$ be vectors. We say that they form an orthonormal basis of $V$ is the following are true. 
\begin{itemize}
\item $\{b_1,\dots,b_n\}$ is a basis for $V$. 
\item $\|b_i\|=1$ for $i\in\{1,\dots,n\}$
\item The vectors $b_1,\dots,b_n$ are pairwise orthogonal. 
\end{itemize}
\end{defn}

\begin{prp}{}{} Let $\F=\R$ or $\C$. Let $(V,\langle-,-\rangle)$ be an inner product space over $\F$. Let $\|-\|$ be the induced norm. Let $b_1,\dots,b_n$ be an orthonormal basis for $V$. Let $v=\sum_{k=1}^na_kb_k\in V$. Then the following are true. 
\begin{itemize}
\item $\|v\|^2=\sum_{k=1}^n\abs{a_k}^2$
\item $v=\sum_{k=1}^n\langle v,b_k\rangle b_k$
\end{itemize} \tcbline
\begin{proof}
We have that 
\begin{align*}
\|v\|^2&=\left\langle\sum_{k=1}^na_kb_k, \sum_{k=1}^na_kb_k\right\rangle \\
&=\sum_{i=1}^n\sum_{j=1}^na_ia_j(b_i\cdot b_j)\\
&=\sum_{i=1}^n\sum_{j=1}^na_ia_j\delta_{ij}\\
&=\sum_{k=1}^n\abs{a_k}^2
\end{align*}
and we are done. \\~\\

Applying the inner product with $b_i$ for each $i\in\{1,\dots,n\}$ gives $a_i=\langle v,b_i\rangle$ since $\langle b_i,b_k\rangle=0$ for any $k\neq i$. Thus if $v=\sum_{k=1}^na_kb_k$ then $v=\sum_{k=1}^n\langle v,b_k\rangle b_k$ and we are done. 
\end{proof}
\end{prp}

\begin{thm}{Gram-Schmidt Procedure}{} Let $\F=\R$ or $\C$. Let $(V,\langle-,-\rangle)$ be an inner product space over $\F$. Let $\|-\|$ be the induced norm. Let $v_1,\dots v_m$ be a list of linearly independent vectors of $V$. Define the following vectors. 
\begin{itemize}
\item $b_1=\frac{v_1}{\|v_1\|}$. 
\item For $i=2,\dots,m$, define $$b_i=\frac{v_i-\sum_{k=0}^{i-1}\langle v_i,b_k\rangle b_k}{\|v_i-\sum_{k=0}^{i-1}\langle v_i,b_k\rangle b_k\|}$$
\end{itemize}
Then $b_1,\dots,b_m$ are orthonormal and has the same span as $v_1,\dots,v_m$. 
\end{thm}

\begin{crl}{}{} Every finite dimensional inner product space has an orthonormal basis. \tcbline
\begin{proof}
By the Gram-Schmidt procedure, every basis can be transformed into an orthonormal basis. 
\end{proof}
\end{crl}

\begin{defn}{Orthgonal Maps}{} A linear map $T:V\to V$ is said to be orthogonal if $$\langle T(v), T(w)\rangle=\langle v, w\rangle$$ for all $v,w\in V$. 
\end{defn}

One can think of orthgonal maps as orthgonality preserving maps. If $\langle v,w\rangle=0$ then $\langle T(v),T(w)\rangle=0$ which means orthogonality is preserved.

\begin{prp}{}{} Let $T:V\to V$ be a linear map over an inner product space $V$. Let $A$ represent the linear map $T$. Then the following are equivalent. 
\begin{itemize}
\item $T$ is orthogonal
\item $A$ is orthogonal
\item $T^\ast=T^{-1}$. 
\item $T$ maps orthonormal bases to orthonormal bases
\end{itemize} \tcbline
\begin{proof} Suppose that $T:V\to V$ is represented by $A$. 
\begin{itemize}
\item $(1)\iff(2)$: We have that $\langle T(v),T(w)\rangle=v^TA^TAw$. Thus it is equal to $v^Tw$ if and only if $A^TA=I$. 
\item $(1)\iff(3)$: Suppose that $T$ is orthogonal. Suppose that $\{b_1,\dots,b_n\}$ is orthonormal. Then $\langle T(b_i),T(b_j)\rangle=\langle b_i,b_j\rangle=0$ for $i,j\in\{1,\dots,n\}$. Thus $\{T(b_1),\dots,T(b_n)\}$ is orthogonal. But they are also orthonormal since $\|T(b_i)\|^2=\langle T(b_i),T(b_i)\rangle=\langle b_i,b_i\rangle=\|b_i\|^2=1$. This means that $\|T(b_i)\|=1$ for $i\in\{1,\dots,n\}$. \\~\\
Now suppose that $T$ maps orthonormal bases to orthonormal bases. Then if $\{b_1,\dots,b_n\}$ is orthonormal, we have 
\begin{align*}
\langle T(v),T(w)\rangle&=\left\langle\left(\sum_{k=1}^nv_kT(b_k)\right),\left(\sum_{k=1}^nw_kT(b_k)\right)\right\rangle\\
&=\sum_{k=1}^n(v_kw_k)\langle T(b_k),T(b_k)\rangle\tag{$\langle T(b_i),T(b_j)\rangle=0$ if $i\neq j$}\\
&=\sum_{k=1}^nv_kw_k\\
&=\langle v,w\rangle
\end{align*}
Thus we are done. 
\end{itemize}
\end{proof}
\end{prp}

\pagebreak
\section{Reductions and Decompositions of Linear Maps over Real Vector Spaces}
\subsection{Reduction of Quadratic Forms over $\R$}
Orthogonality is interesting for real matrices because the notion of similarity and congruence coinincide under orthogonality. Notice that being similar and congruent to a diagonal matrix at the same time means that there exists an invertible $P$ such that $P^TAP=P^{-1}AP=D$. \\~\\

In the remaining sections we treat the adjugate in the case of $\R^n$ and save the case for $\C^n$ in another chapter. Then $V$ in the remaining sections will only denote $\R^n$. 

\begin{prp}{}{} A function $b:V\times V\to\R$ is an inner product over $\R$ if and only if $b$ is bilinear and positive definite. 
\end{prp}

\begin{lmm}{Polarization Identity}{} For $x,y\in\R^n$, $$\langle x,y\rangle=\frac{1}{4}\|x+y\|^2-\frac{1}{4}\|x-y\|^2$$\tcbline
\begin{proof} Simple proof using the fact that $\|x\|^2=\langle x,x\rangle$. 
\end{proof}
\end{lmm}

\begin{prp}{Sylvester's Law of Inertia}{} A quadratic form $q$ over $\R$ has the form $$q(x_1,\dots,x_n)=\sum_{k=1}^tx_k^2-\sum_{k=1}^ux_k^2$$ where $t+u=\text{rank}(q)$. The pair $(t,u)$ is called the signature of $q$. This reduced quadratic form is also unique in the sense that the number of positives and number of negatives of any two reduced forms are the same. \\~\\ Moreover, every symmetric matrix is congruent to a matrix of the form $$\begin{pmatrix}
I_t & 0 & 0\\
0 & -I_u & 0\\
0 & 0 & 0\\
\end{pmatrix}$$ where $(t,u)$ is the signature of the quadratic form. \tcbline
\begin{proof}
We saw in theorem 1.2.7 that every quadratic form can be expressed as $$q(y_1,\dots,y_n)=\sum_{k=1}^nc_ky_k^2$$ By doing a basis change with $b_k'=\frac{1}{\sqrt{c_k}}b_k$ whenever $c_k\neq 0$ will give us the above sum. For those that have $c_k=0$, the terms vanish and are exactly in the kernel of the quadratic form thus $t+u=\text{rank}(q)$. \\~\\
The second part is direct from the fact that same quadratic forms with different matrix representations imply their representations are similar. 
\end{proof}
\end{prp}

\subsection{Reduction of Inner Products}
\begin{defn}{Dot Product}{} The dot product in $\R^n$ is defined to be the inner product given by $$x\cdot y=x_1y_1+\dots+x_ny_n$$ in standard basis. 
\end{defn}

\begin{thm}{}{} Let $\langle \cdot,\cdot\rangle$ be an inner product on a real vector space $V$. Then there exists an basis $b_1,\dots,b_n$ of $V$ such that the inner product, when represented in the orthonormal basis, takes the form of exactly the dot product. \tcbline
\begin{proof}
Let $\langle \cdot,\cdot\rangle$ be our inner product in question. Define a quadratic form by $$q(x)=\langle x,x\rangle=\|x\|^2$$ We know that this quadratic form can be reduced to $$q(x_1,\dots,x_n)=x_1^2+\dots+x_t^2-x_{t+1}^2-\dots x_{t+u}^2$$ Now we must have $u=0$ since if $u>0$, then the basis vector $b_{t+1}$ satisfies $q(b_{t+1})=-1$ and $q(b_{t+1})=\langle b_{t+1},b_{t+1}\rangle$ which is a contradiction since inner products are positive definite. Also $t=n$ since if $t<n$, then $\langle b_{t+1},b_{t+1}\rangle=0$ which is again a contradiction. \\~\\
Using polarization, we see that $\langle x,y\rangle=x_1y_1+\dots+x_ny_n$ in that basis and we are done. 
\end{proof}
\end{thm}

With this theorem, we know that any inner product can be expressed in the dot product as long as it is under a suitable basis. Thus we now reduce our discussion to only the dot product, as our standard inner product in $\R^n$. \\~\\
The below theorem, while unrelated to the reduction of inner products, is a result of Gram-schmidt process that is only true for real matrices. 

\begin{thm}{QR Decomposition}{} Let $A$ be an $n\times n$ matrix over $\R$. Then we can write $A=QR$ where $Q$ is orthogonal and $R$ is upper triangular. \tcbline
\begin{proof}We split the matrices into two cases. Firstly consider the case where $A$ is invertible. We can treat $A$ as a change of basis matrix from the basis $\{a_1,\dots,a_n\}$ where $a_k$ is the column of $A$ for $k\in\{1,\dots,n\}$. This change of basis matrix takes $\{a_1,\dots,a_n\}$ to $\{e_1,\dots,e_n\}$ which is the standard basis. Apply the Gram-schmidt process to $\{a_1,\dots,a_n\}$ to get $\{b_1,\dots,b_n\}$ which is an orthonormal basis. Let $Q$ be the change of basis matrix from $\{b_1,\dots,b_n\}$ to $\{e_1,\dots,e_n\}$. Let $R$ be the change of basis matrix from $\{a_1,\dots,a_n\}$ to $\{b_1,\dots,b_n\}$. Then clearly $A=QR$. We just have to show that $Q$ is orthogonal and $R$ is upper triangular. \\~\\
$Q$ being orthonormal is trivial since columns of $Q$ are just $b_1,\dots,b_n$. Using the Gram-schimdt process, we can see that the change of basis from $\{b_1,\dots,b_n\}$ to $\{a_1,\dots,a_n\}$, each $b_k$ is only affected by $a_1,\dots,a_k$ from the old basis. This means that the change of basis matrix must be upper triangular and its inverse must also be upper triangular. \\~\\
Now we also have the case when $A$ is not invertible. 
\end{proof}
\end{thm}

We now give an example of QR decomposition, in conjunction with the Gram-schmidt procedure. 

\begin{eg}{}{} Find the QR decomposition of $$A=\begin{pmatrix}
-1 & 0 & -2\\
2 & 0 & -1\\
0 & -2 & -2
\end{pmatrix}$$ \tcbline
\begin{proof}
A quick check shows that $A$ is invertible. Let $a_1,a_2,a_3$ be the columns of $A$. We start the Gram-schimdt process by taking the new basis $f_1=\frac{a_1}{\|a_1\|}=\begin{pmatrix}-\frac{1}{\sqrt{5}}\\ \frac{2}{\sqrt{5}}\\ 0\end{pmatrix}$. We also need to keep track on the change of basis matrix. We have that $a_1=\sqrt{5}f_1$. \\~\\
For the next step, we find that 
\begin{align*}
f_2&=\frac{a_2-(a_2\cdot f_1)f_1}{\|a_2-(a_2\cdot f_1)f_1\|}\\
&=\frac{a_2}{\|a_2\|}\\
&=\begin{pmatrix} 0\\ 0\\ -1\end{pmatrix}
\end{align*}
This means that $a_2=2f_2$. \\~\\
Finally, we have that 
\begin{align*}
f_3&=\frac{a_3-(a_3\cdot f_1)f_1-(a_3\cdot f_2)f_2}{\|a_3-(a_3\cdot f_1)f_1-(a_3\cdot f_2)f_2\|}\\
&=\frac{a_3-2f_2}{\|a_3-2f_2\|}\\
&=\frac{a_3-2f_2}{\sqrt{5}}\\
&=\begin{pmatrix}-\frac{2}{\sqrt{5}}\\ -\frac{1}{\sqrt{5}}\\ 0\end{pmatrix}
\end{align*}
We have that $a_3=2f_2+\sqrt{5}f_3$. Combining everything together, we have that $$\begin{pmatrix}
-1 & 0 & -2\\
2 & 0 & -1\\
0 & -2 & -2
\end{pmatrix}=\begin{pmatrix}
-\frac{1}{\sqrt{5}} & 0 & -\frac{2}{\sqrt{5}}\\
\frac{2}{\sqrt{5}} & 0 & -\frac{1}{\sqrt{5}}\\
0 & -1 & 0
\end{pmatrix}\begin{pmatrix}
\sqrt{5} & 0 & 0\\
0 & 2 & 2\\
0 & 0 & \sqrt{5}
\end{pmatrix}$$
\end{proof}
\end{eg}

\subsection{Self-Adjoints in $\R^n$}
\begin{prp}{}{} Let $T:\R^n\to\R^n$ be self-adjoint. Then every eigenvalues of $T$ are real. \tcbline
\begin{proof}
Suppose that $T(v)=Av$ where $A$ is a representation of $T$. Suppose that $\lambda$ is an eigenvalue of $T$. Then $Av=\lambda v$ for some eigenvector $v\in\R^n$. Then taking complex conjugates give 
\begin{align*}
\overline{Av}&=\overline{\lambda v}\\
A\overline{v}&=\overline{\lambda}\overline{v}
\end{align*}
Taking the transpose of $Av=\lambda v$ gives $v^TA^T=\lambda v^T$ and $v^TA=\lambda v^T$. Multiplying $\overline{v}$ on bothe sides give 
\begin{align*}
v^TA\overline{v}&=\lambda v^T\overline{v}\\
\overline{\lambda}v^T\overline{v}&=\lambda v^T\overline{v}
\end{align*}
But $v^T\overline{v}=v_1\overline{v_1}+\dots+v_n\overline{v_n}=\abs{v_1}^2+\dots+\abs{v_n}^2$ which is $0$ if and only if $v$ is $0$. Since eigenvectors are taken to be nonzero, we must have $\lambda=\overline{\lambda}$ and thus $\lambda$ is real. 
\end{proof}
\end{prp}

\begin{thm}{}{} Let $T:\R^n\to\R^n$ be a linear map on the inner product space $\R^n$ that is seldf-adjoint. Then there exists an orthonormal basis consisting of eigenvectors of $T$. \\~\\
Equivalently, for every quadratic form $q$ on $V$, there exists an orthonormal basis $b_1,\dots,b_n$ such that $$q(x_1,\dots,x_n)=\sum_{k=1}^nc_kx_k^2$$ for some $c_1,\dots,c_n\in\R$. \tcbline
\begin{proof}
Notice that the two statements are exactly the same and I will ommit the reason. \\~\\
We prove by induction on $n$. Suppose that the theorem holds for $n-1$. Let $T$ be the linear map. By the above we know that $T$ has at least one eigenvalue in $\R$, say $\lambda_1$. Let $f_1$ be the corresponding eigenvector with magnitude $1$. \\~\\
Consider the orthogonal complement $W=\{w\in V|w\cdot f_1=0\}$ of $f_1$. Since $W$ is the kernel of the linear map $S:V\to\R$ defined by $S(v)=v\cdot f_1$, it is a subspace of $V$ dimension $n-1$. I claim that $T(W)\subseteq W$. \\~\\
Let $w\in W$. We have $$T(w)\cdot f_1=w\cdot T(f_1)=w\cdot\lambda_1f_1=0$$ by self-adjoint. Thus we have shown that $T(W)\subseteq W$. \\~\\
Applying the induction hypothesis on $W$, we have an orthonormal basis $f_2,\dots,f_n$ of $W$ consisting of eigenvectors of $T$. By definition, $f_1,\dots,f_n$ is an orthonormal basis and we are done. 
\end{proof}
\end{thm}

Notice that the above two statements are also equivalent to saying that every real symmetric matrix is congruent and similar to a diagonal matrix. 

\begin{prp}{}{} If $T:\R^n\to\R^n$ is self-adjoint, and $\lambda,\mu$ are distinct eigenvalues of $T$ with eigenvectors $v,w$, then $v\cdot w=0$. \tcbline
\begin{proof}
We have that
\begin{align*}
v^TAw&=v\cdot Aw\\
&=v^T\mu w\\
&=\mu(v\cdot w)
\end{align*} and 
\begin{align*}
v^TAw&=v^TA^Tw\\
&=(Av)^Tw\\
&=(\lambda v)^Tw\\
&=\lambda v^Tw\\
&=\lambda(v\cdot w)
\end{align*}
Comparing the two results, we have that $(\mu-\lambda)(v\cdot w)=0$ and thus $v\cdot w=0$. 
\end{proof}
\end{prp}

The proposition will prove itself to be useful in finding an orthonormal basis for self-adjoint linear maps. 

\subsection{Singular Value Decomposition}
\begin{thm}{Singular Value Decomposition for Linear Maps}{} Let $T:\R^n\to\R^m$ be a linear map of rank $k$. Then there eixsts unique positive numbers $\gamma_1\geq\gamma_2\geq\dots\geq\gamma_k\geq0$ and orthonormal bases of $\R^n$ and $\R^m$ such that the matrix of $T$ with respect to these bases is $$\begin{pmatrix}
D & 0\\
0 & 0
\end{pmatrix}$$ where $D=\text{diag}(\gamma_1,\dots,\gamma_k)$. In fact, the $\gamma_i$ are exactly the nonzero eigenvalues of $T^*T$, each one appearing as many times as the dimension of the corresponding eigenspace. 
\end{thm}

\begin{thm}{Singular Value Decomposition for Matrices}{} Let $A_{m\times n}$ be a matrix. There exists unique singular values $\gamma_1\geq\gamma_2\geq\dots\gamma_k\geq0$ where $k=$rank$(A)$, and orthogonal matrices $P,Q$ such that $$\begin{pmatrix}
D & 0\\
0 & 0
\end{pmatrix}=P^TAQ$$ where $D=\text{diag}(\gamma_1,\dots,\gamma_k)$. 
\end{thm}

We present an example of singular value decomposition for illustration. 

\begin{eg}{}{} Find the singular value decomposition of the matrix $$A=\begin{pmatrix}
4 & 11 & 14\\
8 & 7 & -2
\end{pmatrix}$$ \tcbline
\begin{proof}
Step 1: We compute the singular values of $A$, which is just the squareroot of the eigenvalues of $A^TA$. Now 
$$A^TA=\begin{pmatrix}
80 & 100 & 40\\
100 & 170 & 140\\
40 & 140 & 200
\end{pmatrix}$$
We have that $c_{A^TA}(x)=x(360-x)(90-x)$. This means that the singular values are $\gamma_1=\sqrt{360}=6\sqrt{10}$ and $\gamma_2=\sqrt{90}=3\sqrt{10}$. Now we want $P$ and $Q$ such that $$P^TAQ=\begin{pmatrix}
6\sqrt{10} & 0 & 0\\
0 & 3\sqrt{10} & 0
\end{pmatrix}$$~\\
Step 2: We find the orthonormal eigenvectors of $A^TA$ so that it forms the matrix $Q$. This gives $$Q=\begin{pmatrix}
\frac{1}{3} & -\frac{2}{3} & \frac{2}{3}\\
\frac{2}{3} & -\frac{1}{3} & -\frac{2}{3}\\
\frac{2}{3} & \frac{2}{3} & \frac{1}{3}
\end{pmatrix}$$~\\
Step 3: We now calculate $P$ by finding the image of the above basis under $A$, and dividing it with the nonzero singular values. This gives 
\begin{align*}
P&=\begin{pmatrix}
\frac{1}{6\sqrt{10}}Ab_1 & \frac{1}{3\sqrt{10}}Ab_2
\end{pmatrix}\\
&=\begin{pmatrix}
\frac{3}{\sqrt{10}} & \frac{1}{\sqrt{10}}\\
\frac{1}{\sqrt{10}} & -\frac{3}{\sqrt{10}}
\end{pmatrix}
\end{align*}
This means that we are done. 
\end{proof}
\end{eg}

\end{document}